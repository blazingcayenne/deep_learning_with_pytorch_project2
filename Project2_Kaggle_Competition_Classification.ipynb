{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhTA9Jbl3-5s"
   },
   "source": [
    "# <font style=\"color:blue\">Project 2: Kaggle Competition - Classification</font>\n",
    "\n",
    "#### Maximum Points: 100\n",
    "\n",
    "<div>\n",
    "    <table>\n",
    "        <tr><td><h3>Sr. no.</h3></td> <td><h3>Section</h3></td> <td><h3>Points</h3></td> </tr>\n",
    "        <tr><td><h3>1</h3></td> <td><h3>Data Loader</h3></td> <td><h3>10</h3></td> </tr>\n",
    "        <tr><td><h3>2</h3></td> <td><h3>Configuration</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>3</h3></td> <td><h3>Evaluation Metric</h3></td> <td><h3>10</h3></td> </tr>\n",
    "        <tr><td><h3>4</h3></td> <td><h3>Train and Validation</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>5</h3></td> <td><h3>Model</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>6</h3></td> <td><h3>Utils</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>7</h3></td> <td><h3>Experiment</h3></td><td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>8</h3></td> <td><h3>TensorBoard Dev Scalars Log Link</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>9</h3></td> <td><h3>Kaggle Profile Link</h3></td> <td><h3>50</h3></td> </tr>\n",
    "    </table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:green\">Project Approach</font>\n",
    "\n",
    "The objective of this project is to correctly classify the following food types in the KenyanFood13 data set.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/monajalal/Kenyan-Food/master/img/KenyanFood13.png\" alt=\"Sample images from the KenyanFood13 data set\" width=\"600px\" />\n",
    "\n",
    "Rather than create my own CNN architecture, I will take advantage of existing models that have been trained on the ImageNet data set.\n",
    "\n",
    "___\n",
    "\n",
    "The **[Transfer learning and the art of using Pre-trained Models in Deep Learning](https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/)** blog post outlines four ways to fine tune a model that has been trained on a different dataset. The following is a short section of this post that I slightly changed.\n",
    "\n",
    "The following diagram depicts four scenarios of using a pretrained model on a new data set.\n",
    "\n",
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2017/05/31112715/finetune1.jpg\" alt=\"Transfer learning approaches\" width=\"300px\">\n",
    "\n",
    "**Scenario 1: Size of the data set is small and the data similarity is high.** In this case, since the data similarity is very high, we do not need to retrain the model. All we need to do is to customize and modify the output layers according to our problem statement. We use the pretrained model as a feature extractor and retrain the classification block/layer.\n",
    "\n",
    "**Scenario 2: Size of the data set is small and the data similairity is low.** In this case we can freeze the initial (letâ€™s say k) layers of the pretrained model and train just the remaining(n-k) layers again. The top layers would then be customized to the new data set. Since the new data set has low similarity it is significant to retrain and customize the higher layers according to the new dataset.  The small size of the data set is compensated by the fact that the initial layers are kept pretrained(which have been trained on a large dataset previously) and the weights for those layers are frozen.\n",
    "\n",
    "**Scenario 3: Size of the data set is large and the data similarity is low.**  In this case, since we have a large dataset, our neural network training would be effective. However, since the data we have is very different as compared to the data used for training our pretrained models. The predictions made using pretrained models would not be effective. Hence, its best to train the neural network from scratch according to your data.\n",
    "\n",
    "**Scenario 4: Size of the data set is large and the data similarity is high.** This is the ideal situation. In this case the pretrained model should be most effective. The best way to use the model is to retain the architecture of the model and the initial weights of the model. Then we can retrain this model using the weights as initialized in the pre-trained model.\n",
    "\n",
    "___\n",
    "\n",
    "Since I did not know how to program in Python before this class, I used this project to improve my Python proficiency. Consequently, I did not only to explore using pretrained models on a new data set, but I also spent significant time developing class hierachies that will allow me to easily conduct experiments on the following pretrained TorchVision models using any of the scenarios described above.\n",
    "\n",
    "* ResNet-18\n",
    "* ResNet-34\n",
    "* ResNet-50\n",
    "* ResNet-101\n",
    "* ResNet-152\n",
    "* ResNeXt-50-32x4d\n",
    "* ResNeXt-101-32x8d\n",
    "* Wide ResNet-50-2\n",
    "* Wide ResNet-101-2\n",
    "* VGG-11 with batch normalization\n",
    "* VGG-13 with batch normalization\n",
    "* VGG-16 with batch normalization\n",
    "* VGG-19 with batch normalization\n",
    "* DenseNet-121\n",
    "* DenseNet-169\n",
    "* DenseNet-201\n",
    "* DenseNet-161\n",
    "\n",
    "I used and modified the trainer module rather than use Pytorch Lightning. Modications to the trainer module include, but are not limited to, adding additional configuration parameters, adding the ability to prematurely stop training when either the loss or accuracy does not significantly improve over a certain number of epochs, extending the visualization base and TensorBoard classes to allow logging of images, figures, graphs, and PR curves.\n",
    "\n",
    "Experiments are identified by uppercase capital letters, _reg expr_ = \\(\\[A-Z\\]\\[A-Z\\]\\[A-Z\\]\\). The first and second letters designate the experiment group and set respectiviely, while the last letter designates an individual experiment. Hence, all experiments that begin with \"A\" belong the Group A, while all experiments that begin with \"ExpAB\" belong to Group A, Set B.\n",
    "\n",
    "I implemented the following groups of expeirments.\n",
    "\n",
    "* Group A to explore the data and verify the training pipeline.\n",
    "* Group B to explore the four transfer learning scenarios on a model from the ResNet, VGG, and DenseNet families.\n",
    "* Group C to explore optimizing the transfer learning approach that worked best.\n",
    "* Group D to explore whether an ensemble performs better than its constitute parts.\n",
    "* Group E to explore miscellaneous issues, e.g., performance of Project 1 model, normalizing KenyanFood13 data by its mean/std, training on grayscale images, impact of no or poor data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4LNCmWgkBNLg",
    "outputId": "0f7e955d-30be-430c-ffc5-a64ec8d98ae4"
   },
   "outputs": [],
   "source": [
    "# This cell initializes the notebook for execution on different hosts.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def get_host() -> str:\n",
    "    \"\"\"\n",
    "    The get_ipython() function returns the following from different hosts.\n",
    "\n",
    "    colab:  <google.colab._shell.Shell object at 0x7f23c5e386d8>\n",
    "    brule:  <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f1990f22a50>\n",
    "    kaggle: <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f9d093aebd0>\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'google.colab' in str(get_ipython()):\n",
    "        return \"colab\"\n",
    "    else:\n",
    "        # ToDo: Determine whether running on kaggle.\n",
    "        return \"brule\"\n",
    "\n",
    "def init_host(host:str):\n",
    "    if host == \"brule\":\n",
    "        # set data and project directories\n",
    "        if os.path.isdir(\"./trainer\"):\n",
    "            data_dir = \"./data\"\n",
    "            proj_dir = \"./\"\n",
    "        elif os.path.isdir(\"./project2/trainer\"):\n",
    "            data_dir = \"./project2/data\"\n",
    "            proj_dir = \"./project2\"\n",
    "        else:\n",
    "            raise SystemExit(\"Cannot locate trainer module.\")\n",
    "\n",
    "    elif host == \"colab\":\n",
    "        # mount Google Drive\n",
    "        from google.colab import drive\n",
    "        drive.mount(\"/content/gdrive\")\n",
    "\n",
    "        # set data and project directories\n",
    "        data_dir = \"/content/data\"\n",
    "        proj_dir = \"/content/gdrive/MyDrive/Colab Notebooks/project2\"\n",
    "\n",
    "        # fetching data from Google Drive is very, very slow ...\n",
    "        # hence, we will unzip the dataset to /content/data if it is not there\n",
    "        dataset = os.path.join(proj_dir, \"data\", \"pytorch-opencv-course-classification.zip\")\n",
    "        if not os.path.isdir(data_dir):\n",
    "              os.makedirs(data_dir)\n",
    "              import zipfile\n",
    "              with zipfile.ZipFile(dataset, 'r') as zip_ref:\n",
    "                  zip_ref.extractall(data_dir)              \n",
    "\n",
    "    else:\n",
    "        raise SystemExit(\"Unknown host! Cannot continue.\")\n",
    "\n",
    "    sys.path.append(proj_dir)\n",
    "    return data_dir, proj_dir\n",
    "\n",
    "data_dir, proj_dir = init_host(get_host())\n",
    "\n",
    "print(f\"data_dir: {data_dir}\")\n",
    "!ls -lh {data_dir.replace(\" \", \"\\\\ \")}\n",
    "\n",
    "print(f\"proj_dir: {proj_dir}\")\n",
    "!ls -lh {proj_dir.replace(\" \", \"\\\\ \")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xiq8YhMC3-5w"
   },
   "outputs": [],
   "source": [
    "# import organzier @ https://pypi.org/project/importanize/\n",
    "\n",
    "from abc import ABC, abstractmethod, abstractproperty\n",
    "from collections import namedtuple\n",
    "from dataclasses import dataclass, replace\n",
    "from enum import Enum, auto\n",
    "from operator import itemgetter\n",
    "from typing import Callable, Iterable, List, Optional, Tuple\n",
    "\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "\n",
    "from trainer import Trainer, configuration, hooks\n",
    "from trainer.configuration import SystemConfig\n",
    "from trainer.configuration import DataAugConfig, DatasetConfig, DataLoaderConfig\n",
    "from trainer.configuration import OptimizerConfig, SchedulerConfig, TrainerConfig\n",
    "from trainer.metrics import AccuracyEstimator\n",
    "from trainer.tensorboard_visualizer import TensorBoardVisualizer\n",
    "from trainer.utils import patch_configs, setup_system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWi_M1eB3-5x"
   },
   "source": [
    "## <font style=\"color:green\">1. Data Loader [10 Points]</font>\n",
    "\n",
    "In this section, you have to write a class or methods that will be used to get training and validation data\n",
    "loader.\n",
    "\n",
    "You will have to write a custom dataset class to load data.\n",
    "\n",
    "**Note that there are not separate validation data, so you will have to create your validation set by dividing train data into train and validation data. Usually, in practice, we do `80:20` ratio for train and validation, respectively.** \n",
    "\n",
    "For example,\n",
    "\n",
    "```\n",
    "class KenyanFood13Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "    ....\n",
    "    ...\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "    ...\n",
    "    ...\n",
    "    \n",
    "    \n",
    "```\n",
    "\n",
    "```\n",
    "def get_data(args1, *agrs):\n",
    "    ....\n",
    "    ....\n",
    "    return train_loader, test_loader\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-BZj-t43-5x"
   },
   "outputs": [],
   "source": [
    "class KenyanFood13Data:\n",
    "    \"\"\"\n",
    "    This class parses the KenyanFood13's test.csv and train.csv files and divides the training data\n",
    "    into training and validation sets preserving the relative ratios of the number of images of each\n",
    "    class type.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_root, valid_size = 0.2, random_seed = 42):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # the root path of the images\n",
    "        self.__image_root = os.path.join(data_root, 'images', 'images')\n",
    "        \n",
    "        # parse the test CSV file to obtain filenames (labels are not given)\n",
    "        test_data_frame = self.__parse_data_file(data_root, 'test.csv')\n",
    "        self.__test_fnames = test_data_frame.values[:,0]\n",
    "        \n",
    "        # parse the train CSV file to obtain filenames and labels       \n",
    "        train_data_frame = self.__parse_data_file(data_root, 'train.csv')\n",
    "        fnames = train_data_frame.values[:,0]\n",
    "        labels = train_data_frame.values[:,1]\n",
    "        \n",
    "        # get the classes and class counts\n",
    "        self.__classes, self.__class_counts = np.unique(labels, return_counts=True)\n",
    "        num_classes = len(self.__classes)\n",
    "        \n",
    "        # create a dictionary of text labels to integer labels\n",
    "        label_dict = {}\n",
    "        for key, value in zip(self.__classes, np.arange(num_classes)):\n",
    "            label_dict[key] = value\n",
    "                \n",
    "        # convert the text labels to their numeric equivalents\n",
    "        labels = [label_dict[label] for label in labels]\n",
    "\n",
    "        # retain the complete unsplit training dataset for visualization purposes\n",
    "        self.__unsplit_fnames = fnames\n",
    "        self.__unsplit_labels = labels\n",
    "\n",
    "        # create a dictionary library that stores list of images of the same label\n",
    "        self.__library = {key : [fname for fname, label in zip(fnames, labels) if label == key] \n",
    "                          for key in range(num_classes)}\n",
    "\n",
    "        # split the training data into training and validation sets\n",
    "        self.__train_fnames, self.__valid_fnames, self.__train_labels, self.__valid_labels = train_test_split(\n",
    "            fnames,                      # image file names w/o path or extension\n",
    "            labels,                      # image labels\n",
    "            test_size = valid_size,      # test size\n",
    "            random_state = random_seed,  # random seed for reproducibility\n",
    "            shuffle = True,              # shuffle data before splitting into training and validation sets\n",
    "            stratify = labels            # maintain equal class representation in training and validation sets\n",
    "        )\n",
    "\n",
    "        # create subsets of the training and validation sets for pipeline check\n",
    "        subset_size = 256.0 / len(self.__train_fnames)\n",
    "\n",
    "        _, self.__train_fnames_subset, _, self.__train_labels_subset = train_test_split(\n",
    "            self.__train_fnames,\n",
    "            self.__train_labels,\n",
    "            test_size = subset_size,\n",
    "            random_state = random_seed,\n",
    "            shuffle = True,\n",
    "            stratify = self.__train_labels\n",
    "        )\n",
    "\n",
    "        _, self.__valid_fnames_subset, _, self.__valid_labels_subset = train_test_split(\n",
    "            self.__valid_fnames,\n",
    "            self.__valid_labels,\n",
    "            test_size = subset_size,\n",
    "            random_state = random_seed,\n",
    "            shuffle = True,\n",
    "            stratify = self.__valid_labels\n",
    "        )\n",
    "\n",
    "        \n",
    "    def __parse_data_file(self, data_root, file):\n",
    "        path = os.path.join(data_root, file)\n",
    "        return pd.read_csv(path, delimiter=',', dtype={'id': 'str'}, engine='python')\n",
    "    \n",
    "    @property\n",
    "    def image_root(self):\n",
    "        return self.__image_root\n",
    "\n",
    "    @property\n",
    "    def classes(self):\n",
    "        return self.__classes\n",
    "    \n",
    "    @property\n",
    "    def class_counts(self):\n",
    "        return self.__class_counts\n",
    "    \n",
    "    @property\n",
    "    def test_fnames(self):\n",
    "        return self.__test_fnames\n",
    "    \n",
    "    @property\n",
    "    def train_fnames(self):\n",
    "        return self.__train_fnames\n",
    "    \n",
    "    @property\n",
    "    def train_labels(self):\n",
    "        return self.__train_labels\n",
    "    \n",
    "    @property\n",
    "    def valid_fnames(self):\n",
    "        return self.__valid_fnames\n",
    "    \n",
    "    @property\n",
    "    def valid_labels(self):\n",
    "        return self.__valid_labels\n",
    "\n",
    "    @property\n",
    "    def train_fnames_subset(self):\n",
    "        return self.__train_fnames_subset\n",
    "    \n",
    "    @property\n",
    "    def train_labels_subset(self):\n",
    "        return self.__train_labels_subset\n",
    "    \n",
    "    @property\n",
    "    def valid_fnames_subset(self):\n",
    "        return self.__valid_fnames_subset\n",
    "    \n",
    "    @property\n",
    "    def valid_labels_subset(self):\n",
    "        return self.__valid_labels_subset\n",
    "\n",
    "    @property\n",
    "    def unsplit_fnames(self):\n",
    "        return self.__unsplit_fnames\n",
    "    \n",
    "    @property\n",
    "    def unsplit_labels(self):\n",
    "        return self.__unsplit_labels\n",
    "\n",
    "    @property\n",
    "    def library(self):\n",
    "          return self.__library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jRUlCSI_3-5y"
   },
   "outputs": [],
   "source": [
    "class KenyanFood13Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    This custom PyTorch dataset contains images and classification labels from\n",
    "    Kaggle's KenyanFood13 dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_root, fnames, labels=None, transform=None):\n",
    "        super().__init__()\n",
    "        self.__fnames = fnames\n",
    "        self.__labels = labels\n",
    "        self.__transform = transform\n",
    "        self.__image_root = image_root\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the dataset's length, i.e., the number of image/label pairs.\n",
    "        \"\"\"\n",
    "\n",
    "        return len(self.__fnames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns the (optionally resized & preprocessed) image that corresponds to the specified index.\n",
    "        \"\"\"\n",
    "\n",
    "        # conversion needed to remove alpha channel, if present\n",
    "        path = os.path.join(self.__image_root, self.__fnames[idx] + \".jpg\")\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        \n",
    "        if self.__transform is not None:\n",
    "            image = self.__transform(image)\n",
    "\n",
    "        if self.__labels is not None:\n",
    "            extra = self.__labels[idx]  # return target with image\n",
    "        else:\n",
    "            extra = self.__fnames[idx]  # return filename with image\n",
    "\n",
    "        return image, extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HrW3bvsx3-5y"
   },
   "outputs": [],
   "source": [
    " def get_datasets(\n",
    "    data: KenyanFood13Data,\n",
    "    test_transforms,\n",
    "    train_transforms,\n",
    "    subset = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates datasets for the training, validation, and testing.\n",
    "    \"\"\"\n",
    "\n",
    "    if not subset:\n",
    "\n",
    "        train_dataset = KenyanFood13Dataset(\n",
    "            image_root = data.image_root, \n",
    "            fnames = data.train_fnames, \n",
    "            labels = data.train_labels, \n",
    "            transform = train_transforms)\n",
    "\n",
    "        valid_dataset = KenyanFood13Dataset(\n",
    "            image_root = data.image_root, \n",
    "            fnames = data.valid_fnames, \n",
    "            labels = data.valid_labels, \n",
    "            transform = test_transforms)\n",
    "\n",
    "\n",
    "    else:\n",
    "        \n",
    "        train_dataset = KenyanFood13Dataset(\n",
    "            image_root = data.image_root, \n",
    "            fnames = data.train_fnames_subset, \n",
    "            labels = data.train_labels_subset, \n",
    "            transform = train_transforms)\n",
    "\n",
    "        valid_dataset = KenyanFood13Dataset(\n",
    "            image_root = data.image_root, \n",
    "            fnames = data.valid_fnames_subset, \n",
    "            labels = data.valid_labels_subset, \n",
    "            transform = test_transforms)\n",
    "\n",
    "    test_dataset = KenyanFood13Dataset(\n",
    "        image_root = data.image_root, \n",
    "        fnames = data.test_fnames, \n",
    "        transform = test_transforms)\n",
    "\n",
    "    return train_dataset, valid_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zvtvn0vO3-5z"
   },
   "outputs": [],
   "source": [
    "def get_data_loaders(\n",
    "    train_dataset: Dataset,\n",
    "    valid_dataset: Dataset,\n",
    "    test_dataset: Dataset,\n",
    "    batch_size = 16, \n",
    "    num_workers = 2\n",
    "):\n",
    "    \"\"\"\n",
    "    This function creates and returns the training and validation data loaders.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        num_workers=num_workers, \n",
    "        shuffle=True)\n",
    "    \n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, \n",
    "        batch_size=batch_size, \n",
    "        num_workers=num_workers, \n",
    "        shuffle=False)\n",
    "\n",
    "    test_data_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        num_workers=num_workers, \n",
    "        shuffle=False)\n",
    "    \n",
    "\n",
    "    return train_data_loader, valid_data_loader, test_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ONTK53m3-5z"
   },
   "outputs": [],
   "source": [
    "def get_mean_std(data_loader=None):\n",
    "    \"\"\"\n",
    "    Computes the mean and standard deviation. Since this method takes a long\n",
    "    time to run and the data for this workbook is fixed, this method was run\n",
    "    once and its result was copied to the normalization transform.\n",
    "    \"\"\"\n",
    "    \n",
    "    if data_loader is None:\n",
    "        \"\"\"\n",
    "        Returns the mean and standard deviation used by the pretrained\n",
    "        classification models.\n",
    "        \"\"\"\n",
    "\n",
    "        mean = [0.485, 0.456, 0.406] \n",
    "        std = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    else:\n",
    "        \"\"\"\n",
    "        Computes the mean and standard deviation of the images returned\n",
    "        by the specified data loader. \n",
    "        \n",
    "        For comparision, the mean and standard deviation of the KenyanFood13\n",
    "        images using the train_dataset and preprocess transforms is as follows.\n",
    "        \n",
    "            mean = [0.5778, 0.4631, 0.3471], \n",
    "            std = [0.2380, 0.2461, 0.2464]):\n",
    "        \"\"\"\n",
    "        \n",
    "        std = 0.\n",
    "        mean = 0.\n",
    "        for images, _ in data_loader:\n",
    "            batch_samples = images.size(0)\n",
    "            images = images.view(batch_samples, images.size(1), -1)\n",
    "            std += images.std(2).sum(0)\n",
    "            mean += images.mean(2).sum(0)\n",
    "        std /= len(data_loader.dataset)\n",
    "        mean /= len(data_loader.dataset)\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gnyICYmG-U-B"
   },
   "outputs": [],
   "source": [
    "class ImageTransforms:\n",
    "    \"\"\"\n",
    "    This utility class has methods to create transforms used to train and evaluate a model as\n",
    "    well as visualize images.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            resize = 256, \n",
    "            crop_size = 224, \n",
    "            mean = [0.485, 0.456, 0.406], \n",
    "            std = [0.229, 0.224, 0.225],\n",
    "            config = DataAugConfig()\n",
    "        ):\n",
    "        self.__resize = resize\n",
    "        self.__crop_size = crop_size\n",
    "        self.__mean = mean\n",
    "        self.__std = std\n",
    "        self.__config = config\n",
    "\n",
    "    def preprocess(self, augment=False):\n",
    "        \"\"\"\n",
    "        These transformations convert PIL images to uniformly sized tensors whose dimensions\n",
    "        are crop_size x crop_size pixels. If the augment parameter is True, then the following\n",
    "        data augmentation transforms are applied: color jitter, horizontal flip, vertical flip,\n",
    "        rotation, translation, scaling, and erasing.\n",
    "        \"\"\"\n",
    "        return transforms.Compose(self.__create_transform_list(normalize=False, augment=augment))\n",
    "    \n",
    "    def common(self):\n",
    "        \"\"\"\n",
    "        These transformations convert PIL images to uniformly sized tensors whose dimensions\n",
    "        are crop_size x crop_size pixels and values are normalized by the mean and standard\n",
    "        deviation.\n",
    "        \"\"\"\n",
    "        return transforms.Compose(self.__create_transform_list(normalize=True, augment=False))\n",
    "    \n",
    "    def augment(self):\n",
    "        \"\"\"\n",
    "        These transformations convert PIL images to uniformly sized tensors whose dimensions\n",
    "        are crop_size x crop_size pixels and values are normalized by the mean and standard\n",
    "        deviation with the following data random augmentations: color jitter, horizontal flip,\n",
    "        vertical flip, rotation, translation, scaling, and erasing.\n",
    "        \"\"\"\n",
    "        return transforms.Compose(self.__create_transform_list(normalize=True, augment=True))\n",
    "\n",
    "    def __create_transform_list(self, normalize, augment):\n",
    "        tlist = []\n",
    "\n",
    "        # resize before data augmentation to reduce execution time\n",
    "        tlist.append(transforms.Resize(\n",
    "            size = self.__resize, \n",
    "            interpolation = PIL.Image.BILINEAR\n",
    "        ))\n",
    "\n",
    "        if augment:\n",
    "            # apply rotation before center cropping to avoid \"corner voids\"\n",
    "            tlist.extend(self.__get_color_jitter())\n",
    "            tlist.extend(self.__get_random_vertical_flip())\n",
    "            tlist.extend(self.__get_random_horizontal_flip())\n",
    "            tlist.extend(self.__get_random_affine())\n",
    "\n",
    "        tlist.append(transforms.CenterCrop(self.__crop_size))\n",
    "        tlist.append(transforms.ToTensor())\n",
    "\n",
    "        if normalize:\n",
    "            tlist.append(transforms.Normalize(self.__mean, self.__std, inplace=True))\n",
    "\n",
    "        if augment:\n",
    "            tlist.extend(self.__get_random_erasing())\n",
    "\n",
    "        return tlist\n",
    "\n",
    "    def __get_color_jitter(self):\n",
    "        tlist = []\n",
    "        if self.__config.color_enabled:\n",
    "            tlist.append(transforms.ColorJitter(\n",
    "                brightness = self.__config.color_brightness, \n",
    "                contrast = self.__config.color_contrast, \n",
    "                saturation = self.__config.color_saturation, \n",
    "                hue = self.__config.color_hue\n",
    "            ))\n",
    "        return tlist\n",
    "\n",
    "    def __get_random_vertical_flip(self):\n",
    "        tlist = []\n",
    "        if self.__config.vert_flip_prob > 0:\n",
    "            tlist.append(transforms.RandomVerticalFlip(\n",
    "                p=self.__config.vert_flip_prob\n",
    "            ))\n",
    "        return tlist \n",
    "\n",
    "    def __get_random_horizontal_flip(self):\n",
    "        tlist = []\n",
    "        if self.__config.horz_flip_prob > 0:\n",
    "            tlist.append(transforms.RandomHorizontalFlip(\n",
    "                p=self.__config.horz_flip_prob\n",
    "            ))\n",
    "        return tlist \n",
    "\n",
    "    def __get_random_affine(self):\n",
    "        tlist = []\n",
    "        if self.__config.affine_enabled:\n",
    "            tlist.append(transforms.RandomAffine(\n",
    "                degrees = self.__config.affine_rotation,\n",
    "                translate = self.__config.affine_translate,\n",
    "                scale = self.__config.affine_scale,\n",
    "                resample=PIL.Image.BILINEAR\n",
    "            ))\n",
    "        return tlist\n",
    "\n",
    "    def __get_random_erasing(self):\n",
    "        tlist = []\n",
    "        if self.__config.erasing_prob > 0:\n",
    "            tlist.append(transforms.RandomErasing(\n",
    "                p = self.__config.erasing_prob,\n",
    "                scale = self.__config.erasing_scale,\n",
    "                ratio = self.__config.erasing_ratio,\n",
    "                inplace = True\n",
    "            ))\n",
    "        return tlist "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SaqNIzTm3-5z"
   },
   "source": [
    "## <font style=\"color:green\">2. Configuration [5 Points]</font>\n",
    "\n",
    "Define your configuration in this section.\n",
    "\n",
    "For example,\n",
    "\n",
    "```\n",
    "@dataclass\n",
    "class TrainingConfiguration:\n",
    "    '''\n",
    "    Describes configuration of the training process\n",
    "    '''\n",
    "    batch_size: int = 10 \n",
    "    epochs_count: int = 50  \n",
    "    init_learning_rate: float = 0.1  # initial learning rate for lr scheduler\n",
    "    log_interval: int = 5  \n",
    "    test_interval: int = 1  \n",
    "    data_root: str = \"/kaggle/input/pytorch-opencv-course-classification/\" \n",
    "    num_workers: int = 2  \n",
    "    device: str = 'cuda'  \n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__6DBlJp3-5z"
   },
   "source": [
    "## <font style=\"color:blue\">Assignment Response</font>\n",
    "\n",
    "Since I am using the **trainer** module, I made minor modifications to the <u>configuration.py</u> file. In addition, I created a master _MasterConfig_ data class that encapsulates the individual configuration data classes. Lastly, I created helper functions to instantiate the _MasterConfig_ class with experiment-specific overrides.\n",
    "\n",
    "The following is the output of the `create_master_config` method w/o any parameter overrides.\n",
    "```\n",
    "MasterConfig(\n",
    "    system=SystemConfig(\n",
    "        proj_dir='./project2',\n",
    "        seed=42,\n",
    "        cudnn_deterministic=True,\n",
    "        cudnn_benchmark_enabled=False\n",
    "    ),\n",
    "    data_aug=DataAugConfig(\n",
    "        color_enabled=True,\n",
    "        color_brightness=(0.85, 1.15),\n",
    "        color_contrast=(0.5, 1.5),\n",
    "        color_saturation=(0.5, 2.0),\n",
    "        color_hue=(-0.03, 0.03),\n",
    "        horz_flip_prob=0.5,\n",
    "        vert_flip_prob=0.5,\n",
    "        affine_enabled=True,\n",
    "        affine_rotation=45,\n",
    "        affine_translate=(0.1, 0.1),\n",
    "        affine_scale=(0.9, 1.1),\n",
    "        erasing_prob=0.5,\n",
    "        erasing_scale=(0.02, 0.33),\n",
    "        erasing_ratio=(0.3, 3.3)\n",
    "    ),\n",
    "    dataset=DatasetConfig(\n",
    "        data_dir='./project2/data',\n",
    "        valid_size=0.2,\n",
    "        train_transforms=Compose(\n",
    "            Resize(size=256, interpolation=PIL.Image.BILINEAR)\n",
    "            ColorJitter(brightness=(0.85, 1.15), contrast=(0.5, 1.5), saturation=(0.5, 2.0), hue=(-0.03, 0.03))\n",
    "            RandomVerticalFlip(p=0.5)\n",
    "            RandomHorizontalFlip(p=0.5)\n",
    "            RandomAffine(degrees=[-45.0, 45.0], translate=(0.1, 0.1), scale=(0.9, 1.1), resample=PIL.Image.BILINEAR)\n",
    "            CenterCrop(size=(224, 224))\n",
    "            ToTensor()\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            RandomErasing()\n",
    "        ),\n",
    "        test_transforms=Compose(\n",
    "            Resize(size=256, interpolation=PIL.Image.BILINEAR)\n",
    "            CenterCrop(size=(224, 224))\n",
    "            ToTensor()\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ),\n",
    "        visual_transforms=Compose(\n",
    "            Resize(size=256, interpolation=PIL.Image.BILINEAR)\n",
    "            CenterCrop(size=(224, 224))\n",
    "            ToTensor()\n",
    "        ),\n",
    "        visual_aug_transforms=Compose(\n",
    "            Resize(size=256, interpolation=PIL.Image.BILINEAR)\n",
    "            ColorJitter(brightness=(0.85, 1.15), contrast=(0.5, 1.5), saturation=(0.5, 2.0), hue=(-0.03, 0.03))\n",
    "            RandomVerticalFlip(p=0.5)\n",
    "            RandomHorizontalFlip(p=0.5)\n",
    "            RandomAffine(degrees=[-45.0, 45.0], translate=(0.1, 0.1), scale=(0.9, 1.1), resample=PIL.Image.BILINEAR)\n",
    "            CenterCrop(size=(224, 224))\n",
    "            ToTensor()\n",
    "            RandomErasing()\n",
    "        )\n",
    "    ),\n",
    "    data_loader=DataLoaderConfig(\n",
    "        batch_size=32,\n",
    "        num_workers=4\n",
    "    ),\n",
    "    optimizer=OptimizerConfig(\n",
    "        learning_rate=0.001,\n",
    "        momentum=0.9,\n",
    "        weight_decay=0.0001,\n",
    "        betas=(0.9, 0.999)\n",
    "    ),\n",
    "    scheduler=SchedulerConfig(\n",
    "        gamma=0.1,\n",
    "        step_size=10,\n",
    "        milestones=(20, 30, 40),\n",
    "        patience=10,\n",
    "        threshold=0.0001\n",
    "    ),\n",
    "    trainer=TrainerConfig(\n",
    "        device='cuda',\n",
    "        training_epochs=50,\n",
    "        weighted_loss_fn=True,\n",
    "        progress_bar=True,\n",
    "        model_dir='models',\n",
    "        model_saving_period=0,\n",
    "        visualizer_dir='runs',\n",
    "        stop_loss_epochs=0,\n",
    "        stop_acc_epochs=0,\n",
    "        stop_acc_ema_alpha=0.3,\n",
    "        stop_acc_threshold=2.0\n",
    "    )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_system_config() -> SystemConfig:\n",
    "    return SystemConfig(\n",
    "        proj_dir = proj_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_aug_config(\n",
    "    color_enabled: Optional[bool] = None,\n",
    "    color_brightness: Optional[Tuple[float, float]] = None,\n",
    "    color_contrast: Optional[Tuple[float, float]] = None,\n",
    "    color_saturation: Optional[Tuple[float, float]] = None,\n",
    "    color_hue: Optional[Tuple[float, float]] = None,\n",
    "    horz_flip_prob: Optional[float] = None,\n",
    "    vert_flip_prob: Optional[float] = None,\n",
    "    affine_enabled: Optional[bool] = None,\n",
    "    affine_rotation: Optional[float] = None,\n",
    "    affine_translate: Optional[Tuple[float, float]] = None,\n",
    "    affine_scale: Optional[Tuple[float, float]] = None,\n",
    "    erasing_prob: Optional[float] = None,\n",
    "    erasing_scale: Optional[Tuple[float, float]] = None,\n",
    "    erasing_ratio: Optional[Tuple[float, float]] = None\n",
    ") -> DataAugConfig:\n",
    "    config = DataAugConfig()\n",
    "    if color_enabled is None:\n",
    "        color_enabled = config.color_enabled\n",
    "    if color_brightness is None:\n",
    "        color_brightness = config.color_brightness\n",
    "    if color_contrast is None:\n",
    "        color_contrast = config.color_contrast\n",
    "    if color_saturation is None:\n",
    "        color_saturation = config.color_saturation\n",
    "    if color_hue is None:\n",
    "        color_hue = config.color_hue\n",
    "    if horz_flip_prob is None:\n",
    "        horz_flip_prob = config.horz_flip_prob\n",
    "    if vert_flip_prob is None:\n",
    "        vert_flip_prob = config.vert_flip_prob\n",
    "    if affine_enabled is None:\n",
    "        affine_enabled = config.affine_enabled\n",
    "    if affine_rotation is None:\n",
    "        affine_rotation = config.affine_rotation\n",
    "    if affine_translate is None:\n",
    "        affine_translate = config.affine_translate\n",
    "    if affine_scale is None:\n",
    "        affine_scale = config.affine_scale\n",
    "    if erasing_prob is None:\n",
    "        erasing_prob = config.erasing_prob\n",
    "    if erasing_scale is None:\n",
    "        erasing_scale = config.erasing_scale\n",
    "    if erasing_ratio is None:\n",
    "        erasing_ratio = config.erasing_ratio\n",
    "    return DataAugConfig(\n",
    "        color_enabled = color_enabled,\n",
    "        color_brightness = color_brightness,\n",
    "        color_contrast = color_contrast,\n",
    "        color_saturation = color_saturation,\n",
    "        color_hue = color_hue,\n",
    "        horz_flip_prob = horz_flip_prob,\n",
    "        vert_flip_prob = vert_flip_prob,\n",
    "        affine_enabled = affine_enabled,\n",
    "        affine_rotation = affine_rotation,\n",
    "        affine_translate = affine_translate,\n",
    "        affine_scale = affine_scale,\n",
    "        erasing_prob = erasing_prob,\n",
    "        erasing_scale = erasing_scale,\n",
    "        erasing_ratio = erasing_ratio\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_config(\n",
    "    resize: int = 256, \n",
    "    crop_size: int = 224,\n",
    "    data_aug_config = DataAugConfig()\n",
    ") -> DatasetConfig:\n",
    "    mean, std = get_mean_std()\n",
    "    transforms = ImageTransforms(\n",
    "        resize = resize, \n",
    "        crop_size = crop_size, \n",
    "        mean = mean, \n",
    "        std = std,\n",
    "        config = data_aug_config\n",
    "    )\n",
    "    return DatasetConfig(\n",
    "        data_dir = data_dir,\n",
    "        test_transforms = transforms.common(),\n",
    "        train_transforms = transforms.augment(),\n",
    "        visual_transforms = transforms.preprocess(augment=False),\n",
    "        visual_aug_transforms = transforms.preprocess(augment=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader_config(\n",
    "    batch_size: Optional[int] = None, \n",
    "    num_workers: Optional[int] = None\n",
    ") -> DataLoaderConfig:\n",
    "    config = DataLoaderConfig()\n",
    "    if batch_size is None:\n",
    "        batch_size = config.batch_size\n",
    "    if num_workers is None:\n",
    "        num_workers = config.num_workers\n",
    "    return DataLoaderConfig(\n",
    "        batch_size = batch_size,\n",
    "        num_workers = num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer_config(\n",
    "    learning_rate: Optional[float] = None, \n",
    "    momentum: Optional[float] = None, \n",
    "    weight_decay: Optional[float] = None,\n",
    "    betas: Optional[Tuple[float, float]] = None\n",
    ") -> OptimizerConfig():\n",
    "    config = OptimizerConfig()\n",
    "    if learning_rate is None:\n",
    "        learning_rate = config.learning_rate\n",
    "    if momentum is None:\n",
    "        momentum = config.momentum\n",
    "    if weight_decay is None:\n",
    "        weight_decay = config.weight_decay\n",
    "    if betas is None:\n",
    "        betas = config.betas\n",
    "    return OptimizerConfig(\n",
    "        learning_rate = learning_rate,\n",
    "        momentum = momentum,\n",
    "        weight_decay = weight_decay,\n",
    "        betas = betas\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scheduler_config(\n",
    "    gamma: Optional[float] = None,\n",
    "    step_size: Optional[int] = None,\n",
    "    milestones: Optional[Iterable] = None,\n",
    "    patience: Optional[int] = None,\n",
    "    threshold: Optional[float] = None\n",
    ") -> SchedulerConfig:\n",
    "    config = SchedulerConfig()\n",
    "    if gamma is None:\n",
    "        gamma = config.gamma\n",
    "    if step_size is None:\n",
    "        step_size = config.step_size\n",
    "    if milestones is None:\n",
    "        milestones = config.milestones\n",
    "    if patience is None:\n",
    "        patience = config.patience\n",
    "    if threshold is None:\n",
    "        threshold = config.threshold\n",
    "    return SchedulerConfig(\n",
    "        gamma = gamma,\n",
    "        step_size = step_size,\n",
    "        milestones = milestones,\n",
    "        patience = patience,\n",
    "        threshold = threshold\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainer_config(\n",
    "    training_epochs: Optional[int] = None,\n",
    "    weighted_loss_fn: Optional[bool] = None,\n",
    "    model_saving_period: Optional[int] = None,\n",
    "    stop_loss_epochs: Optional[int] = None,\n",
    "    stop_acc_epochs: Optional[int] = None, \n",
    "    stop_acc_ema_alpha: Optional[float] = None,\n",
    "    stop_acc_threshold: Optional[float] = None\n",
    ") -> TrainerConfig:\n",
    "    config = TrainerConfig()\n",
    "    if training_epochs is None:\n",
    "        training_epochs = config.training_epochs\n",
    "    if weighted_loss_fn is None:\n",
    "        weighted_loss_fn = config.weighted_loss_fn\n",
    "    if model_saving_period is None:\n",
    "        model_saving_period = config.model_saving_period\n",
    "    if stop_loss_epochs is None:\n",
    "        stop_loss_epochs = config.stop_loss_epochs\n",
    "    if stop_acc_epochs is None:\n",
    "        stop_acc_epochs = config.stop_acc_epochs\n",
    "    if stop_acc_ema_alpha is None:\n",
    "        stop_acc_ema_alpha = config.stop_acc_ema_alpha\n",
    "    if stop_acc_threshold is None:\n",
    "        stop_acc_threshold = config.stop_acc_threshold\n",
    "    return TrainerConfig(\n",
    "        training_epochs = training_epochs,\n",
    "        weighted_loss_fn = weighted_loss_fn,\n",
    "        model_saving_period = model_saving_period,\n",
    "        stop_loss_epochs = stop_loss_epochs,\n",
    "        stop_acc_epochs = stop_acc_epochs,\n",
    "        stop_acc_ema_alpha = stop_acc_ema_alpha,\n",
    "        stop_acc_threshold = stop_acc_threshold\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hi4_VFrx3-50"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MasterConfig:\n",
    "    system: SystemConfig = create_system_config()\n",
    "    data_aug: DataAugConfig = create_data_aug_config()\n",
    "    dataset: DatasetConfig = create_dataset_config()\n",
    "    data_loader: DataLoaderConfig = create_data_loader_config()\n",
    "    optimizer: OptimizerConfig = create_optimizer_config()\n",
    "    scheduler: SchedulerConfig = create_scheduler_config()\n",
    "    trainer: TrainerConfig = create_trainer_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_HPBi-DL-U-C"
   },
   "outputs": [],
   "source": [
    "def create_master_config(\n",
    "    transform_resize: int = 256,\n",
    "    transform_crop_size: int = 224,\n",
    "    data_aug_color_enabled: Optional[bool] = None,\n",
    "    data_aug_color_brightness: Optional[Tuple[float, float]] = None,\n",
    "    data_aug_color_contrast: Optional[Tuple[float, float]] = None,\n",
    "    data_aug_color_saturation: Optional[Tuple[float, float]] = None,\n",
    "    data_aug_color_hue: Optional[Tuple[float, float]] = None,\n",
    "    data_aug_horz_flip_prob: Optional[float] = None,\n",
    "    data_aug_vert_flip_prob: Optional[float] = None,\n",
    "    data_aug_affine_enabled: Optional[bool] = None,\n",
    "    data_aug_affine_rotation: Optional[float] = None,\n",
    "    data_aug_affine_translate: Optional[Tuple[float, float]] = None,\n",
    "    data_aug_affine_scale: Optional[Tuple[float, float]] = None,\n",
    "    data_aug_erasing_prob: Optional[float] = None,\n",
    "    data_aug_erasing_scale: Optional[Tuple[float, float]] = None,\n",
    "    data_aug_erasing_ratio: Optional[Tuple[float, float]] = None,\n",
    "    data_loader_batch_size: Optional[int] = None,\n",
    "    data_loader_num_workers: Optional[int] = None,\n",
    "    optimizer_learning_rate: Optional[float] = None,\n",
    "    optimizer_momentum: Optional[float] = None,\n",
    "    optimizer_weight_decay: Optional[float] = None,\n",
    "    optimizer_betas: Optional[Tuple[float, float]] = None,\n",
    "    lr_scheduler_gamma: Optional[float] = None,\n",
    "    lr_scheduler_step_size: Optional[int] = None,\n",
    "    lr_scheduler_milestones: Optional[Iterable] = None,\n",
    "    lr_scheduler_patience: Optional[int] = None,\n",
    "    lr_scheduler_threshold: Optional[float] = None,\n",
    "    trainer_training_epochs: Optional[int] = None,\n",
    "    trainer_weighted_loss_fn: Optional[bool] = None,\n",
    "    trainer_model_saving_period: Optional[int] = None,\n",
    "    trainer_stop_loss_epochs: Optional[int] = None,\n",
    "    trainer_stop_acc_epochs: Optional[int] = None,\n",
    "    trainer_stop_acc_ema_alpha: Optional[float] = None,\n",
    "    trainer_stop_acc_threshold: Optional[float] = None       \n",
    ") -> MasterConfig:\n",
    "    # used to initialize MasterConfig data class and as a parameter to the\n",
    "    # create_data_config function\n",
    "    data_aug_config = create_data_aug_config(\n",
    "        data_aug_color_enabled,\n",
    "        data_aug_color_brightness,\n",
    "        data_aug_color_contrast,\n",
    "        data_aug_color_saturation,\n",
    "        data_aug_color_hue,\n",
    "        data_aug_horz_flip_prob,\n",
    "        data_aug_vert_flip_prob,\n",
    "        data_aug_affine_enabled,\n",
    "        data_aug_affine_rotation,\n",
    "        data_aug_affine_translate,\n",
    "        data_aug_affine_scale,\n",
    "        data_aug_erasing_prob,\n",
    "        data_aug_erasing_scale,\n",
    "        data_aug_erasing_ratio\n",
    "    )\n",
    "    return MasterConfig(\n",
    "        system = create_system_config(),\n",
    "        data_aug = data_aug_config,\n",
    "        dataset = create_dataset_config(\n",
    "            transform_resize,\n",
    "            transform_crop_size,\n",
    "            data_aug_config\n",
    "        ),\n",
    "        data_loader = create_data_loader_config(\n",
    "            data_loader_batch_size,\n",
    "            data_loader_num_workers\n",
    "        ),\n",
    "        optimizer = create_optimizer_config(\n",
    "            optimizer_learning_rate,\n",
    "            optimizer_momentum,\n",
    "            optimizer_weight_decay,\n",
    "            optimizer_betas\n",
    "        ),\n",
    "        scheduler = create_scheduler_config(\n",
    "            lr_scheduler_gamma,\n",
    "            lr_scheduler_step_size,\n",
    "            lr_scheduler_milestones,\n",
    "            lr_scheduler_patience,\n",
    "            lr_scheduler_threshold   \n",
    "        ),\n",
    "        trainer = create_trainer_config(\n",
    "            trainer_training_epochs,\n",
    "            trainer_weighted_loss_fn,\n",
    "            trainer_model_saving_period,\n",
    "            trainer_stop_loss_epochs,\n",
    "            trainer_stop_acc_epochs,\n",
    "            trainer_stop_acc_ema_alpha,\n",
    "            trainer_stop_acc_threshold       \n",
    "        )\n",
    "    )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYVGcNyX3-50"
   },
   "source": [
    "## <font style=\"color:green\">3. Evaluation Metric [10 Points]</font>\n",
    "\n",
    "Define methods or classes that will be used in model evaluation, for example, accuracy, f1-score, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJg9CPFB3-50"
   },
   "source": [
    "### Loss Function\n",
    "\n",
    "The number of images per class are slightly imbalanced. The most represented class, chapati, has approximately five times more images than the least represented class, kukuchoma. Consequently, I will explore weighted and non-weighted cross-entropy loss functions. I will use `nn.CrossEntropyLoss()` with and without passing a tensor of rescaling weights. The computation of the rescaling weights is described below.\n",
    "\n",
    "The number of images per class were obtained via the following code.\n",
    "```\n",
    "    config = create_master_config()\n",
    "    setup_system(config.system)       \n",
    "    data = KenyanFood13Data(\n",
    "        data_root = config.dataset.data_dir,\n",
    "        valid_size = config.dataset.valid_size,\n",
    "        random_seed = config.system.seed\n",
    "    )\n",
    "    images_per_class = np.column_stack((data.classes, data.class_counts))\n",
    "    print(images_per_class)\n",
    "\n",
    "    [['bhaji' 632]\n",
    "     ['chapati' 862]\n",
    "     ['githeri' 479]\n",
    "     ['kachumbari' 494]\n",
    "     ['kukuchoma' 173]\n",
    "     ['mandazi' 620]\n",
    "     ['masalachips' 438]\n",
    "     ['matoke' 483]\n",
    "     ['mukimo' 212]\n",
    "     ['nyamachoma' 784]\n",
    "     ['pilau' 329]\n",
    "     ['sukumawiki' 402]\n",
    "     ['ugali' 628]]\n",
    "```\n",
    "The normalized rescaling weights given to each class were obtained via the following code.\n",
    "```\n",
    "    weights = np.sum(data.class_counts) / data.class_counts\n",
    "    norm_weights = weights / np.sum(weights)\n",
    "    print(norm_weights)\n",
    "\n",
    "    [0.04989366 0.03658097 0.06583046 0.06383156 0.18227048 0.05085934\n",
    "     0.07199268 0.06528528 0.14873959 0.0402204  0.09584435 0.07843978\n",
    "     0.05021145]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qW__PU7F3-50"
   },
   "outputs": [],
   "source": [
    "loss_rescaling_weight = torch.tensor([\n",
    "    0.04989366, 0.03658097, 0.06583046, 0.06383156, 0.18227048,\n",
    "    0.05085934, 0.07199268, 0.06528528, 0.14873959, 0.04022040,\n",
    "    0.09584435, 0.07843978, 0.05021145,\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQjdvYWV3-50"
   },
   "source": [
    "### <font style=\"color:blue\">Metric Function</font>\n",
    "\n",
    "I am using the <b>trainer</b> module's <i>AccuracyEstimator</i> class from <u>metrics.py</u> file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sSIXHXg3-51"
   },
   "source": [
    "## <font style=\"color:green\">4. Train and Validation [5 Points]</font>\n",
    "\n",
    "Write the methods or classes that will be used for training and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhF41hAH3-51"
   },
   "source": [
    "## Assignment Response\n",
    "\n",
    "Since I am using the **trainer** module, I made the following modifications to the `trainer.py` file.\n",
    "* Added the ability to save the model only when the test loss reaches a new minimum.\n",
    "* Added the ability to terminate training after a specified number of epochs where the test loss is not further reduced.\n",
    "* Added the ability to terminate training after a specified number of epochs where the exponential moving average of the test loss does not significantly increase.\n",
    "\n",
    "I made the following modifications to the `visualizer.py` and `tensorboard_visualizer.py` files.\n",
    "* Added an <code>add_image(self, tag, image)</code> method to visualize the dataset.\n",
    "* Added an <code>add_figure(elf, tag, figure, close=True)</code> method to visulize matplotlib figures, e.g., confusion matrices.\n",
    "* Added an <code>add_graph(self, model, images)</code> method to document the model.\n",
    "* Added an <code>add_pr_curves(self, classes, pred_probs, targets)</code> method to document the precision-recall curves of the fully trained model for each class type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mxi77QT13-51"
   },
   "outputs": [],
   "source": [
    "class Optimizer(Enum):\n",
    "    SGD = auto()\n",
    "    ADAM = auto()\n",
    "    \n",
    "def get_optimizer(\n",
    "    model: nn.Module,\n",
    "    optimizer: Optimizer = Optimizer.SGD,\n",
    "    config: OptimizerConfig = OptimizerConfig()\n",
    "):\n",
    "    \"\"\"\n",
    "    Gets the specified optimizer.\n",
    "    \"\"\"\n",
    "    \n",
    "    if optimizer == Optimizer.SGD:\n",
    "        return optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr = config.learning_rate,\n",
    "            weight_decay = config.weight_decay,\n",
    "            momentum = config.momentum\n",
    "        )\n",
    "    \n",
    "    elif optimizer == Optimizer.ADAM:\n",
    "        return optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr = config.learning_rate,\n",
    "            betas = config.betas\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        raise SystemExit(\"Invalid lr_scheduler value.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ji0-7_V_3-51"
   },
   "outputs": [],
   "source": [
    "class LrScheduler(Enum):\n",
    "    STEP = auto()\n",
    "    MULTI_STEP = auto()\n",
    "    EXPONENTIAL = auto()\n",
    "    REDUCE_ON_PLATEAU = auto()\n",
    "    \n",
    "def get_lr_scheduler(\n",
    "    optimizer: optim.Optimizer,\n",
    "    lr_scheduler: LrScheduler = LrScheduler.STEP,\n",
    "    config: SchedulerConfig = SchedulerConfig()\n",
    "):\n",
    "    \"\"\"\n",
    "    Gets the specified LR scheduler.\n",
    "    \"\"\"\n",
    "\n",
    "    if lr_scheduler == LrScheduler.STEP:\n",
    "        return optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size = config.step_size,\n",
    "            gamma = config.gamma\n",
    "        )\n",
    "    \n",
    "    elif lr_scheduler == LrScheduler.MULTI_STEP:\n",
    "        return optim.lr_scheduler.MultiStepLR(\n",
    "            optimizer, \n",
    "            milestones = config.milestones, \n",
    "            gamma = config.gamma\n",
    "        )\n",
    "    \n",
    "    elif lr_scheduler == LrScheduler.EXPONENTIAL:\n",
    "        return optim.lr_scheduler.ExponentialLR(\n",
    "            optimizer, \n",
    "            gamma = config.gamma\n",
    "        )\n",
    "    \n",
    "    \n",
    "    elif lr_scheduler == LrScheduler.REDUCE_ON_PLATEAU:\n",
    "        return optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, \n",
    "            factor = config.gamma,\n",
    "            patience = config.patience,\n",
    "            threshold = config.threshold\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        raise SystemExit(\"Invalid lr_scheduler value.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DlaX2uwY3-51"
   },
   "outputs": [],
   "source": [
    "def predict_batch(model, data, max_prob=True):\n",
    "    \"\"\"\n",
    "    Get prediction for a batch of data. This function assumes the model and data\n",
    "    have be sent to the appropriate device and the model is in evaluation mode.\n",
    "    \"\"\"\n",
    "\n",
    "    output = model(data)\n",
    "\n",
    "    # get probability score using softmax\n",
    "    prob = F.softmax(output, dim=1)\n",
    "    \n",
    "    if max_prob:\n",
    "        # get the max probability\n",
    "        pred_prob = prob.data.max(dim=1)[0]\n",
    "    else:\n",
    "        # return all probabilties\n",
    "        pred_prob = prob.data\n",
    "    \n",
    "    # get the index of the max probability\n",
    "    pred_index = prob.data.max(dim=1)[1]\n",
    "    \n",
    "    return pred_index.cpu().numpy(), pred_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MtIeeYLzasi-"
   },
   "outputs": [],
   "source": [
    "def get_targets_and_pred_probs(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Get targets and prediction probabilities.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.to(device)  # send model to cpu or cuda\n",
    "    model.eval()      # set model to evaluation mode\n",
    "\n",
    "    targets = []\n",
    "    pred_probs = []\n",
    "\n",
    "    for _, (data, target) in enumerate(dataloader):\n",
    "        _, probs = predict_batch(model, data.to(device), max_prob=False)       \n",
    "        pred_probs.append(probs)\n",
    "        targets.append(target.numpy())\n",
    "        \n",
    "    targets = np.concatenate(targets).astype(int)\n",
    "    pred_probs = np.concatenate(pred_probs, axis=0)\n",
    "    \n",
    "    return targets, pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FezIMZnRvPUZ"
   },
   "outputs": [],
   "source": [
    "def predict_test_data(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Predict the class of the test data.\n",
    "    \"\"\"\n",
    "\n",
    "    model.to(device)  # send model to cpu or cuda\n",
    "    model.eval()      # set model to evaluation mode\n",
    "\n",
    "    fnames = []\n",
    "    preds = []\n",
    "\n",
    "    for _, (data, fname) in enumerate(dataloader):\n",
    "        pred, _ = predict_batch(model, data.to(device), max_prob=True)       \n",
    "        fnames.append(fname)\n",
    "        preds.append(pred)\n",
    "        \n",
    "    fnames = np.concatenate(fnames)\n",
    "    preds = np.concatenate(preds).astype(int)\n",
    "    \n",
    "    return fnames, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_valid_data(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Predict the class of the validation data.\n",
    "    \"\"\"\n",
    "\n",
    "    model.to(device)  # send model to cpu or cuda\n",
    "    model.eval()      # set model to evaluation mode\n",
    "\n",
    "    targets = []\n",
    "    preds = []\n",
    "\n",
    "    for _, (data, target) in enumerate(dataloader):\n",
    "        pred, _ = predict_batch(model, data.to(device), max_prob=True)       \n",
    "        targets.append(target)\n",
    "        preds.append(pred)\n",
    "        \n",
    "    targets = np.concatenate(targets)\n",
    "    preds = np.concatenate(preds).astype(int)\n",
    "    \n",
    "    return targets, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajr4pCOz3-51"
   },
   "source": [
    "## <font style=\"color:green\">5. Model [5 Points]</font>\n",
    "\n",
    "Define your model in this section.\n",
    "\n",
    "**You are allowed to use any pre-trained model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02C_Nwdo7SBK"
   },
   "source": [
    "## Assignment Response\n",
    "\n",
    "Since my approach is to explore transfer learning in numerous pretrained models, I created classes\n",
    "to easily set the \"tuning level\" of the ResNet, VGG, and DenseNet family of TorchVision models. I\n",
    "also want to see how the model I developed for Project 1 performs, so I created as a class for it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TuningParam = namedtuple(\"TuningParam\", [\"level\", \"block\", \"layers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchVisionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Base class for TorchVision models, which provides a method to freeze network\n",
    "    layers allowing fine tuning. This class does change the network's output layer.\n",
    "    Derived classes must do this!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, network: nn.Module):\n",
    "        super().__init__()\n",
    "        self._network = network\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self._network(x)\n",
    "    \n",
    "    def _freeze_layers(\n",
    "        self, \n",
    "        tuning_params: List[TuningParam], \n",
    "        pretrained:bool, \n",
    "        tuning_level:int\n",
    "    ):\n",
    "        # freeze network if using a pretrained model\n",
    "        if pretrained:\n",
    "            self._set_requires_grad(self._network, False)\n",
    "        \n",
    "        # unfreeze blocks/layers based on tuning_level\n",
    "        for param in tuning_params:\n",
    "            if param.level <= tuning_level:\n",
    "                block = getattr(self._network, param.block)\n",
    "                if param.layers is None:\n",
    "                    self._set_requires_grad(block, True)\n",
    "                else:\n",
    "                    for layer in param.layers:\n",
    "                        if isinstance(layer, int):\n",
    "                            self._set_requires_grad(block[layer], True)\n",
    "                        else:\n",
    "                            self._set_requires_grad(getattr(block, layer), True)\n",
    "            \n",
    "    def _set_requires_grad(self, block, value):\n",
    "        for param in block.parameters():\n",
    "            param.requires_grad = value\n",
    "            \n",
    "    def _inclusive_range(self, start:int, stop:int) -> List[int]:\n",
    "        return list(range(start, stop + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DonDtN5MBBU1"
   },
   "outputs": [],
   "source": [
    "class ResNetBase(TorchVisionModel):\n",
    "    \"\"\"\n",
    "    Base class for ResNet models that may be pretrained and fine tuned. The\n",
    "    tuning_level parameter controls the degree of fine tuning as depicted in\n",
    "    the table below.\n",
    "        \n",
    "        ResNet     tuning_level\n",
    "        -------    ------------\n",
    "        conv1          >= 5        \n",
    "        bn1            >= 5\n",
    "        relu           >= 5\n",
    "        maxpool        >= 5\n",
    "        layer1         >= 4\n",
    "        layer2         >= 3\n",
    "        layer3         >= 2\n",
    "        layer4         >= 1\n",
    "        avgpool        >= 1\n",
    "        fc             >= 0\n",
    "        \n",
    "    If tuning_level = 0, then only the classifier layer is trained.\n",
    "    If tuning_level = 5, then the entire network is trained.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_fn: Callable, pretrained=True, tuning_level=0):\n",
    "        super().__init__(model_fn(pretrained=pretrained))\n",
    "\n",
    "        # change the output layer\n",
    "        last_layer_in = self._network.fc.in_features\n",
    "        self._network.fc = nn.Linear(last_layer_in, 13)\n",
    "\n",
    "        # ToDo: Omit layer types that do not have trainable parameters\n",
    "        tuning_params = [\n",
    "            TuningParam(0, \"fc\", None),\n",
    "            TuningParam(1, \"avgpool\", None),\n",
    "            TuningParam(1, \"layer4\", None),\n",
    "            TuningParam(2, \"layer3\", None),\n",
    "            TuningParam(3, \"layer2\", None),\n",
    "            TuningParam(4, \"layer1\", None),\n",
    "            TuningParam(5, \"maxpool\", None),\n",
    "            TuningParam(5, \"relu\", None),\n",
    "            TuningParam(5, \"bn1\", None),\n",
    "            TuningParam(5, \"conv1\", None)\n",
    "        ]\n",
    "\n",
    "        self._freeze_layers(tuning_params, pretrained, tuning_level)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self._network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDbYluEM3-52"
   },
   "outputs": [],
   "source": [
    "class ResNet18(ResNetBase):\n",
    "    def __init__(self, pretrained=True, tuning_level=0):\n",
    "        super().__init__(models.resnet18, pretrained, tuning_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xRrkXWSABBU2"
   },
   "outputs": [],
   "source": [
    "class ResNet34(ResNetBase):\n",
    "    def __init__(self, pretrained=True, tuning_level=0):\n",
    "        super().__init__(models.resnet34, pretrained, tuning_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qom1WSnGBBU2"
   },
   "outputs": [],
   "source": [
    "class ResNet50(ResNetBase):\n",
    "    def __init__(self, pretrained=True, tuning_level=0):\n",
    "        super().__init__(models.resnet50, pretrained, tuning_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dA-QcvSGBBU2"
   },
   "outputs": [],
   "source": [
    "class ResNet101(ResNetBase):\n",
    "    def __init__(self, pretrained=True, tuning_level=0):\n",
    "        super().__init__(models.resnet101, pretrained, tuning_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dA-QcvSGBBU2"
   },
   "outputs": [],
   "source": [
    "class ResNet152(ResNetBase):\n",
    "    def __init__(self, pretrained=True, tuning_level=0):\n",
    "        super().__init__(models.resnet152, pretrained, tuning_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wGTEW_FoBBU2"
   },
   "outputs": [],
   "source": [
    "class ResNeXt50(ResNetBase):\n",
    "    def __init__(self, pretrained=True, tuning_level=0):\n",
    "        super().__init__(models.resnext50_32x4d, pretrained, tuning_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1_tKL2_A7Am"
   },
   "outputs": [],
   "source": [
    "class ResNeXt101(ResNetBase):\n",
    "    def __init__(self, pretrained=True, tuning_level=0):\n",
    "        super().__init__(models.resnext101_32x8d, pretrained, tuning_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ucBgzJWvA8Rn"
   },
   "outputs": [],
   "source": [
    "class WideResNet50(ResNetBase):\n",
    "    def __init__(self, pretrained=True, tuning_level=0):\n",
    "        super().__init__(models.wide_resnet50_2, pretrained, tuning_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ShfGU7oCHvj"
   },
   "outputs": [],
   "source": [
    "class WideResNet101(ResNetBase):\n",
    "    def __init__(self, pretrained=True, tuning_level=0):\n",
    "        super().__init__(models.wide_resnet101_2, pretrained, tuning_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2e70wQoxzkgh"
   },
   "outputs": [],
   "source": [
    "class VGGBase(TorchVisionModel):\n",
    "    \"\"\"\n",
    "    Base class for ResNet models that may be pretrained and fine tuned.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_fn: Callable, pretrained=True):\n",
    "        super().__init__(model_fn(pretrained=pretrained))\n",
    "\n",
    "        last_layer_in = self._network.classifier[6].in_features\n",
    "        self._network.classifier[6] = nn.Linear(last_layer_in, 13)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self._network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xkG8GBX8207n"
   },
   "outputs": [],
   "source": [
    "class VGG11BN(VGGBase):\n",
    "    \"\"\"\n",
    "    VGG11BN model that may be pretrained and fine tuned. The tuning_level\n",
    "    parameter controls the degree of fine tuning as depicted in the table\n",
    "    below.\n",
    "    \n",
    "        VGG11_BN            tuning_level\n",
    "        ----------------    ------------\n",
    "        features\n",
    "          [00-02] CNR           >= 5\n",
    "          [03] MaxPool2d        >= 5\n",
    "          [04-06] CNR           >= 4\n",
    "          [07] MaxPool2d        >= 4\n",
    "          [08-10] CNR           >= 3\n",
    "          [11-13] CNR           >= 3\n",
    "          [14] MaxPool2d        >= 3\n",
    "          [15-17] CNR           >= 2\n",
    "          [18-20] CNR           >= 2\n",
    "          [21] MaxPool2d        >= 2\n",
    "          [22-24] CNR           >= 1\n",
    "          [25-27] CNR           >= 1\n",
    "          [28] MaxPool2d        >= 1\n",
    "        avgpool                 >= 1\n",
    "        classifier              \n",
    "          [00-02] LRD           >= 0\n",
    "          [03-05] LRD           >= 0\n",
    "          [06] Linear           >= 0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pretrained=True, tuning_level=0):\n",
    "        super().__init__(models.vgg11_bn, pretrained)\n",
    "            \n",
    "        # ToDo: Omit layer types that do not have trainable parameters\n",
    "        tuning_params = [\n",
    "            TuningParam(0, \"classifier\", None),\n",
    "            TuningParam(1, \"avgpool\", None),\n",
    "            TuningParam(1, \"features\", self._inclusive_range(22, 28)),\n",
    "            TuningParam(2, \"features\", self._inclusive_range(15, 21)),\n",
    "            TuningParam(3, \"features\", self._inclusive_range(8, 14)),\n",
    "            TuningParam(4, \"features\", self._inclusive_range(4, 7)),\n",
    "            TuningParam(5, \"features\", self._inclusive_range(0, 3))\n",
    "        ]\n",
    "\n",
    "        self._freeze_layers(tuning_params, pretrained, tuning_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7IyXWM521Jy"
   },
   "outputs": [],
   "source": [
    "class VGG13BN(VGGBase):\n",
    "    \"\"\"\n",
    "    VGG13BN model that may be pretrained and fine tuned. The tuning_level\n",
    "    parameter controls the degree of fine tuning as depicted in the table\n",
    "    below.\n",
    "    \n",
    "        VGG13_BN            tuning_level\n",
    "        ----------------    ------------\n",
    "        features\n",
    "          [00-02] CNR           >= 5\n",
    "          [03-05] CNR           >= 5\n",
    "          [06] MaxPool2d        >= 5\n",
    "          [07-09] CNR           >= 4\n",
    "          [10-12] CNR           >= 4\n",
    "          [13] MaxPool2d        >= 4\n",
    "          [14-16] CNR           >= 3\n",
    "          [17-19] CNR           >= 3\n",
    "          [20] MaxPool2d        >= 3\n",
    "          [21-23] CNR           >= 2\n",
    "          [24-26] CNR           >= 2\n",
    "          [27] MaxPool2d        >= 2\n",
    "          [28-30] CNR           >= 1\n",
    "          [31-33] CNR           >= 1\n",
    "          [34] MaxPool2d        >= 1\n",
    "        avgpool                 >= 1\n",
    "        classifier\n",
    "          [00-02] LRD           >= 0\n",
    "          [03-05] LRD           >= 0\n",
    "          [06] Linear           >= 0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pretrained=True, tuning_level=0):\n",
    "        super().__init__(models.vgg13_bn, pretrained)\n",
    "            \n",
    "        # ToDo: Omit layer types that do not have trainable parameters\n",
    "        tuning_params = [\n",
    "            TuningParam(0, \"classifier\", None),\n",
    "            TuningParam(1, \"avgpool\", None),\n",
    "            TuningParam(1, \"features\", self._inclusive_range(28, 34)),\n",
    "            TuningParam(2, \"features\", self._inclusive_range(21, 27)),\n",
    "            TuningParam(3, \"features\", self._inclusive_range(14, 20)),\n",
    "            TuningParam(4, \"features\", self._inclusive_range(7, 13)),\n",
    "            TuningParam(5, \"features\", self._inclusive_range(0, 6))\n",
    "        ]\n",
    "\n",
    "        self._freeze_layers(tuning_params, pretrained, tuning_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rvNj4Qb421UH"
   },
   "outputs": [],
   "source": [
    "class VGG16BN(VGGBase):\n",
    "    \"\"\"\n",
    "    VGG16BN model that may be pretrained and fine tuned. The tuning_level\n",
    "    parameter controls the degree of fine tuning as depicted in the table\n",
    "    below.\n",
    "    \n",
    "        VGG16_BN            tuning_level\n",
    "        ----------------    ------------\n",
    "        features\n",
    "          [00-02] CNR           >= 5\n",
    "          [03-05] CNR           >= 5\n",
    "          [06] MaxPool2d        >= 5\n",
    "          [07-09] CNR           >= 4\n",
    "          [10-12] CNR           >= 4\n",
    "          [13] MaxPool2d        >= 4\n",
    "          [14-16] CNR           >= 3\n",
    "          [17-19] CNR           >= 3\n",
    "          [20-22] CNR           >= 3\n",
    "          [23] MaxPool2d        >= 3\n",
    "          [24-26] CNR           >= 2\n",
    "          [27-29] CNR           >= 2\n",
    "          [30-32] CNR           >= 2\n",
    "          [33] MaxPool2d        >= 2\n",
    "          [34-36] CNR           >= 1\n",
    "          [37-39] CNR           >= 1\n",
    "          [40-42] CNR           >= 1\n",
    "          [43] MaxPool2d        >= 1\n",
    "        avgpool                 >= 1\n",
    "        classifier              \n",
    "          [00-02] LRD           >= 0\n",
    "          [03-05] LRD           >= 0\n",
    "          [06] Linear           >= 0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pretrained=True, tuning_level=0):\n",
    "        super().__init__(models.vgg16_bn, pretrained)\n",
    "            \n",
    "        # ToDo: Omit layer types that do not have trainable parameters\n",
    "        tuning_params = [\n",
    "            TuningParam(0, \"classifier\", None),\n",
    "            TuningParam(1, \"avgpool\", None),\n",
    "            TuningParam(1, \"features\", self._inclusive_range(34, 43)),\n",
    "            TuningParam(2, \"features\", self._inclusive_range(24, 33)),\n",
    "            TuningParam(3, \"features\", self._inclusive_range(14, 23)),\n",
    "            TuningParam(4, \"features\", self._inclusive_range(7, 13)),\n",
    "            TuningParam(5, \"features\", self._inclusive_range(0, 6))\n",
    "        ]\n",
    "\n",
    "        self._freeze_layers(tuning_params, pretrained, tuning_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckZqYccc21c3"
   },
   "outputs": [],
   "source": [
    "class VGG19BN(VGGBase):\n",
    "    \"\"\"\n",
    "    VGG19BN model that may be pretrained and fine tuned. The tuning_level\n",
    "    parameter controls the degree of fine tuning as depicted in the table\n",
    "    below.\n",
    "\n",
    "        VGG11_BN            tuning_level\n",
    "        ----------------    ------------\n",
    "        features\n",
    "          [00-02] CNR           >= 5\n",
    "          [03-05] CNR           >= 5\n",
    "          [06] MaxPool2d        >= 5\n",
    "          [07-09] CNR           >= 4\n",
    "          [10-12] CNR           >= 4\n",
    "          [13] MaxPool2d        >= 4\n",
    "          [14-16] CNR           >= 3\n",
    "          [17-19] CNR           >= 3\n",
    "          [20-22] CNR           >= 3\n",
    "          [23-25] CNR           >= 3\n",
    "          [26] MaxPool2d        >= 3\n",
    "          [27-29] CNR           >= 2\n",
    "          [30-32] CNR           >= 2\n",
    "          [33-35] CNR           >= 2\n",
    "          [36-38] CNR           >= 2\n",
    "          [39] MaxPool2d        >= 2\n",
    "          [40-42] CNR           >= 1\n",
    "          [43-45] CNR           >= 1\n",
    "          [46-48] CNR           >= 1\n",
    "          [49-51] CNR           >= 1\n",
    "          [52] MaxPool2d        >= 1\n",
    "        avgpool                 >= 1\n",
    "        classifier              \n",
    "          [00-02] LRD           >= 0\n",
    "          [03-05] LRD           >= 0\n",
    "          [06] Linear           >= 0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pretrained=True, tuning_level=0):\n",
    "        super().__init__(models.vgg19_bn, pretrained)\n",
    "            \n",
    "        # ToDo: Omit layer types that do not have trainable parameters\n",
    "        tuning_params = [\n",
    "            TuningParam(0, \"classifier\", None),\n",
    "            TuningParam(1, \"avgpool\", None),\n",
    "            TuningParam(1, \"features\", self._inclusive_range(40, 52)),\n",
    "            TuningParam(2, \"features\", self._inclusive_range(27, 39)),\n",
    "            TuningParam(3, \"features\", self._inclusive_range(14, 26)),\n",
    "            TuningParam(4, \"features\", self._inclusive_range(7, 13)),\n",
    "            TuningParam(5, \"features\", self._inclusive_range(0, 6))\n",
    "        ]\n",
    "\n",
    "        self._freeze_layers(tuning_params, pretrained, tuning_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DonDtN5MBBU1"
   },
   "outputs": [],
   "source": [
    "class DenseNetBase(TorchVisionModel):\n",
    "    \"\"\"\n",
    "    Base class for DenseNet models that may be pretrained and fine tuned. The\n",
    "    tuning_level parameter controls the degree of fine tuning as depicted in\n",
    "    the table below.\n",
    "        \n",
    "        DenseNet          tuning_level\n",
    "        -------------     ------------\n",
    "        features\n",
    "          conv0               >= 5\n",
    "          norm0               >= 5\n",
    "          relu0               >= 5\n",
    "          pool0               >= 5\n",
    "          denseblock1         >= 4\n",
    "          transition1         >= 4\n",
    "          denseblock2         >= 3\n",
    "          transition2         >= 3\n",
    "          denseblock3         >= 2\n",
    "          transition3         >= 2\n",
    "          denseblock4         >= 1\n",
    "          norm5               >= 1\n",
    "        classifier            >= 0\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_fn: Callable, pretrained=True, tuning_level=0):\n",
    "        super().__init__(model_fn(pretrained=pretrained))\n",
    "\n",
    "        # change the output layer\n",
    "        last_layer_in = self._network.classifier.in_features\n",
    "        self._network.classifier = nn.Linear(last_layer_in, 13)\n",
    "\n",
    "        # ToDo: Omit layer types that do not have trainable parameters\n",
    "        tuning_params = [\n",
    "            TuningParam(0, \"classifier\", None),\n",
    "            TuningParam(1, \"features\", [\"denseblock4\", \"norm5\"]),\n",
    "            TuningParam(2, \"features\", [\"denseblock3\", \"transition3\"]),\n",
    "            TuningParam(3, \"features\", [\"denseblock2\", \"transition2\"]),\n",
    "            TuningParam(4, \"features\", [\"denseblock1\", \"transition1\"]),\n",
    "            TuningParam(5, \"features\", [\"conv0\", \"norm0\", \"relu0\", \"pool0\"])\n",
    "        ]\n",
    "\n",
    "        self._freeze_layers(tuning_params, pretrained, tuning_level)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self._network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckZqYccc21c3"
   },
   "outputs": [],
   "source": [
    "class DenseNet121(DenseNetBase):\n",
    "    def __init__(self, pretrained=True, tuning_level=0):\n",
    "        super().__init__(models.densenet121, pretrained, tuning_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckZqYccc21c3"
   },
   "outputs": [],
   "source": [
    "class DenseNet169(DenseNetBase):\n",
    "    def __init__(self, pretrained=True, tuning_level=0):\n",
    "        super().__init__(models.densenet169, pretrained, tuning_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckZqYccc21c3"
   },
   "outputs": [],
   "source": [
    "class DenseNet201(DenseNetBase):\n",
    "    def __init__(self, pretrained=True, tuning_level=0):\n",
    "        super().__init__(models.densenet201, pretrained, tuning_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet161(DenseNetBase):\n",
    "    def __init__(self, pretrained=True, tuning_level=0):\n",
    "        super().__init__(models.densenet161, pretrained, tuning_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHRk6r0s3-51"
   },
   "outputs": [],
   "source": [
    "class Project1Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Modified the last layer to output 13, rather than 3, features.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolution layers\n",
    "        self._body = nn.Sequential(\n",
    "            # input 3 x 224 x 224\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "            # input 24 * 112 * 112\n",
    "            nn.Conv2d(in_channels=16, out_channels=24, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(24),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "            # input 36 * 56 * 56\n",
    "            nn.Conv2d(in_channels=24, out_channels=36, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(36),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "            #input 54 * 28 * 28\n",
    "            nn.Conv2d(in_channels=36, out_channels=54, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(54),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "            #input 81 * 14 * 14\n",
    "            nn.Conv2d(in_channels=54, out_channels=81, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(81),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self._head = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=81*7*7, out_features=1024), \n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=1024, out_features=256), \n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(in_features=256, out_features=13)            \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self._body(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self._head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XjG813k3-52"
   },
   "source": [
    "## <font style=\"color:green\">6. Utils [5 Points]</font>\n",
    "\n",
    "Define your methods or classes which are not covered in the above sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(cm, classes, model_name=None):\n",
    "    \"\"\"\n",
    "    Creates and returns a confusion matrix figure that can be saved to a file or .\n",
    "    \"\"\"\n",
    "\n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "    # compute accuracy, normalized confusion matrix\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # initialize the plot tick marks and title\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    title = \"Confusion Matrix\"\n",
    "    if model_name is not None:\n",
    "        title = title + \" ({})\".format(model_name)\n",
    "    \n",
    "    # plot the confusion matrix\n",
    "    plt.style.use('default')\n",
    "    fig = plt.figure(figsize=(11,10), tight_layout=True)\n",
    "    im = plt.imshow(cm_norm, interpolation=\"nearest\", cmap=plt.cm.Blues, vmin=0., vmax=1.)\n",
    "\n",
    "    plt.title(title + \"\\n\")\n",
    "    plt.xticks(tick_marks, classes, rotation=22.5)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(\n",
    "            j, i,\n",
    "            format(cm[i, j], \"d\") + \"\\n\" + format(cm_norm[i, j], \".2f\"), \n",
    "            horizontalalignment=\"center\",\n",
    "            verticalalignment=\"center\",\n",
    "            color=\"white\" if cm_norm[i, j] > 0.5 else \"black\"\n",
    "        )\n",
    "\n",
    "    plt.ylabel(\"Target Labels\")\n",
    "    plt.xlabel(\"Predicted Labels\\nAccuracy={:0.4f}\".format(accuracy))\n",
    "    \n",
    "    # plot the color bar\n",
    "    divider = make_axes_locatable(plt.gca())\n",
    "    cax = divider.append_axes(\"right\", size=0.3, pad=0.2)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "\n",
    "    # close the plot and return the figure\n",
    "    plt.close()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_requires_grad_status(block) -> str:\n",
    "    params = list(block.parameters())\n",
    "    if not params:\n",
    "        return \"N/A\"\n",
    "    \n",
    "    or_of_params = False\n",
    "    and_of_params = True\n",
    "    for param in params:\n",
    "        or_of_params = or_of_params or param.requires_grad\n",
    "        and_of_params = and_of_params and param.requires_grad\n",
    "    if or_of_params and and_of_params:\n",
    "        return \"True\"\n",
    "    elif not or_of_params and not and_of_params:\n",
    "        return \"False\"\n",
    "    else:\n",
    "        return \"Mixed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_level_model_blocks(\n",
    "    model:nn.Module, \n",
    "    include_grandchildren:bool = False, \n",
    "    display_requires_grad = False\n",
    "):\n",
    "    status = \"\"\n",
    "    if display_requires_grad:\n",
    "        status = f\", requires_grad={get_requires_grad_status(model)}\"\n",
    "    print(f\"{type(model).__name__}{status}\")\n",
    "    for child in model.named_children():\n",
    "        if display_requires_grad:\n",
    "            status = f\", requires_grad={get_requires_grad_status(child[1])}\"\n",
    "        print(f\"  {child[0]}{status}\")\n",
    "        if include_grandchildren:\n",
    "            for grandchild in child[1].named_children():\n",
    "                if display_requires_grad:\n",
    "                    status = f\", requires_grad={get_requires_grad_status(grandchild[1])}\"\n",
    "                if not grandchild[0].isnumeric():\n",
    "                    print(f\"    { grandchild[0]}{status}\")\n",
    "                else:\n",
    "                    print(f\"    [{grandchild[0]}] {type(grandchild[1]).__name__}{status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output the architecture of several pretrained PyTorch models.\n",
    "\n",
    "The following models all have the same high level ResNet architecture.\n",
    "* ResNet-18\n",
    "* ResNet-34\n",
    "* ResNet-50\n",
    "* ResNet-101\n",
    "* ResNet-152\n",
    "* ResNeXt-50-32x4d\n",
    "* Wide ResNet-50-2\n",
    "* Wide ResNet-101-2\n",
    "\n",
    "The following models all have he same high level DenseNet architecture.\n",
    "* Densenet-121\n",
    "* Densenet-169\n",
    "* Densenet-201\n",
    "* Densenet-161\n",
    "\n",
    "```\n",
    "print_top_level_model_blocks(models.resnet18(), False)\n",
    "print_top_level_model_blocks(models.densenet121(), True)\n",
    "print_top_level_model_blocks(models.vgg11_bn(), True)\n",
    "print_top_level_model_blocks(models.vgg13_bn(), True)\n",
    "print_top_level_model_blocks(models.vgg16_bn(), True)\n",
    "print_top_level_model_blocks(models.vgg19_bn(), True)\n",
    "```\n",
    "\n",
    "The (formatted) output the `print_top_level_model_blocks` statements yields the following.\n",
    "\n",
    "Note:\n",
    "* Groups of Conv2d, BatchNorm, and ReLU layers have been condensed to CNR\n",
    "* Groups of Linear, ReLU, and Dropout layers have been condensed to LRD\n",
    "\n",
    "```\n",
    "ResNet           | DenseNet         | VGG11_BN         | VGG13_BN         | VGG16_BN         | VGG19_BN\n",
    "  conv1          |   features       |   features       |   features       |   features       |   features\n",
    "  bn1            |     conv0        |     [00-02] CNR  |     [00-02] CNR  |     [00-02] CNR  |     [00-02] CNR\n",
    "  relu           |     norm0        |                  |     [03-05] CNR  |     [03-05] CNR  |     [03-05] CNR\n",
    "  maxpool        |     relu0        |     [03] MaxPool |     [06] MaxPool |     [06] MaxPool |     [06] MaxPool2d\n",
    "  layer1         |     pool0        |     [04-06] CNR  |     [07-09] CNR  |     [07-09] CNR  |     [07-09] CNR\n",
    "  layer2         |     denseblock1  |                  |     [10-12] CNR  |     [10-12] CNR  |     [10-12] CNR\n",
    "  layer3         |     transition1  |     [07] MaxPool |     [13] MaxPool |     [13] MaxPool |     [13] MaxPool2d\n",
    "  layer4         |     denseblock2  |     [08-10] CNR  |     [14-16] CNR  |     [14-16] CNR  |     [14-16] CNR\n",
    "  avgpool        |     transition2  |     [11-13] CNR  |     [17-19] CNR  |     [17-19] CNR  |     [17-19] CNR\n",
    "  fc             |     denseblock3  |                  |                  |     [20-22] CNR  |     [20-22] CNR\n",
    "                 |     transition3  |                  |                  |                  |     [23-25] CNR\n",
    "                 |     denseblock4  |     [14] MaxPool |     [20] MaxPool |     [23] MaxPool |     [26] MaxPool2d\n",
    "                 |     norm5        |     [15-17] CNR  |     [21-23] CNR  |     [24-26] CNR  |     [27-29] CNR\n",
    "                 |   classifier     |     [18-20] CNR  |     [24-26] CNR  |     [27-29] CNR  |     [30-32] CNR\n",
    "                 |                  |                  |                  |     [30-32] CNR  |     [33-35] CNR\n",
    "                 |                  |                  |                  |                  |     [36-38] CNR\n",
    "                 |                  |     [21] MaxPool |     [27] MaxPool |     [33] MaxPool |     [39] MaxPool2d\n",
    "                 |                  |     [22-24] CNR  |     [28-30] CNR  |     [34-36] CNR  |     [40-42] CNR\n",
    "                 |                  |     [25-27] CNR  |     [31-33] CNR  |     [37-39] CNR  |     [43-45] CNR\n",
    "                 |                  |                  |                  |     [40-42] CNR  |     [46-48] CNR\n",
    "                 |                  |                  |                  |                  |     [49-51] CNR\n",
    "                 |                  |     [28] MaxPool |     [34] MaxPool |     [43] MaxPool |     [52] MaxPool2d\n",
    "                 |                  |   avgpool        |   avgpool        |   avgpool        |   avgpool\n",
    "                 |                  |   classifier     |   classifier     |   classifier     |   classifier\n",
    "                 |                  |     [00-02] LRD  |     [00-02] LRD  |     [00-02] LRD  |     [00-02] LRD\n",
    "                 |                  |     [03-05] LRD  |     [03-05] LRD  |     [03-05] LRD  |     [03-05] LRD\n",
    "                 |                  |     [06] Linear  |     [06] Linear  |     [06] Linear  |     [06] Linear\n",
    "```\n",
    "\n",
    "The `print_top_level_model_blocks` function was also used to test whether I properly implemented the fine tuning code. For example,\n",
    "\n",
    "```\n",
    "model = ResNet18(pretrained=True, tuning_level=0)\n",
    "print_top_level_model_blocks(model._network, include_grandchildren=False, display_requires_grad=True)\n",
    "\n",
    "    ResNet, requires_grad=Mixed\n",
    "      conv1, requires_grad=False\n",
    "      bn1, requires_grad=False\n",
    "      relu, requires_grad=N/A\n",
    "      maxpool, requires_grad=N/A\n",
    "      layer1, requires_grad=False\n",
    "      layer2, requires_grad=False\n",
    "      layer3, requires_grad=False\n",
    "      layer4, requires_grad=False\n",
    "      avgpool, requires_grad=N/A\n",
    "      fc, requires_grad=True#\n",
    "\n",
    "model = ResNet18(pretrained=True, tuning_level=1)\n",
    "print_top_level_model_blocks(model._network, include_grandchildren=False, display_requires_grad=True)\n",
    "\n",
    "    ResNet, requires_grad=Mixed\n",
    "      conv1, requires_grad=False\n",
    "      bn1, requires_grad=False\n",
    "      relu, requires_grad=N/A\n",
    "      maxpool, requires_grad=N/A\n",
    "      layer1, requires_grad=False\n",
    "      layer2, requires_grad=False\n",
    "      layer3, requires_grad=False\n",
    "      layer4, requires_grad=True\n",
    "      avgpool, requires_grad=N/A\n",
    "      fc, requires_grad=True\n",
    "\n",
    "...\n",
    "\n",
    "model = ResNet18(pretrained=True, tuning_level=5)\n",
    "print_top_level_model_blocks(model._network, include_grandchildren=False, display_requires_grad=True)\n",
    "\n",
    "    ResNet, requires_grad=True\n",
    "      conv1, requires_grad=True\n",
    "      bn1, requires_grad=True\n",
    "      relu, requires_grad=N/A\n",
    "      maxpool, requires_grad=N/A\n",
    "      layer1, requires_grad=True\n",
    "      layer2, requires_grad=True\n",
    "      layer3, requires_grad=True\n",
    "      layer4, requires_grad=True\n",
    "      avgpool, requires_grad=N/A\n",
    "      fc, requires_grad=True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sxmMNijlAb9v"
   },
   "outputs": [],
   "source": [
    "def create_submission_csv(path, exp):\n",
    "    \"\"\"\n",
    "    ToDo: Need to test and execute on the best model.\n",
    "    \"\"\"\n",
    "\n",
    "    # create a dictionary of numeric labels to text labels\n",
    "    label_dict = {}\n",
    "    for key, value in zip(np.arange(len(exp.classes)), exp.classes):\n",
    "        label_dict[key] = value\n",
    "\n",
    "    # get predictions for the test data using the trained model            \n",
    "    fnames, labels = predict_test_data(exp.trained_model, exp.test_loader, exp.device)\n",
    "\n",
    "    # convert the numeric labels to their text equivalents\n",
    "    labels = [label_dict[label] for label in labels]\n",
    "\n",
    "    # create a pandas data frame and write it to a CSV file\n",
    "    data_frame = pd.DataFrame(\n",
    "        np.stack((fnames, labels), axis=-1), \n",
    "        columns=[\"id\", \"class\"]\n",
    "    )\n",
    "\n",
    "    data_frame.to_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssjl7-QG3-52"
   },
   "source": [
    "## <font style=\"color:green\">7. Experiment [5 Points]</font>\n",
    "\n",
    "Choose your optimizer and LR-scheduler and use the above methods and classes to train your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exu5ZBHV-U-G"
   },
   "source": [
    "### Base Experiment Classes\n",
    "\n",
    "The following base classes facilitate rapid experiment creation.\n",
    "* Experiment - Base class for the following classes.\n",
    "* VisualExperiment - Conduct data visualization experiments.\n",
    "* ModelExperiment - Conduct model training experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQXZGHy0CzTZ"
   },
   "outputs": [],
   "source": [
    "class Experiment(ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        abbr: Optional[str] = None,\n",
    "        transform_resize: int = 256,\n",
    "        transform_crop_size: int = 224,\n",
    "        data_aug_color_enabled: Optional[bool] = None,\n",
    "        data_aug_color_brightness: Optional[Tuple[float, float]] = None,\n",
    "        data_aug_color_contrast: Optional[Tuple[float, float]] = None,\n",
    "        data_aug_color_saturation: Optional[Tuple[float, float]] = None,\n",
    "        data_aug_color_hue: Optional[Tuple[float, float]] = None,\n",
    "        data_aug_horz_flip_prob: Optional[float] = None,\n",
    "        data_aug_vert_flip_prob: Optional[float] = None,\n",
    "        data_aug_affine_enabled: Optional[bool] = None,\n",
    "        data_aug_affine_rotation: Optional[float] = None,\n",
    "        data_aug_affine_translate: Optional[Tuple[float, float]] = None,\n",
    "        data_aug_affine_scale: Optional[Tuple[float, float]] = None,\n",
    "        data_aug_erasing_prob: Optional[float] = None,\n",
    "        data_aug_erasing_scale: Optional[Tuple[float, float]] = None,\n",
    "        data_aug_erasing_ratio: Optional[Tuple[float, float]] = None,\n",
    "        data_loader_batch_size: Optional[int] = None,\n",
    "        data_loader_num_workers: Optional[int] = None,\n",
    "        optimizer_learning_rate: Optional[float] = None,\n",
    "        optimizer_momentum: Optional[float] = None,\n",
    "        optimizer_weight_decay: Optional[float] = None,\n",
    "        optimizer_betas: Optional[Tuple[float, float]] = None,\n",
    "        lr_scheduler_gamma: Optional[float] = None,\n",
    "        lr_scheduler_step_size: Optional[int] = None,\n",
    "        lr_scheduler_milestones: Optional[Iterable] = None,\n",
    "        lr_scheduler_patience: Optional[int] = None,\n",
    "        lr_scheduler_threshold: Optional[float] = None,\n",
    "        trainer_training_epochs: Optional[int] = None,\n",
    "        trainer_weighted_loss_fn: Optional[bool] = None,\n",
    "        trainer_model_saving_period: Optional[int] = None,\n",
    "        trainer_stop_loss_epochs: Optional[int] = None,\n",
    "        trainer_stop_acc_epochs: Optional[int] = None,\n",
    "        trainer_stop_acc_ema_alpha: Optional[float] = None,\n",
    "        trainer_stop_acc_threshold: Optional[float] = None\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        This base class for data visualization and model training experiment does the following.\n",
    "        \n",
    "            - Creates the master configuration instance accomodating constructor overrides\n",
    "            - Sets up the system, e.g., ensures reproducibility, enables CUDA acceleration, etc.\n",
    "            - Initializes the KenyanFood13 dataset\n",
    "            - Configures experiment visualization \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # the experient abbreviation is not specified, use the class name removing the prefix\n",
    "        # \"Exp\" if present\n",
    "        if abbr is None:\n",
    "            name = type(self).__name__\n",
    "            self._abbr = name\n",
    "            if name.startswith(\"Exp\"):\n",
    "                self._abbr = name[3:]\n",
    "        else:\n",
    "            self._abbr = abbr\n",
    "        \n",
    "        # ToDo: Apply patch if CUDA is not available.\n",
    "        self._resize = transform_resize\n",
    "        self._crop_size = transform_crop_size\n",
    "        self._config = create_master_config(\n",
    "            transform_resize,\n",
    "            transform_crop_size,\n",
    "            data_aug_color_enabled,\n",
    "            data_aug_color_brightness,\n",
    "            data_aug_color_contrast,\n",
    "            data_aug_color_saturation,\n",
    "            data_aug_color_hue,\n",
    "            data_aug_horz_flip_prob,\n",
    "            data_aug_vert_flip_prob,\n",
    "            data_aug_affine_enabled,\n",
    "            data_aug_affine_rotation,\n",
    "            data_aug_affine_translate,\n",
    "            data_aug_affine_scale,\n",
    "            data_aug_erasing_prob,\n",
    "            data_aug_erasing_scale,\n",
    "            data_aug_erasing_ratio,\n",
    "            data_loader_batch_size,\n",
    "            data_loader_num_workers,\n",
    "            optimizer_learning_rate,\n",
    "            optimizer_momentum,\n",
    "            optimizer_weight_decay,\n",
    "            optimizer_betas,\n",
    "            lr_scheduler_gamma,\n",
    "            lr_scheduler_step_size,\n",
    "            lr_scheduler_milestones,\n",
    "            lr_scheduler_patience,\n",
    "            lr_scheduler_threshold,  \n",
    "            trainer_training_epochs,\n",
    "            trainer_weighted_loss_fn,\n",
    "            trainer_model_saving_period,\n",
    "            trainer_stop_loss_epochs,\n",
    "            trainer_stop_acc_epochs,\n",
    "            trainer_stop_acc_ema_alpha,\n",
    "            trainer_stop_acc_threshold       \n",
    "        )\n",
    "        \n",
    "\n",
    "        setup_system(self._config.system)\n",
    "        \n",
    "        self._data = KenyanFood13Data(\n",
    "            data_root = self._config.dataset.data_dir,\n",
    "            valid_size = self._config.dataset.valid_size,\n",
    "            random_seed = self._config.system.seed\n",
    "        )\n",
    "\n",
    "        self._classes = self._data.classes\n",
    "        self._library = self._data.library\n",
    "        self.__visualizer = None\n",
    "        \n",
    "    @property\n",
    "    def classes(self):\n",
    "        return self._classes\n",
    "\n",
    "    @property\n",
    "    def library(self):\n",
    "        return self._library\n",
    "\n",
    "    \"\"\"\n",
    "    Protected methods that may or must be overridden by derived classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    @abstractproperty\n",
    "    def _visualizer_name(self) -> str:\n",
    "        pass\n",
    "\n",
    "    def _open_visualizer(self):\n",
    "        if self.__visualizer is None:\n",
    "            self.__visualizer = TensorBoardVisualizer(os.path.join(\n",
    "                self._config.system.proj_dir,\n",
    "                self._config.trainer.visualizer_dir, \n",
    "                self._visualizer_name\n",
    "            ))\n",
    "        return self.__visualizer\n",
    "\n",
    "    def _close_visualizer(self):\n",
    "        if self.__visualizer is not None:\n",
    "            self.__visualizer.close_tensorboard()\n",
    "            self.__visualizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SL2_K0Br-U-G"
   },
   "outputs": [],
   "source": [
    "class VisualExperiment(Experiment):\n",
    "    \"\"\"\n",
    "    This is the base class for data visualization experiments.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        abbr: Optional[str] = None,\n",
    "        log_originals: bool = True,\n",
    "        log_augmentations:bool = True,\n",
    "        transform_resize: int = 256,\n",
    "        transform_crop_size: int = 224,\n",
    "        data_aug_color_enabled: Optional[bool] = None,\n",
    "        data_aug_color_brightness: Optional[Tuple[float, float]] = None,\n",
    "        data_aug_color_contrast: Optional[Tuple[float, float]] = None,\n",
    "        data_aug_color_saturation: Optional[Tuple[float, float]] = None,\n",
    "        data_aug_color_hue: Optional[Tuple[float, float]] = None,\n",
    "        data_aug_horz_flip_prob: Optional[float] = None,\n",
    "        data_aug_vert_flip_prob: Optional[float] = None,\n",
    "        data_aug_affine_enabled: Optional[bool] = None,\n",
    "        data_aug_affine_rotation: Optional[float] = None,\n",
    "        data_aug_affine_translate: Optional[Tuple[float, float]] = None,\n",
    "        data_aug_affine_scale: Optional[Tuple[float, float]] = None,\n",
    "        data_aug_erasing_prob: Optional[float] = None,\n",
    "        data_aug_erasing_scale: Optional[Tuple[float, float]] = None,\n",
    "        data_aug_erasing_ratio: Optional[Tuple[float, float]] = None\n",
    "    ):\n",
    "        super().__init__(\n",
    "            abbr,\n",
    "            transform_resize,\n",
    "            transform_crop_size,\n",
    "            data_aug_color_enabled,\n",
    "            data_aug_color_brightness,\n",
    "            data_aug_color_contrast,\n",
    "            data_aug_color_saturation,\n",
    "            data_aug_color_hue,\n",
    "            data_aug_horz_flip_prob,\n",
    "            data_aug_vert_flip_prob,\n",
    "            data_aug_affine_enabled,\n",
    "            data_aug_affine_rotation,\n",
    "            data_aug_affine_translate,\n",
    "            data_aug_affine_scale,\n",
    "            data_aug_erasing_prob,\n",
    "            data_aug_erasing_scale,\n",
    "            data_aug_erasing_ratio\n",
    "        )\n",
    "        \n",
    "        self.__log_originals = log_originals\n",
    "        self.__log_augmentations = log_augmentations\n",
    "\n",
    "    def log_sample_images(\n",
    "        self, \n",
    "        num_of_contact_sheets: int = 1, \n",
    "        log_originals: Optional[bool] = None, \n",
    "        log_augmentations: Optional[bool] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create a 6 x 6 grid of images for each type of food in the data and\n",
    "        log these images to the visualizer.\n",
    "        \"\"\"\n",
    "\n",
    "        if log_originals is None:\n",
    "            log_originals = self.__log_originals\n",
    "\n",
    "        if log_augmentations is None:\n",
    "            log_augmentations = self.__log_augmentations\n",
    "\n",
    "        # abort if not logging either originals or augmentations\n",
    "        if not log_originals and not log_augmentations:\n",
    "            return\n",
    "\n",
    "        visualizer = self._open_visualizer()\n",
    "\n",
    "        for food, fnames in self._library.items():\n",
    "            \n",
    "            # since we want to visualize the same images before and after data\n",
    "            # augmentation, we need to shuffle the data ourselves rather than\n",
    "            # have the data loader do this for us\n",
    "            shuffled = random.sample(fnames, len(fnames))\n",
    "\n",
    "            # create food specific datasets\n",
    "            original_dataset = KenyanFood13Dataset(\n",
    "                image_root = self._data.image_root,\n",
    "                fnames = shuffled,\n",
    "                transform = self._config.dataset.visual_transforms\n",
    "            )\n",
    "            augmented_dataset = KenyanFood13Dataset(\n",
    "                image_root = self._data.image_root,\n",
    "                fnames = shuffled,\n",
    "                transform = self._config.dataset.visual_aug_transforms\n",
    "            )\n",
    "\n",
    "            # load 36 original and augmented images\n",
    "            original_dataloader = DataLoader(original_dataset, batch_size=36, shuffle=False)\n",
    "            original_iter = iter(original_dataloader)\n",
    "            augmented_dataloader = DataLoader(augmented_dataset, batch_size=36, shuffle=False)\n",
    "            augmented_iter = iter(augmented_dataloader)\n",
    "\n",
    "            for idx in list(range(num_of_contact_sheets)):\n",
    "                try:\n",
    "                    original_images, _ = next(original_iter)\n",
    "                    augmented_images, _ = next(augmented_iter)\n",
    "\n",
    "                    # save images to project's image directory (needs to exist!)\n",
    "                    # if log_originals:\n",
    "                    #     original_name = f\"{self._classes[food]}{(idx + 1):02d}.jpg\"\n",
    "                    #     original_path = os.path.join(proj_dir, \"images\", original_name)\n",
    "                    #     torchvision.utils.save_image(original_images, fp=original_path, nrow=6)\n",
    "                    # if log_augmentations:\n",
    "                    #     augmented_name = f\"{self._classes[food]}{(idx + 1):02d}_aug.jpg\"\n",
    "                    #     augmented_path = os.path.join(proj_dir, \"images\", augmented_name)\n",
    "                    #     torchvision.utils.save_image(augmented_images, fp=augmented_path, nrow=6)\n",
    "\n",
    "                    # add image grid to visualizer\n",
    "                    if log_originals:\n",
    "                        visualizer.add_image(\n",
    "                            tag=self._classes[food], \n",
    "                            image=torchvision.utils.make_grid(original_images, nrow=6)\n",
    "                        )\n",
    "                    if log_augmentations:\n",
    "                        visualizer.add_image(\n",
    "                            tag=self._classes[food] + \" (augmented)\", \n",
    "                            image=torchvision.utils.make_grid(augmented_images, nrow=6)\n",
    "                        )\n",
    "\n",
    "                except StopIteration:\n",
    "                    break\n",
    "        \n",
    "        self._close_visualizer()\n",
    "        \n",
    "    @property\n",
    "    def _visualizer_name(self) -> str:\n",
    "        return self._abbr + f\"-DV-RS_{self._resize}-CS_{self._crop_size}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmU0Q8Ek3-52"
   },
   "outputs": [],
   "source": [
    "class ModelExperiment(Experiment):\n",
    "    \"\"\"\n",
    "    This is the base class for model training experiments.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        abbr: Optional[str] = None,\n",
    "        data_augmentation: bool = True,\n",
    "        optimizer: Optimizer = Optimizer.SGD,\n",
    "        lr_scheduler: LrScheduler = LrScheduler.STEP,\n",
    "        transform_resize: int = 256,\n",
    "        transform_crop_size: int = 224,\n",
    "        data_aug_color_enabled: Optional[bool] = None,\n",
    "        data_aug_color_brightness: Optional[Tuple[float, float]] = None,\n",
    "        data_aug_color_contrast: Optional[Tuple[float, float]] = None,\n",
    "        data_aug_color_saturation: Optional[Tuple[float, float]] = None,\n",
    "        data_aug_color_hue: Optional[Tuple[float, float]] = None,\n",
    "        data_aug_horz_flip_prob: Optional[float] = None,\n",
    "        data_aug_vert_flip_prob: Optional[float] = None,\n",
    "        data_aug_affine_enabled: Optional[bool] = None,\n",
    "        data_aug_affine_rotation: Optional[float] = None,\n",
    "        data_aug_affine_translate: Optional[Tuple[float, float]] = None,\n",
    "        data_aug_affine_scale: Optional[Tuple[float, float]] = None,\n",
    "        data_aug_erasing_prob: Optional[float] = None,\n",
    "        data_aug_erasing_scale: Optional[Tuple[float, float]] = None,\n",
    "        data_aug_erasing_ratio: Optional[Tuple[float, float]] = None,\n",
    "        data_loader_batch_size: Optional[int] = None,\n",
    "        data_loader_num_workers: Optional[int] = None,\n",
    "        optimizer_learning_rate: Optional[float] = None,\n",
    "        optimizer_momentum: Optional[float] = None,\n",
    "        optimizer_weight_decay: Optional[float] = None,\n",
    "        optimizer_betas: Optional[Tuple[float, float]] = None,\n",
    "        lr_scheduler_gamma: Optional[float] = None,\n",
    "        lr_scheduler_step_size: Optional[int] = None,\n",
    "        lr_scheduler_milestones: Optional[Iterable] = None,\n",
    "        lr_scheduler_patience: Optional[int] = None,\n",
    "        lr_scheduler_threshold: Optional[float] = None,\n",
    "        trainer_training_epochs: Optional[int] = None,\n",
    "        trainer_weighted_loss_fn: Optional[bool] = None,\n",
    "        trainer_model_saving_period: Optional[int] = None,\n",
    "        trainer_stop_loss_epochs: Optional[int] = None,\n",
    "        trainer_stop_acc_epochs: Optional[int] = None,\n",
    "        trainer_stop_acc_ema_alpha: Optional[float] = None,\n",
    "        trainer_stop_acc_threshold: Optional[float] = None,\n",
    "        use_data_subsets: bool = False\n",
    "    ):\n",
    "        super().__init__(\n",
    "            abbr,\n",
    "            transform_resize,\n",
    "            transform_crop_size,\n",
    "            data_aug_color_enabled,\n",
    "            data_aug_color_brightness,\n",
    "            data_aug_color_contrast,\n",
    "            data_aug_color_saturation,\n",
    "            data_aug_color_hue,\n",
    "            data_aug_horz_flip_prob,\n",
    "            data_aug_vert_flip_prob,\n",
    "            data_aug_affine_enabled,\n",
    "            data_aug_affine_rotation,\n",
    "            data_aug_affine_translate,\n",
    "            data_aug_affine_scale,\n",
    "            data_aug_erasing_prob,\n",
    "            data_aug_erasing_scale,\n",
    "            data_aug_erasing_ratio,\n",
    "            data_loader_batch_size,\n",
    "            data_loader_num_workers,\n",
    "            optimizer_learning_rate,\n",
    "            optimizer_momentum,\n",
    "            optimizer_weight_decay,\n",
    "            optimizer_betas,\n",
    "            lr_scheduler_gamma,\n",
    "            lr_scheduler_step_size,\n",
    "            lr_scheduler_milestones,\n",
    "            lr_scheduler_patience,\n",
    "            lr_scheduler_threshold,\n",
    "            trainer_training_epochs,\n",
    "            trainer_weighted_loss_fn,\n",
    "            trainer_model_saving_period,\n",
    "            trainer_stop_loss_epochs,\n",
    "            trainer_stop_acc_epochs,\n",
    "            trainer_stop_acc_ema_alpha,\n",
    "            trainer_stop_acc_threshold\n",
    "        )\n",
    "\n",
    "        test_transforms = self._config.dataset.test_transforms\n",
    "        train_transforms = self._config.dataset.train_transforms\n",
    "        if not data_augmentation:\n",
    "            train_transforms = test_transforms\n",
    "\n",
    "        train_dataset, valid_dataset, test_dataset = get_datasets(\n",
    "            data = self._data,\n",
    "            test_transforms = test_transforms,\n",
    "            train_transforms = train_transforms,\n",
    "            subset = use_data_subsets\n",
    "        )\n",
    "\n",
    "        self.__train_loader, self.__valid_loader, self.__test_loader = get_data_loaders(\n",
    "            train_dataset = train_dataset,\n",
    "            valid_dataset = valid_dataset,\n",
    "            test_dataset = test_dataset,\n",
    "            batch_size = self._config.data_loader.batch_size,\n",
    "            num_workers = self._config.data_loader.num_workers\n",
    "        )                \n",
    "    \n",
    "        weight = None\n",
    "        if trainer_weighted_loss_fn:\n",
    "            weight = loss_rescaling_weight\n",
    "\n",
    "        self.__model, model_id = self._get_model()\n",
    "        self.__model_name = self._abbr + \"-\" + model_id\n",
    "        self.__model_dir = os.path.join(self._config.system.proj_dir, self._config.trainer.model_dir)\n",
    "        self.__loss_fn = nn.CrossEntropyLoss(weight=weight)\n",
    "        self.__metric_fn = AccuracyEstimator(topk=(1, )) # ToDo: Fix! (trainer.py expects a dictionary w/ 'top1' key)\n",
    "        self.__optimizer = get_optimizer(self.__model, optimizer, self._config.optimizer)\n",
    "        self.__lr_scheduler = get_lr_scheduler(self.__optimizer, lr_scheduler, self._config.scheduler)\n",
    "\n",
    "    @property\n",
    "    def test_loader(self) -> DataLoader:\n",
    "        return self.__test_loader\n",
    "\n",
    "    @property\n",
    "    def train_loader(self):\n",
    "        return self.__train_loader\n",
    "    \n",
    "    @property\n",
    "    def valid_loader(self):\n",
    "        return self.__valid_loader\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return torch.device(self._config.trainer.device)\n",
    "\n",
    "    @property\n",
    "    def trained_model_path(self):\n",
    "        return os.path.join(self.__model_dir, self.__model_name + \".pt\")\n",
    "    \n",
    "    @property\n",
    "    def trained_model(self) -> nn.Module:\n",
    "        self.__load_model()\n",
    "        return self.__model\n",
    "\n",
    "    def train(self):\n",
    "        device = self.device\n",
    "        self.__model = self.__model.to(device)\n",
    "        self.__loss_fn = self.__loss_fn.to(device)\n",
    "\n",
    "        visualizer = self._open_visualizer()\n",
    "        model_trainer = Trainer(\n",
    "            model=self.__model,\n",
    "            loader_train=self.__train_loader,\n",
    "            loader_test=self.__valid_loader,\n",
    "            loss_fn=self.__loss_fn,\n",
    "            metric_fn=self.__metric_fn,\n",
    "            optimizer=self.__optimizer,\n",
    "            lr_scheduler=self.__lr_scheduler,\n",
    "            model_save_dir=self.__model_dir,\n",
    "            model_name=self.__model_name,\n",
    "            model_saving_period=self._config.trainer.model_saving_period,\n",
    "            stop_loss_epochs=self._config.trainer.stop_loss_epochs,\n",
    "            stop_acc_ema_alpha=self._config.trainer.stop_acc_ema_alpha,\n",
    "            stop_acc_epochs=self._config.trainer.stop_acc_epochs,\n",
    "            stop_acc_threshold=self._config.trainer.stop_acc_threshold,\n",
    "            device=device,\n",
    "            data_getter=itemgetter(0),\n",
    "            target_getter=itemgetter(1),\n",
    "            stage_progress=self._config.trainer.progress_bar,\n",
    "            visualizer=visualizer,\n",
    "            get_key_metric=itemgetter(\"top1\")\n",
    "        )\n",
    "        model_trainer.register_hook(\"end_epoch\", hooks.end_epoch_hook_classification)\n",
    "        metrics = model_trainer.fit(self._config.trainer.training_epochs)\n",
    "        self._close_visualizer()\n",
    "\n",
    "        return metrics\n",
    "    \n",
    "    def log_graph(self):\n",
    "        model = self.trained_model\n",
    "        images, _ = next(iter(self.valid_loader))\n",
    "        device = self.device\n",
    "\n",
    "        visualizer = self._open_visualizer()\n",
    "        visualizer.add_graph(model.to(device), images.to(device))\n",
    "        self._close_visualizer()\n",
    "        \n",
    "    \n",
    "    def log_pr_curves(self):\n",
    "        targets, pred_probs = get_targets_and_pred_probs(\n",
    "            self.trained_model, \n",
    "            self.valid_loader,\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        visualizer = self._open_visualizer()\n",
    "        visualizer.add_pr_curves(self._classes, targets, pred_probs)\n",
    "        self._close_visualizer()\n",
    "    \n",
    "    def log_confusion_matrix(self):\n",
    "        targets, preds = predict_valid_data(\n",
    "            self.trained_model,\n",
    "            self.valid_loader,\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        visualizer = self._open_visualizer()\n",
    "        cm = confusion_matrix(targets, preds)\n",
    "        tag = f\"Confusion Matrix ({self.__model_name})\"\n",
    "        figure = create_confusion_matrix(cm, self.classes, self.__model_name)\n",
    "        visualizer.add_figure(tag=tag, figure=figure, close=True)\n",
    "        self._close_visualizer()\n",
    "\n",
    "    \"\"\"\n",
    "    Protected methods that may or must be overridden by derived classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    @property\n",
    "    def _visualizer_name(self) -> str:\n",
    "        return self.__model_name\n",
    "            \n",
    "    @abstractmethod\n",
    "    def _get_model(self) -> Tuple[nn.Module, str]:\n",
    "        pass\n",
    "    \n",
    "    \"\"\"\n",
    "    Private methods that should only be called by this base class.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __load_model(self):\n",
    "        path = self.trained_model_path\n",
    "        if os.path.exists(path):\n",
    "            self.__model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalyzeTensorBoardRun:\n",
    "    \"\"\"\n",
    "    A utility class to do the following for a TensorBoard run:\n",
    "    \n",
    "        - return a list of run matching the filter\n",
    "        - return a dictionary of scalars *\n",
    "        - return accuracy at epoch where loss is lowest *\n",
    "        - return overfitting metric *\n",
    "        - return a summary of all runs\n",
    "        - return a figure *\n",
    "        \n",
    "        * specific to a given run\n",
    "        \n",
    "    Note: The overfitting metric is the slope of the test loss divided\n",
    "          by the train loss. A value of zero indicates no overfitting.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, visualizer_dir, filter:str=\"^.*\"):\n",
    "        import re\n",
    "        runs = os.listdir(os.path.join(visualizer_dir))\n",
    "        runs = [run for run in runs if re.search(filter, run) is not None]\n",
    "        runs.sort()\n",
    "        self.__visualizer_dir = visualizer_dir\n",
    "        self.__runs = runs \n",
    "\n",
    "    @property\n",
    "    def runs(self) -> List[str]:\n",
    "        return self.__runs\n",
    "    \n",
    "    def get_scalars(self, run: str):\n",
    "        from tensorboard.backend.event_processing import event_accumulator\n",
    "        path = os.path.join(self.__visualizer_dir, run)\n",
    "        event_acc = event_accumulator.EventAccumulator(path)\n",
    "        event_acc.Reload()\n",
    "\n",
    "        scalars = {}\n",
    "        for tag in sorted(event_acc.Tags()[\"scalars\"]):\n",
    "            x, y = [], []\n",
    "            for scalar_event in event_acc.Scalars(tag):\n",
    "                x.append(scalar_event.step)\n",
    "                y.append(scalar_event.value)\n",
    "            scalars[tag] = (np.asarray(x), np.asarray(y))\n",
    "        return scalars\n",
    "\n",
    "    def get_accuracy(self, scalars) -> float:\n",
    "        index = np.argmin(scalars[\"data/test_loss\"][1])\n",
    "        return scalars[\"data/test_metric:top1\"][1][index].item()\n",
    "\n",
    "    def get_overfitting_metric(self, scalars, alpha:float=0.3) -> float:\n",
    "        from numpy.polynomial.polynomial import polyfit\n",
    "        loss_tst = scalars[\"data/test_loss\"][1]\n",
    "        loss_trn = scalars[\"data/train_loss\"][1]\n",
    "        ratio = [tst / trn for tst, trn in zip(loss_tst, loss_trn)]   \n",
    "        _, m = polyfit(list(range(len(ratio))), ratio, 1)\n",
    "        return m\n",
    "\n",
    "    def get_summary(self) -> List[Tuple[str, float, float]]:\n",
    "        summary = []\n",
    "        for run in self.runs:\n",
    "            scalars = self.get_scalars(run)\n",
    "            accuracy = self.get_accuracy(scalars)\n",
    "            overfitting = self.get_overfitting_metric(scalars)\n",
    "            summary.append((run, accuracy, overfitting))\n",
    "        return summary\n",
    "\n",
    "    def get_loss_plot(self, scalars, title, alpha:float=0.3):\n",
    "        from numpy.polynomial.polynomial import polyfit\n",
    "        accuracy = self.get_accuracy(scalars)\n",
    "        loss_tst = scalars[\"data/test_loss\"][1]\n",
    "        loss_trn = scalars[\"data/train_loss\"][1]\n",
    "        loss_tst_ema = self.__ema_smoothing(loss_tst, alpha)\n",
    "        loss_trn_ema = self.__ema_smoothing(loss_trn, alpha)\n",
    "\n",
    "        x = list(range(len(loss_tst_ema)))\n",
    "        ratio = [tst / trn for tst, trn in zip(loss_tst, loss_trn)]       \n",
    "        b, m = polyfit(x, ratio, 1)\n",
    "        ratio_bf = [m * x1 + b for x1 in x] \n",
    "\n",
    "        fig = plt.figure(figsize=(8,4))\n",
    "        plt.suptitle(f\"{title} ({accuracy:.2f}%)\")\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"loss\")\n",
    "        plt.plot(x, loss_trn_ema, \"r\", label=\"test\")\n",
    "        plt.plot(x, loss_tst_ema, \"b\", label=\"train\")\n",
    "        plt.legend()\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(f\"loss (overfit: {m:.3f})\")\n",
    "        plt.gca().set_ylim([0.0, 4.0])\n",
    "        plt.plot(x, ratio, color=\"#a0ffa0\")\n",
    "        plt.plot(x, ratio_bf, color=\"#00ff00\", label=\"test/train\")\n",
    "        plt.legend()\n",
    "        plt.close()\n",
    "        return fig      \n",
    "\n",
    "    def __ema_smoothing(self, data, alpha=0.3) -> List[float]:\n",
    "        data_ema = []\n",
    "        last = data[0]\n",
    "        for datum in data:\n",
    "            last = alpha * datum + (1 - alpha) * last\n",
    "            data_ema.append(last)\n",
    "        return data_ema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct(\n",
    "    exp: Experiment, \n",
    "    log_graph:bool = True,\n",
    "    log_pr_curves:bool = True,\n",
    "    log_confusion_matrix:bool = True,\n",
    "    free_experiment:bool = True):   \n",
    "    \"\"\"\n",
    "    This method conducts an visual or model experiment. A visual experiment logs original\n",
    "    and/or augmented images in 6 x 6 contact sheets to TensorBoard. A model experiment\n",
    "    performs the following steps and returns its training metrics.\n",
    "\n",
    "    1. Logs the model's graph.\n",
    "    2. Trains the model.\n",
    "    3. Logs the precision-recall curve for each food class.\n",
    "    4. Logs the confusion matrix.\n",
    "    \n",
    "    Note: Steps 3 and 4 are performed on the validation data set using model weights that\n",
    "          achieved the lowest average loss on the validaton data set.\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(exp, VisualExperiment):\n",
    "        exp.log_sample_images()\n",
    "        result = None\n",
    "    \n",
    "    elif isinstance(exp, ModelExperiment):\n",
    "        if log_graph:\n",
    "            exp.log_graph()\n",
    "        metrics = exp.train()\n",
    "        if log_pr_curves:\n",
    "            exp.log_pr_curves()\n",
    "        if log_confusion_matrix:\n",
    "            exp.log_confusion_matrix()\n",
    "        result = metrics\n",
    "        \n",
    "    if free_experiment:\n",
    "        # after running several experiments, a \"RuntimeError: CUDA out of memory\"\n",
    "        # exception was raised ... trying to see whether explicitly deleting the\n",
    "        # experiment helps\n",
    "        del exp\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tensor_board_runs(filter: str = \"^B\"):\n",
    "    config = create_trainer_config()\n",
    "    analyze = AnalyzeTensorBoardRun(os.path.join(proj_dir, config.visualizer_dir), filter=filter)\n",
    "\n",
    "    # create a markdown table\n",
    "    print(\"|Experiment|Accuracy|Overfitting Metric|\")\n",
    "    print(\"|:---|:---:|:---:|\")\n",
    "    for item in analyze.get_summary():\n",
    "        print(f\"|{item[0]}|{item[1]:.2f}|{item[2]:.3f}|\")\n",
    "\n",
    "    # save loss images ... destination directory must exist!\n",
    "    for run in analyze.runs:\n",
    "        path = os.path.join(proj_dir, \"images\", \"loss\", run + \".png\")\n",
    "        scalars = analyze.get_scalars(run)\n",
    "        fig = analyze.get_loss_plot(scalars, run)\n",
    "        fig.savefig(path, facecolor=\"#ffffff\")\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLi5W6s06tZO"
   },
   "source": [
    "### Experiment Group A: Data Visualization Experiments and Training Pipeline Check\n",
    "\n",
    "**Summary**\n",
    "\n",
    "This first set of experiments log contact sheets, i.e., 6 x 6 grids of images, of each food type to the visualizer with and without data augmentation. \n",
    "\n",
    "The second set of experiments train only the classifier layer (fc) of the pretrained Resnet18 model using a subset of the data without augmentation to check the training pipeline. In the first experiment, training will stop after 40 epochs. In the second experiment, training will stop after 40 epochs or when the smoothed accuracy (computed by expontential moving average with an alpha = 0.3) does not decrease by 2% within 10 epochs.\n",
    "\n",
    "The third set of experiments vary the number of data loader worker threads to determine the optimal number for future experiments. These experiments stop after 11 epochs. The time between logging the 2nd and 11th epochs' test metrics divided by 10 will be used to evaluate data loading efficiency. Saving the model's state is disabled to eliminate its time contribution.\n",
    "\n",
    "___\n",
    "\n",
    "**Results**\n",
    "\n",
    "I used the data augmentation transforms I created for project 1. The data validation experiment revealed an issue with these transforms. First, the color jitter transform dramatically changed the image's color. While this was not detrimental to classify cats, dogs, and pandas; I suspect it may reduce accuracy on the KenyanFood13 dataset. I will test this theory in the last group of experiments after the assignment has been completed.\n",
    "\n",
    "To properly set the data augmentation transform parameters, I ran several experiments not shown in this notebook. These experiments disabled all but one type of augmentation in order to \"tune\" it. For example, to properly set the hue parameter of the color jitter transform, I disabled the horizontal/vertical flips, affine, and erase transforms. Furthermore, I set the color jitter's brightness, contrast, and saturation to the values that would produce the original image. I then found acceptable minimum and maximum values for the hue parameter. After conducting all of these data augmentation tuning experiments, I updated the configuration file and re-ran the data visualization experiment. I visualized the entire dataset to external files, but only logged the following 6 x 6 contact sheets to Tensorboard.\n",
    "* ExpAAA - Original Images\n",
    "* ExpAAA - \"Tuned\" Augmented Images\n",
    "* ExpAAB - \"Untuned\" Augmented Images\n",
    "\n",
    "The training pipeline check experiment performed as expected. The training and test loss decreased and the accuracy increased to ~ 60%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group A, Set A (Data Visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1nCwE0ULBBU6"
   },
   "outputs": [],
   "source": [
    "class ExpAAA(VisualExperiment):\n",
    "    \"\"\"\n",
    "    Log original and augmented images using \"tuned\" parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpAAB(VisualExperiment):\n",
    "    \"\"\"\n",
    "    Log augmented images using Project 1's \"untuned\" parameters.\n",
    "    \"\"\"   \n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            log_originals = False,\n",
    "            log_augmentations = True,\n",
    "            data_aug_color_enabled = True,\n",
    "            data_aug_color_brightness = (0.75, 1.25),\n",
    "            data_aug_color_contrast = (0.75, 1.25),\n",
    "            data_aug_color_saturation = (0.75, 1.25),\n",
    "            data_aug_color_hue = (-0.25, 0.25),\n",
    "            data_aug_horz_flip_prob = 0.5,\n",
    "            data_aug_vert_flip_prob = 0.5,\n",
    "            data_aug_affine_enabled = True,\n",
    "            data_aug_affine_rotation = 45,\n",
    "            data_aug_affine_translate = (0.2, 0.2),\n",
    "            data_aug_affine_scale = (0.8, 1.2),\n",
    "            data_aug_erasing_prob = 0.5,\n",
    "            data_aug_erasing_scale = (0.02, 0.33),\n",
    "            data_aug_erasing_ratio = (0.3, 3.3)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group A, Set B (Training Pipeline Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HiT-bcR-U-G"
   },
   "outputs": [],
   "source": [
    "class ExpABA(ModelExperiment):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            use_data_subsets = True,\n",
    "            data_augmentation = False,\n",
    "            trainer_training_epochs = 40 \n",
    "        )\n",
    "    def _get_model(self):\n",
    "        return ResNet18(pretrained=True, tuning_level=0), \"ResNet18-PT0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpABB(ModelExperiment):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            use_data_subsets = True,\n",
    "            data_augmentation = False,\n",
    "            trainer_training_epochs = 40, \n",
    "            trainer_stop_acc_epochs = 10,\n",
    "            trainer_stop_acc_ema_alpha = 0.3,\n",
    "            trainer_stop_acc_threshold = 2.0\n",
    "        )\n",
    "    def _get_model(self):\n",
    "        return ResNet18(pretrained=True, tuning_level=0), \"ResNet18-PT0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group A, Set C (Data Loader Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpAC_(ModelExperiment):\n",
    "    def __init__(\n",
    "        self,\n",
    "        exp_id: str, # expects a single uppercase letter\n",
    "        data_loader_num_workers: int\n",
    "    ):\n",
    "        super().__init__(\n",
    "            abbr = \"AC\" + exp_id,\n",
    "            lr_scheduler_step_size = 20,\n",
    "            trainer_training_epochs = 11,\n",
    "            trainer_model_saving_period = -1, # disable\n",
    "            data_loader_num_workers = data_loader_num_workers\n",
    "        )\n",
    "    def _get_model(self):\n",
    "        return ResNet18(pretrained=True, tuning_level=0), \"ResNet18-PT0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group A - Conduct Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_group_A():\n",
    "    # set 1 - visualize data\n",
    "    conduct(ExpAAA())\n",
    "    conduct(ExpAAB())\n",
    "    # set 2 - check trainer pipeline\n",
    "    conduct(ExpABA(), log_graph=False, log_pr_curves=False, log_confusion_matrix=False)\n",
    "    conduct(ExpABB(), log_graph=False, log_pr_curves=False, log_confusion_matrix=False)\n",
    "    # set 3 - optimizer data loader\n",
    "    conduct(ExpAC_('A', 1), log_graph=False, log_pr_curves=False, log_confusion_matrix=False)\n",
    "    conduct(ExpAC_('B', 2), log_graph=False, log_pr_curves=False, log_confusion_matrix=False)\n",
    "    conduct(ExpAC_('C', 3), log_graph=False, log_pr_curves=False, log_confusion_matrix=False)\n",
    "    conduct(ExpAC_('D', 4), log_graph=False, log_pr_curves=False, log_confusion_matrix=False)\n",
    "    conduct(ExpAC_('E', 5), log_graph=False, log_pr_curves=False, log_confusion_matrix=False)\n",
    "    conduct(ExpAC_('F', 6), log_graph=False, log_pr_curves=False, log_confusion_matrix=False)\n",
    "    conduct(ExpAC_('G', 7), log_graph=False, log_pr_curves=False, log_confusion_matrix=False)\n",
    "    conduct(ExpAC_('H', 8), log_graph=False, log_pr_curves=False, log_confusion_matrix=False)\n",
    "    conduct(ExpAC_('I', 9), log_graph=False, log_pr_curves=False, log_confusion_matrix=False)\n",
    "    conduct(ExpAC_('J', 10), log_graph=False, log_pr_curves=False, log_confusion_matrix=False)\n",
    "    conduct(ExpAC_('K', 11), log_graph=False, log_pr_curves=False, log_confusion_matrix=False)\n",
    "    conduct(ExpAC_('L', 12), log_graph=False, log_pr_curves=False, log_confusion_matrix=False)\n",
    "    conduct(ExpAC_('M', 13), log_graph=False, log_pr_curves=False, log_confusion_matrix=False)\n",
    "    conduct(ExpAC_('N', 14), log_graph=False, log_pr_curves=False, log_confusion_matrix=False)\n",
    "    conduct(ExpAC_('O', 15), log_graph=False, log_pr_curves=False, log_confusion_matrix=False)\n",
    "    conduct(ExpAC_('P', 16), log_graph=False, log_pr_curves=False, log_confusion_matrix=False)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Do_oOfMcBBU4"
   },
   "source": [
    "### Experiment Group B: Exploring Transfer Learning Approaches\n",
    "\n",
    "<b>Summary</b>\n",
    "\n",
    "This group of experiments explores transfer learning. Each set of experiments in this group explores transfer learning on a specific model and conducts the following experiments. \n",
    "* ExpB?A - Train the entire untrained model\n",
    "* ExpB?B - Train the pretrained model's classifier layer (tuning_level = 0)\n",
    "* ExpB?C - Train the pretrained model's classifier layer and last convolution block (tuning_level = 1)\n",
    "* ExpB?D - Train the pretrained model's classifier layer and last 2 convolution blocks (tuning_level = 2)\n",
    "* ExpB?E - Train the pretrained model's classifier layer and last 3 convolution blocks (tuning_level = 3)\n",
    "* ExpB?F - Train the pretrained model's classifier layer and last 4 convolution blocks (tuning_level = 4)\n",
    "* ExpB?G - Train the entire pretrained model\n",
    "\n",
    "Preliminary tests indicate that overfitting is possible with the large models even when the KenyanFood13 images are significantly augmented. My first inclination, which I rejected, was to explore transfer learning on representatives from the ResNet, VGG, and DenseNet model families. My selection criterion was the model with the lowest ImageNet Top-1 error. However, these representatives were significantly more complex than their siblings, so overfitting is likely. Consequently, I will perform transfer learning experiments on every implemented models (see below).\n",
    "* ResNet-18\n",
    "* ResNet-34\n",
    "* ResNet-50\n",
    "* ResNet-101\n",
    "* ResNet-152\n",
    "* ResNeXt-50-32x4d\n",
    "* ResNeXt-101-32x8d\n",
    "* Wide ResNet-50-2\n",
    "* Wide ResNet-101-2\n",
    "* VGG-11 with batch normalization\n",
    "* VGG-13 with batch normalization\n",
    "* VGG-16 with batch normalization\n",
    "* VGG-19 with batch normalization\n",
    "* DenseNet-121\n",
    "* DenseNet-169\n",
    "* DenseNet-201\n",
    "* DenseNet-161\n",
    "\n",
    "Training will stop after 100 epochs or when the smoothed accuracy (computed by expontential moving average with an alpha = 0.3) does not decrease by 1% within 10 epochs.\n",
    "\n",
    "___\n",
    "\n",
    "**Results**\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group B, Sets A, B, C, ... (Transfer Learning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qszJetrV-U-H"
   },
   "outputs": [],
   "source": [
    "class ExpB__(ModelExperiment):\n",
    "    def __init__(\n",
    "        self,\n",
    "        set_id: str, # expects a single uppercase letter\n",
    "        exp_id: str, # expects a single uppercase letter\n",
    "        model_type: TorchVisionModel,\n",
    "        model_abbr: str\n",
    "    ):\n",
    "        self.__model_type = model_type\n",
    "        if exp_id == 'A':\n",
    "            self.__pretrained = False\n",
    "            self.__tuning_level = 0\n",
    "            self.__model_abbr = model_abbr\n",
    "        elif exp_id == 'B':\n",
    "            self.__pretrained = True\n",
    "            self.__tuning_level = 0\n",
    "            self.__model_abbr = model_abbr + \"-PT0\"\n",
    "        elif exp_id == 'C':\n",
    "            self.__pretrained = True\n",
    "            self.__tuning_level = 1\n",
    "            self.__model_abbr = model_abbr + \"-PT1\"\n",
    "        elif exp_id == 'D':\n",
    "            self.__pretrained = True\n",
    "            self.__tuning_level = 2\n",
    "            self.__model_abbr = model_abbr + \"-PT2\"\n",
    "        elif exp_id == 'E':\n",
    "            self.__pretrained = True\n",
    "            self.__tuning_level = 3\n",
    "            self.__model_abbr = model_abbr + \"-PT3\"\n",
    "        elif exp_id == 'F':\n",
    "            self.__pretrained = True\n",
    "            self.__tuning_level = 4\n",
    "            self.__model_abbr = model_abbr + \"-PT4\"\n",
    "        elif exp_id == 'G':\n",
    "            self.__pretrained = True\n",
    "            self.__tuning_level = 5\n",
    "            self.__model_abbr = model_abbr + \"-PT5\"\n",
    "\n",
    "        super().__init__(\n",
    "            abbr = 'B' + set_id + exp_id,\n",
    "            data_loader_num_workers = 12,\n",
    "            trainer_training_epochs = 100, \n",
    "            trainer_stop_acc_epochs = 10,\n",
    "            trainer_stop_acc_ema_alpha = 0.3,\n",
    "            trainer_stop_acc_threshold = 1.0\n",
    "        )\n",
    "\n",
    "    def _get_model(self):\n",
    "        return self.__model_type(self.__pretrained, self.__tuning_level), self.__model_abbr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group B - Conduct Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_group_B():\n",
    "\n",
    "    model_types = [\n",
    "        ResNet18, ResNet34, ResNet50, ResNet101, ResNet152,\n",
    "        ResNeXt50, ResNeXt101, WideResNet50, WideResNet101,\n",
    "        VGG11BN, VGG13BN, VGG16BN, VGG19BN,\n",
    "        DenseNet121, DenseNet169, DenseNet201, DenseNet161\n",
    "    ]\n",
    "\n",
    "    set_id = 'A'\n",
    "    for model_type in model_types:\n",
    "        model_name = model_type.__name__\n",
    "        conduct(ExpB__(set_id, 'A', model_type, model_name))\n",
    "        conduct(ExpB__(set_id, 'B', model_type, model_name))\n",
    "        conduct(ExpB__(set_id, 'C', model_type, model_name))\n",
    "        conduct(ExpB__(set_id, 'D', model_type, model_name))\n",
    "        conduct(ExpB__(set_id, 'E', model_type, model_name))\n",
    "        conduct(ExpB__(set_id, 'F', model_type, model_name))\n",
    "        conduct(ExpB__(set_id, 'G', model_type, model_name))\n",
    "        set_id = chr(ord(set_id[0]) + 1)"
   ]
  },
  {
   "source": [
    "#### Group C, Sets A, B, C, ... (Transfer Learning) "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpC__(ModelExperiment):\n",
    "    def __init__(\n",
    "        self,\n",
    "        set_id: str, # expects a single uppercase letter\n",
    "        exp_id: str, # expects a single uppercase letter\n",
    "        model_type: TorchVisionModel,\n",
    "        model_abbr: str\n",
    "    ):\n",
    "        self.__model_type = model_type\n",
    "        if exp_id == 'A':\n",
    "            lr = 1e-3\n",
    "            self.__model_abbr = model_abbr + \"-LR1E-3\"\n",
    "        elif exp_id == 'B':\n",
    "            lr = 5e-4\n",
    "            self.__model_abbr = model_abbr + \"-LR5E-4\"\n",
    "        elif exp_id == 'C':\n",
    "            lr = 1e-4\n",
    "            self.__model_abbr = model_abbr + \"-LR1E-4\"\n",
    "        elif exp_id == 'D':\n",
    "            lr = 5e-5\n",
    "            self.__model_abbr = model_abbr + \"-LR5E-5\"\n",
    "        elif exp_id == 'E':\n",
    "            lr = 1e-5\n",
    "            self.__model_abbr = model_abbr + \"-LR1E-5\"\n",
    "\n",
    "        super().__init__(\n",
    "            abbr = 'C' + set_id + exp_id,\n",
    "            data_loader_num_workers = 12,\n",
    "            optimizer_learning_rate = lr,\n",
    "            lr_scheduler_step_size = 1000,\n",
    "            trainer_training_epochs = 100,\n",
    "            trainer_stop_loss_epochs = 20,\n",
    "        )\n",
    "\n",
    "    def _get_model(self):\n",
    "        return self.__model_type(pretrained=True, tuning_level=2), self.__model_abbr"
   ]
  },
  {
   "source": [
    "#### Group C - Conduct Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_group_C():\n",
    "\n",
    "    model_types = [ResNeXt101, VGG19BN, DenseNet161]\n",
    "\n",
    "    set_id = 'A'\n",
    "    for model_type in model_types:\n",
    "        model_name = model_type.__name__\n",
    "        conduct(ExpC__(set_id, 'A', model_type, model_name))\n",
    "        conduct(ExpC__(set_id, 'B', model_type, model_name))\n",
    "        conduct(ExpC__(set_id, 'C', model_type, model_name))\n",
    "        conduct(ExpC__(set_id, 'D', model_type, model_name))\n",
    "        conduct(ExpC__(set_id, 'E', model_type, model_name))\n",
    "        set_id = chr(ord(set_id[0]) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kx9Gbfds3-53"
   },
   "source": [
    "### <font style=\"color:blue\">Main Function</font>\n",
    "\n",
    "A simple function that conducts groups of experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I9Q6_bS9lDh8"
   },
   "outputs": [],
   "source": [
    "def main():   \n",
    "    for group in []:\n",
    "        if group == 'A':\n",
    "            conduct_group_A()\n",
    "        elif group == 'B':\n",
    "            conduct_group_B()\n",
    "        elif group == 'C':\n",
    "            conduct_group_C()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167,
     "referenced_widgets": [
      "3fd2b77dfa544f85837435ca5961ba8f",
      "96accc2fe8944764afa8c16633660ac7",
      "974c6aa67ffc42698fdb80f439dc4569",
      "ecd844ae701c4a31a9dd1f5cafde9078",
      "806a3f07103f4edc90294dc92b8aa455",
      "d24d1c8a70294e0690041ffb9d45453d",
      "bc96aba5b5da4c0ebac3137284bba1c1",
      "ad93bc6d66074fe8b119243baa0f4cf8",
      "823bc44be56c4a9993125d310e637705",
      "f1b83c7eddc44d2fb54726efc1848ce8",
      "f3dfe5dd75b24bada66d8ac1fc792fe4",
      "49fc9f376afb496e855d300a106e50e4",
      "bd9aafdad8654a1f931330bf91cdebd3",
      "de5dc6c1cc494e929f5d2d95813a17cc",
      "ac7ee576d9704909b64b208915e03a8a",
      "718df99acdca4a738abb6d301a5a0449",
      "889c5df7ebe543eb95e266bc51af7e21",
      "44af0c82674c4ae58d56c5ee7806e9fd",
      "542db1f2ce2f402c85a437ac303ac2fb",
      "a4ff39f28ca046a894a31d953fb9014a",
      "c4fdea457140450abff31fd1063400cc",
      "69147e87b38549a58fd337d31051742c",
      "c032ee196ffe4992a0eb175bb26d3358",
      "8edc6f437a3047f593fc706e4d9587c3"
     ]
    },
    "id": "G4Fe0mLZ3-53",
    "outputId": "50773fbe-c4b3-4e43-f0e4-7b513f8c5c6d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "source": [
    "### Analyze Experiment Groups"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run analysis ... make sure image destination directory exist!\n",
    "# analyze_tensor_board_runs(filter=\"^B\")  # analyze Group B experiments\n",
    "# analyze_tensor_board_runs(filter=\"^C\")  # analyze Group C experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFt15GPK3-53"
   },
   "source": [
    "## <font style=\"color:green\">8. TensorBoard Dev Scalars Log Link [5 Points]</font>\n",
    "\n",
    "Share your tensorboard scalars logs link in this section. You can also share (not mandatory) your GitHub link if you have pushed this project in GitHub. \n",
    "\n",
    "For example, [Find Project2 logs here](https://tensorboard.dev/experiment/kMJ4YU0wSNG0IkjrluQ5Dg/#scalars)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hh36KFmK3-53"
   },
   "source": [
    "## <font style=\"color:green\">9. Kaggle Profile Link [50 Points]</font>\n",
    "\n",
    "Share your Kaggle profile link here with us so that we can give points for the competition score. \n",
    "\n",
    "You should have a minimum accuracy of `75%` on the test data to get all points. If accuracy is less than `70%`, you will not get any points for the section. \n",
    "\n",
    "**You must have to submit `submission.csv` (prediction for images in `test.csv`) in `Submit Predictions` tab in Kaggle to get any evaluation in this section.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Project2_Kaggle_Competition_Classification-4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "3fd2b77dfa544f85837435ca5961ba8f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_974c6aa67ffc42698fdb80f439dc4569",
       "IPY_MODEL_ecd844ae701c4a31a9dd1f5cafde9078"
      ],
      "layout": "IPY_MODEL_96accc2fe8944764afa8c16633660ac7"
     }
    },
    "44af0c82674c4ae58d56c5ee7806e9fd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "49fc9f376afb496e855d300a106e50e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_718df99acdca4a738abb6d301a5a0449",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ac7ee576d9704909b64b208915e03a8a",
      "value": " 0/100 [00:00&lt;?, ?it/s]"
     }
    },
    "542db1f2ce2f402c85a437ac303ac2fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "[0/100][Train][115] Loss_avg: 2.1766, Loss: 2.2303, LR: 0.001:  71%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_69147e87b38549a58fd337d31051742c",
      "max": 164,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c4fdea457140450abff31fd1063400cc",
      "value": 116
     }
    },
    "69147e87b38549a58fd337d31051742c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "718df99acdca4a738abb6d301a5a0449": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "806a3f07103f4edc90294dc92b8aa455": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "823bc44be56c4a9993125d310e637705": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f3dfe5dd75b24bada66d8ac1fc792fe4",
       "IPY_MODEL_49fc9f376afb496e855d300a106e50e4"
      ],
      "layout": "IPY_MODEL_f1b83c7eddc44d2fb54726efc1848ce8"
     }
    },
    "889c5df7ebe543eb95e266bc51af7e21": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_542db1f2ce2f402c85a437ac303ac2fb",
       "IPY_MODEL_a4ff39f28ca046a894a31d953fb9014a"
      ],
      "layout": "IPY_MODEL_44af0c82674c4ae58d56c5ee7806e9fd"
     }
    },
    "8edc6f437a3047f593fc706e4d9587c3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "96accc2fe8944764afa8c16633660ac7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "974c6aa67ffc42698fdb80f439dc4569": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d24d1c8a70294e0690041ffb9d45453d",
      "max": 100441675,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_806a3f07103f4edc90294dc92b8aa455",
      "value": 100441675
     }
    },
    "a4ff39f28ca046a894a31d953fb9014a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8edc6f437a3047f593fc706e4d9587c3",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c032ee196ffe4992a0eb175bb26d3358",
      "value": " 116/164 [07:15&lt;01:39,  2.07s/it]"
     }
    },
    "ac7ee576d9704909b64b208915e03a8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad93bc6d66074fe8b119243baa0f4cf8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bc96aba5b5da4c0ebac3137284bba1c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bd9aafdad8654a1f931330bf91cdebd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "c032ee196ffe4992a0eb175bb26d3358": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c4fdea457140450abff31fd1063400cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "d24d1c8a70294e0690041ffb9d45453d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de5dc6c1cc494e929f5d2d95813a17cc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ecd844ae701c4a31a9dd1f5cafde9078": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad93bc6d66074fe8b119243baa0f4cf8",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_bc96aba5b5da4c0ebac3137284bba1c1",
      "value": " 95.8M/95.8M [00:12&lt;00:00, 7.82MB/s]"
     }
    },
    "f1b83c7eddc44d2fb54726efc1848ce8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "f3dfe5dd75b24bada66d8ac1fc792fe4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "  0%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_de5dc6c1cc494e929f5d2d95813a17cc",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bd9aafdad8654a1f931330bf91cdebd3",
      "value": 0
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}