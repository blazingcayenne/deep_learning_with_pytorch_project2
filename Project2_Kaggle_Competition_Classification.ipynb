{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0-final"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhTA9Jbl3-5s"
      },
      "source": [
        "# <font style=\"color:blue\">Project 2: Kaggle Competition - Classification</font>\n",
        "\n",
        "#### Maximum Points: 100\n",
        "\n",
        "<div>\n",
        "    <table>\n",
        "        <tr><td><h3>Sr. no.</h3></td> <td><h3>Section</h3></td> <td><h3>Points</h3></td> </tr>\n",
        "        <tr><td><h3>1</h3></td> <td><h3>Data Loader</h3></td> <td><h3>10</h3></td> </tr>\n",
        "        <tr><td><h3>2</h3></td> <td><h3>Configuration</h3></td> <td><h3>5</h3></td> </tr>\n",
        "        <tr><td><h3>3</h3></td> <td><h3>Evaluation Metric</h3></td> <td><h3>10</h3></td> </tr>\n",
        "        <tr><td><h3>4</h3></td> <td><h3>Train and Validation</h3></td> <td><h3>5</h3></td> </tr>\n",
        "        <tr><td><h3>5</h3></td> <td><h3>Model</h3></td> <td><h3>5</h3></td> </tr>\n",
        "        <tr><td><h3>6</h3></td> <td><h3>Utils</h3></td> <td><h3>5</h3></td> </tr>\n",
        "        <tr><td><h3>7</h3></td> <td><h3>Experiment</h3></td><td><h3>5</h3></td> </tr>\n",
        "        <tr><td><h3>8</h3></td> <td><h3>TensorBoard Dev Scalars Log Link</h3></td> <td><h3>5</h3></td> </tr>\n",
        "        <tr><td><h3>9</h3></td> <td><h3>Kaggle Profile Link</h3></td> <td><h3>50</h3></td> </tr>\n",
        "    </table>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sDaF3ovpuwb"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This work is the second project of the OpenCV Pytroch Course's. Its focus is image classification. Half of the score is based on implementation. The other half is based on model performance.\n",
        "\n",
        "### Data Description\n",
        "\n",
        "The dataset consists of 8,174 images in 13 Kenyan food type classes. Sample images of KenyanFood13 dataset and the number of images in each of the classes are shown below:\n",
        "\n",
        "![Sample images from the KenyanFood13 Dataset](https://media.githubusercontent.com/media/blazingcayenne/deep_learning_with_pytorch_project2/main/images/KenyanFood13.jpg?raw=true)<br>\n",
        "**Figure 1:** A sample image from each class of the KenyanFood13 dataset.\n",
        "\n",
        "The data was split into public training set and private test set which is used for evaluation of submissions. The public set can be split into training and validation subsets.\n",
        "\n",
        "### Goal\n",
        "\n",
        "To create a model that predicts a type of food represented on each image in the KenyanFood13 dataset's private test set. Pre-trained models may be used. The performance metric is accuracy. To receive any performance point, an accuracy of 70% must be achieved. To receive full points, an accuracy of 75% must be achieved.\n",
        "\n",
        "### Approach\n",
        "\n",
        "Rather than create my own CNN architecture, I will explore transfer learning of models that have been trained on the ImageNet data set. Pedro Marcelino defines three strategies for fine-tuning pretrained models in **[Transfer learning from pre-trained models](https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751)**.\n",
        "\n",
        "* Train the entire model\n",
        "* Train some layers and leave others frozen\n",
        "* Train only the classifer by freezing the convolutional base\n",
        "\n",
        "Marcelino gives guidance as to which strategy to use based on one's dataset size and similarity (see Figure 2). Marcelino defines a small dataset as one with less than 1000 images per class. According to this definition, the KenyanFood13 dataset is small. However, it is unclear whether the application of data augmentation would reclassify the size of this dataset. Hence, this project will explore training the entire pretrained model as well as training some layers and leaving others frozen. Regarding dataset similarity, Marceline states:\n",
        "\n",
        "> ...  let common sense prevail. For example, if your task is to identify cats and dogs, ImageNet would be a similar dataset because it has images of cats and dogs. However, if your task is to identify cancer cells, ImageNet canâ€™t be considered a similar dataset.\n",
        "\n",
        "Fortunately, the ImageNet dataset contains 10 food classes: apple, banana, broccoli, burger, egg, french fries, hot dog, pizza, rice, and strawberry. Unfortunately, ImageNet's food images look significantly different from the images in Kenyan13Food dataset. Hence, the dataset similarity is moderate. Consequently, this project will also explore freezing the convolutional base and training only the classifier.\n",
        "\n",
        "![Decision map](https://media.githubusercontent.com/media/blazingcayenne/deep_learning_with_pytorch_project2/main/images/TransferLearningApproaches.png?raw=true)<br>\n",
        "**Figure 2:** Decision map for fine-tuning pre-trained models.\n",
        "\n",
        "According to the decision map, pre-trained models should be fine tuned by either freezing part or all of the convolutional base. This project will test the efficiency of this guidance.\n",
        "\n",
        "Dishashree Gupta also defines strategies for fine-tuning pre-trained models in his blog post, **[Transfer learning and the art of using Pre-trained Models in Deep Learning](https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/)**. Gupta's strategies, while similar to Marcelino, have the following differences.\n",
        "\n",
        "* Large datasets with low similarity should train models from scratch.\n",
        "* Large datasets with high similarity should train the entire pre-trained model.\n",
        "\n",
        "### Implementation Overview\n",
        "\n",
        "My primary objective is to fine-tune a pretrained model to achieve a minimum of 75% accuracy on the KenyanFood13 dataset. My secondary objective is to improve my proficiency with the [Python](https://www.python.org/) computer programming language. Prior to this class, my Python expertise was almost non-existent. Consequently, I developed class hierachies that enable the rapid implementation of fine-tuning experiments on following pre-trained TorchVision and EfficientNet models using any of Marcelino or Gupta's strategies.\n",
        "\n",
        "* ResNet-18\n",
        "* ResNet-34\n",
        "* ResNet-50\n",
        "* ResNet-101\n",
        "* ResNet-152\n",
        "* ResNeXt-50-32x4d\n",
        "* ResNeXt-101-32x8d\n",
        "* Wide ResNet-50-2\n",
        "* Wide ResNet-101-2\n",
        "* VGG-11 with batch normalization\n",
        "* VGG-13 with batch normalization\n",
        "* VGG-16 with batch normalization\n",
        "* VGG-19 with batch normalization\n",
        "* DenseNet-121\n",
        "* DenseNet-169\n",
        "* DenseNet-201\n",
        "* DenseNet-161\n",
        "* EfficientNet-B0\n",
        "* EfficientNet-B1\n",
        "* EfficientNet-B2\n",
        "* EfficientNet-B3\n",
        "* EfficientNet-B4\n",
        "* EfficientNet-B5\n",
        "* EfficientNet-B6\n",
        "* EfficientNet-B7\n",
        "\n",
        "I used and modified the Trainer Pipeline module introduced in **Week 6 - Best Practicing in Deep Learning > How to structure your Project for Scale**. Modications to the trainer module include, but are not limited to, the following:\n",
        "\n",
        "* additional configuration parameters,\n",
        "* saving the model state only when the average loss on the validation set reaches a new low,\n",
        "* prematurely stopping training when either the loss or accuracy does not significantly improve over a certain number of epochs, and\n",
        "* extending the visualization base and TensorBoard classes to support the logging of images, figures, graphs, and PR curves.\n",
        "\n",
        "Experiments are identified by three uppercase capital letters per the regular expression \\(\\[A-Z\\]\\[A-Z\\]\\[A-Z\\]\\). The first and second letters designate the experiment group and experiment set respectively. The last letter designates an individual experiment. Hence, all experiments that begin with \"A\" belong to Group A, while all experiments that begin with \"AB\" belong to Group A, Set B.\n",
        "\n",
        "I implemented the following groups of experiments.\n",
        "\n",
        "* Group A to explore the data and verify the training pipeline.\n",
        "* Group B to explore the impact of learning rate on training pretrained EfficientNet models.\n",
        "* Group C to explore the impact of EfficientNet model size and transfer learning approach on bias and variance.\n",
        "* Group D to explore the impact of additional regularization and dataset imbalance approaches.\n",
        "* Group E to explore the impact of a two-stage classifier.\n",
        "* Group F to explore training the Project 1 model from scratch.\n",
        "* Group G to explore and compare ResNet, ResNeXt, WideResnet, VGG, DenseNet, and EfficientNet models.\n",
        "* Group H to explore model ensembles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LNCmWgkBNLg"
      },
      "source": [
        "# This cell initializes the notebook for execution on different hosts.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "def get_host() -> str:\n",
        "    \"\"\"\n",
        "    The get_ipython() function returns the following from different hosts.\n",
        "\n",
        "    colab:  <google.colab._shell.Shell object at 0x7f23c5e386d8>\n",
        "    brule:  <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f1990f22a50>\n",
        "    kaggle: <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f9d093aebd0>\n",
        "    \"\"\"\n",
        "    \n",
        "    if 'google.colab' in str(get_ipython()):\n",
        "        return \"colab\"\n",
        "    else:\n",
        "        # ToDo: Determine whether running on kaggle.\n",
        "        return \"brule\"\n",
        "\n",
        "def init_host(host:str):\n",
        "    if host == \"brule\":\n",
        "        # set data and project directories\n",
        "        if os.path.isdir(\"./trainer\"):\n",
        "            data_dir = \"./data\"\n",
        "            proj_dir = \"./\"\n",
        "        elif os.path.isdir(\"./project2/trainer\"):\n",
        "            data_dir = \"./project2/data\"\n",
        "            proj_dir = \"./project2\"\n",
        "        else:\n",
        "            raise SystemExit(\"Cannot locate trainer module.\")\n",
        "\n",
        "    elif host == \"colab\":\n",
        "        # mount Google Drive\n",
        "        from google.colab import drive\n",
        "        drive.mount(\"/content/gdrive\")\n",
        "\n",
        "        # set data and project directories\n",
        "        data_dir = \"/content/data\"\n",
        "        proj_dir = \"/content/gdrive/MyDrive/Colab Notebooks/project2\"\n",
        "\n",
        "        # fetching data from Google Drive is very, very slow ...\n",
        "        # hence, we will unzip the dataset to /content/data if it is not there\n",
        "        dataset = os.path.join(proj_dir, \"data\", \"pytorch-opencv-course-classification.zip\")\n",
        "        if not os.path.isdir(data_dir):\n",
        "              os.makedirs(data_dir)\n",
        "              import zipfile\n",
        "              with zipfile.ZipFile(dataset, 'r') as zip_ref:\n",
        "                  zip_ref.extractall(data_dir)              \n",
        "\n",
        "    else:\n",
        "        raise SystemExit(\"Unknown host! Cannot continue.\")\n",
        "\n",
        "    sys.path.append(proj_dir)\n",
        "    return data_dir, proj_dir\n",
        "\n",
        "data_dir, proj_dir = init_host(get_host())\n",
        "\n",
        "print(f\"data_dir: {data_dir}\")\n",
        "!ls -lh {data_dir.replace(\" \", \"\\\\ \")}\n",
        "\n",
        "print(f\"proj_dir: {proj_dir}\")\n",
        "!ls -lh {proj_dir.replace(\" \", \"\\\\ \")}"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_dir: ./project2/data\n",
            "total 276K\n",
            "drwxr-xr-x 3 1002 1003 4.0K Jan 10 04:07 images\n",
            "-rwxr-xr-x 1 1002 1003  47K Nov 14 10:51 sample_submission.csv\n",
            "-rwxr-xr-x 1 1002 1003  33K Nov 14 10:51 test.csv\n",
            "-rwxr-xr-x 1 1002 1003 185K Nov 14 10:51 train.csv\n",
            "proj_dir: ./project2\n",
            "lrwxrwxrwx 1 root root 40 Jan 13 01:56 ./project2 -> /home/kevinkramer/class/week07/project2/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xiq8YhMC3-5w"
      },
      "source": [
        "# import organzier @ https://pypi.org/project/importanize/\n",
        "\n",
        "from abc import ABC, abstractmethod, abstractproperty\n",
        "from collections import namedtuple\n",
        "from dataclasses import dataclass, replace\n",
        "from enum import Enum, auto\n",
        "from operator import itemgetter\n",
        "from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union\n",
        "\n",
        "import itertools\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import PIL\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, Dataset, Sampler, WeightedRandomSampler\n",
        "from torchvision import models, transforms\n",
        "\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "from trainer import Trainer, configuration, hooks\n",
        "from trainer.configuration import SystemConfig\n",
        "from trainer.configuration import DataAugConfig, DatasetConfig, DataLoaderConfig\n",
        "from trainer.configuration import OptimizerConfig, SchedulerConfig, TrainerConfig\n",
        "from trainer.metrics import AccuracyEstimator\n",
        "from trainer.tensorboard_visualizer import TensorBoardVisualizer\n",
        "from trainer.utils import patch_configs, setup_system"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWi_M1eB3-5x"
      },
      "source": [
        "## <font style=\"color:green\">1. Data Loader [10 Points]</font>\n",
        "\n",
        "In this section, you have to write a class or methods that will be used to get training and validation data\n",
        "loader.\n",
        "\n",
        "You will have to write a custom dataset class to load data.\n",
        "\n",
        "**Note that there are not separate validation data, so you will have to create your validation set by dividing train data into train and validation data. Usually, in practice, we do `80:20` ratio for train and validation, respectively.** \n",
        "\n",
        "For example,\n",
        "\n",
        "```\n",
        "class KenyanFood13Dataset(Dataset):\n",
        "    \"\"\"\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, *args):\n",
        "    ....\n",
        "    ...\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "    ...\n",
        "    ...\n",
        "    \n",
        "    \n",
        "```\n",
        "\n",
        "```\n",
        "def get_data(args1, *agrs):\n",
        "    ....\n",
        "    ....\n",
        "    return train_loader, test_loader\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KF13Dataset(Dataset):\n",
        "    \"\"\"\n",
        "    This custom PyTorch dataset contains images and classification labels from\n",
        "    Kaggle's KenyanFood13 dataset.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, image_root, fnames, labels=None, transform=None):\n",
        "        super().__init__()\n",
        "        self.__fnames = fnames\n",
        "        self.__labels = labels\n",
        "        self.__transform = transform\n",
        "        self.__image_root = image_root\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the dataset's length, i.e., the number of image/label pairs.\n",
        "        \"\"\"\n",
        "\n",
        "        return len(self.__fnames)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns the (optionally resized & preprocessed) image that corresponds to the specified index.\n",
        "        \"\"\"\n",
        "\n",
        "        # conversion needed to remove alpha channel, if present\n",
        "        path = os.path.join(self.__image_root, self.__fnames[idx] + \".jpg\")\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "        \n",
        "        if self.__transform is not None:\n",
        "            image = self.__transform(image)\n",
        "\n",
        "        if self.__labels is None:\n",
        "            extra = self.__fnames[idx]  # return file name with image\n",
        "        else:\n",
        "            extra = self.__labels[idx]  # return target with image\n",
        "\n",
        "        return image, extra\n",
        "\n",
        "\n",
        "class KF13IndexedDataset(Dataset):\n",
        "    \"\"\"\n",
        "    This custom PyTorch dataset contains images and classification labels from\n",
        "    Kaggle's KenyanFood13 dataset.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, image_root, indices, fnames, labels=None, transform=None):\n",
        "        super().__init__()\n",
        "        self.__fnames = fnames\n",
        "        self.__labels = labels\n",
        "        self.__indices = indices\n",
        "        self.__transform = transform\n",
        "        self.__image_root = image_root\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the dataset's length, i.e., the number of image/label pairs.\n",
        "        \"\"\"\n",
        "\n",
        "        return len(self.__indices)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns the (optionally resized & preprocessed) image that corresponds to the specified index.\n",
        "        \"\"\"\n",
        "        idx = self.__indices[idx]\n",
        "\n",
        "        # conversion needed to remove alpha channel, if present\n",
        "        path = os.path.join(self.__image_root, self.__fnames[idx] + \".jpg\")\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "        \n",
        "        if self.__transform is not None:\n",
        "            image = self.__transform(image)\n",
        "\n",
        "        if self.__labels is None:\n",
        "            extra = self.__fnames[idx]  # return file name with image\n",
        "        else:\n",
        "            extra = self.__labels[idx]  # return target with image\n",
        "\n",
        "        return idx, image, extra\n",
        "\n",
        "\n",
        "class MySampler(Sampler):\n",
        "    def __init__(self, labels):\n",
        "        unique = np.unique(labels)\n",
        "        self.num_labels = len(labels)\n",
        "\n",
        "        self.lottery = list(range(len(unique)))\n",
        "        self.lottery_iter = None\n",
        "        self.__init_lottery()\n",
        "\n",
        "        self.indices = [[idx for idx, label in enumerate(labels) if label == target] for target in unique]\n",
        "        self.indices_iters = [None for _ in unique]\n",
        "        for idx in range(len(unique)):\n",
        "            self.__init_indices(idx)\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.count = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        self.count += 1\n",
        "        if self.count >= self.num_labels:\n",
        "            raise StopIteration\n",
        "        \n",
        "        label = next(self.lottery_iter, None)\n",
        "        if label is None:\n",
        "            self.__init_lottery()\n",
        "            label = next(self.lottery_iter)\n",
        "\n",
        "        index = next(self.indices_iters[label], None)\n",
        "        if index is None:\n",
        "            self.__init_indices(label)\n",
        "            index = next(self.indices_iters[label])\n",
        "\n",
        "        return index\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_labels\n",
        "\n",
        "    def __init_lottery(self):\n",
        "        random.shuffle(self.lottery)\n",
        "        self.lottery_iter = iter(self.lottery)\n",
        "\n",
        "    def __init_indices(self, label):\n",
        "        random.shuffle(self.indices[label])\n",
        "        self.indices_iters[label] = iter(self.indices[label])\n",
        "\n",
        "\n",
        "class KF13TrainingData:\n",
        "    \"\"\"\n",
        "    Splits the specified data into training and validation sets preserving the\n",
        "    relative ratios of the number of images of each class type.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self, \n",
        "        image_root: str,\n",
        "        class_names: List[str],\n",
        "        fnames: List[str], \n",
        "        labels: List[int],\n",
        "        mapping: Optional[List[int]] = None,\n",
        "        subset: bool = False,\n",
        "        valid_size: float = 0.2, \n",
        "        random_seed: int = 42\n",
        "    ):\n",
        "        self.__mapping = mapping\n",
        "        self.__image_root = image_root\n",
        "        self.__class_names = class_names\n",
        "        self.__valid_size = valid_size\n",
        "\n",
        "        # split the training data into training and validation sets\n",
        "        train_fnames, valid_fnames, train_labels, valid_labels = train_test_split(\n",
        "            fnames,                      # image file names w/o path or extension\n",
        "            labels,                      # image labels\n",
        "            test_size = valid_size,      # test size\n",
        "            random_state = random_seed,  # random seed for reproducibility\n",
        "            shuffle = True,              # shuffle data before splitting into training and validation sets\n",
        "            stratify = labels            # maintain equal class representation in training and validation sets\n",
        "        )\n",
        "        \n",
        "        if subset:\n",
        "            subset_size = 256.0 / len(train_fnames)\n",
        "            _, train_fnames_subset, _, train_labels_subset = train_test_split(\n",
        "                train_fnames,\n",
        "                train_labels,\n",
        "                test_size = subset_size,\n",
        "                random_state = random_seed,\n",
        "                shuffle = True,\n",
        "                stratify = train_labels\n",
        "            )\n",
        "            _, valid_fnames_subset, _, valid_labels_subset = train_test_split(\n",
        "                valid_fnames,\n",
        "                valid_labels,\n",
        "                test_size = subset_size,\n",
        "                random_state = random_seed,\n",
        "                shuffle = True,\n",
        "                stratify = valid_labels\n",
        "            )\n",
        "            #subset_labels = train_labels_subset + valid_labels_subset\n",
        "            #_, class_counts = np.unique(subset_labels, return_counts=True)\n",
        "            #self.__class_counts = class_counts.tolist()\n",
        "            train_fnames = train_fnames_subset\n",
        "            train_labels = train_labels_subset\n",
        "            valid_fnames = valid_fnames_subset\n",
        "            valid_labels = valid_labels_subset\n",
        "\n",
        "        # ToDo: filter data\n",
        "        if mapping is not None:\n",
        "            train_fnames, train_labels = self.__map_data(train_fnames, train_labels)\n",
        "            valid_fnames, valid_labels = self.__map_data(valid_fnames, valid_labels)\n",
        "\n",
        "        _, class_counts = np.unique(train_labels + valid_labels, return_counts=True)\n",
        "\n",
        "        self.__class_counts = class_counts.tolist()\n",
        "        self.__train_fnames = train_fnames\n",
        "        self.__train_labels = train_labels\n",
        "        self.__valid_fnames = valid_fnames\n",
        "        self.__valid_labels = valid_labels\n",
        "\n",
        "        class_weights = 1. / np.array(self.__class_counts)\n",
        "        class_weights = class_weights / np.sum(class_weights)\n",
        "        self.__class_weights = class_weights.tolist()\n",
        "\n",
        "    @property\n",
        "    def valid_size(self) -> float:\n",
        "        return self.__valid_size\n",
        "\n",
        "    @property\n",
        "    def class_names(self) -> List[str]:\n",
        "        return self.__class_names\n",
        "    \n",
        "    @property\n",
        "    def class_counts(self) -> List[int]:\n",
        "        return self.__class_counts\n",
        "\n",
        "    @property\n",
        "    def class_weights(self) -> List[float]:\n",
        "        return self.__class_weights\n",
        "\n",
        "    @property\n",
        "    def train_fnames(self) -> List[str]:\n",
        "        return self.__train_fnames\n",
        "\n",
        "    @property\n",
        "    def train_labels(self) -> List[int]:\n",
        "        return self.__train_labels\n",
        "\n",
        "    @property\n",
        "    def valid_fnames(self) -> List[str]:\n",
        "        return self.__valid_fnames\n",
        "\n",
        "    @property\n",
        "    def valid_labels(self) -> List[int]:\n",
        "        return self.__valid_labels\n",
        "\n",
        "    @property\n",
        "    def mapping(self) -> List[int]:\n",
        "        return self.__mapping\n",
        "\n",
        "    def get_train_data_loader(\n",
        "        self,\n",
        "        transform: Iterable[Callable],\n",
        "        batch_size = 16, \n",
        "        num_workers = 2,\n",
        "        use_my_sampler: bool = False,\n",
        "        use_random_sampler: bool = False\n",
        "    ) -> DataLoader:\n",
        "        return DataLoader(\n",
        "            dataset = self.__get_train_dataset(transform),\n",
        "            sampler = self.__get_sampler(use_my_sampler, use_random_sampler),\n",
        "            batch_size = batch_size,\n",
        "            num_workers = num_workers,\n",
        "            shuffle = not (use_my_sampler or use_random_sampler)\n",
        "        )\n",
        "\n",
        "    def get_valid_data_loader(\n",
        "        self,\n",
        "        transform: Iterable[Callable],\n",
        "        batch_size = 16, \n",
        "        num_workers = 2\n",
        "    ) -> DataLoader:\n",
        "        return DataLoader(\n",
        "            dataset = self.__get_valid_dataset(transform),\n",
        "            batch_size = batch_size,\n",
        "            num_workers = num_workers,\n",
        "            shuffle = False\n",
        "        )\n",
        "\n",
        "    def __map_data(self, fnames, labels):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        count = len(self.__class_names)\n",
        "        labels = [self.__mapping[label] for label in labels]\n",
        "        mapflt = [label < count for label in labels]\n",
        "        labels = [label for (label, fltval) in zip(labels, mapflt) if fltval]\n",
        "        fnames = [fname for (fname, fltval) in zip(fnames, mapflt) if fltval]\n",
        "        return fnames, labels\n",
        "\n",
        "    def __get_sampler(self, use_my_sampler, use_random_sampler) -> Optional[Sampler]:\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        if use_my_sampler:\n",
        "            return MySampler(self.__train_labels)\n",
        "        elif use_random_sampler:\n",
        "            weights = [self.__class_weights[e] for e in self.__train_labels]\n",
        "            return WeightedRandomSampler(torch.DoubleTensor(weights), len(weights))\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def __get_train_dataset(\n",
        "        self, \n",
        "        transform: Iterable[Callable]\n",
        "    ) -> Dataset:\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        return KF13Dataset(\n",
        "            image_root = self.__image_root,\n",
        "            fnames = self.__train_fnames,\n",
        "            labels = self.__train_labels,\n",
        "            transform = transform\n",
        "        )\n",
        "\n",
        "    def __get_valid_dataset(\n",
        "        self, \n",
        "        transform: Iterable[Callable]\n",
        "    ) -> Dataset:\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        return KF13Dataset(\n",
        "            image_root = self.__image_root,\n",
        "            fnames = self.__valid_fnames,\n",
        "            labels = self.__valid_labels,\n",
        "            transform = transform\n",
        "        )\n",
        "\n",
        "\n",
        "class KF13Datastore:\n",
        "    \"\"\"\n",
        "    This class parses the KenyanFood13's test.csv and train.csv files. It has\n",
        "    properties to retrieve the following.\n",
        "\n",
        "          - a list of class names (classes)\n",
        "          - a list of class counts (class_counts)\n",
        "          - a list of test filenames (test_fnames)\n",
        "          - a dictionary of filenames for each class (library)\n",
        "\n",
        "    In addition, it has a method to create KF13TrainingData instances, which\n",
        "    splits the training data into training and validation sets. In addition,\n",
        "    this method can group classes together and create datasets to train a two\n",
        "    stage classifer.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, data_root, valid_size = 0.2, random_seed = 42):\n",
        "        self.__valid_size = valid_size\n",
        "        self.__random_seed = random_seed\n",
        "\n",
        "        # the root path of the images\n",
        "        image_root = os.path.join(data_root, 'images', 'images')\n",
        "        self.__image_root = image_root\n",
        "        \n",
        "        # parse the test CSV file to obtain filenames (labels are not given)\n",
        "        test_data_frame = self.__parse_data_file(data_root, 'test.csv')\n",
        "        test_fnames = test_data_frame.values[:,0].tolist()\n",
        "        self.__test_fnames = test_fnames\n",
        "        \n",
        "        # parse the train CSV file to obtain filenames and labels       \n",
        "        train_data_frame = self.__parse_data_file(data_root, 'train.csv')\n",
        "        fnames = train_data_frame.values[:,0].tolist()\n",
        "        cnames = train_data_frame.values[:,1].tolist()\n",
        "        \n",
        "        # get the classes and class counts\n",
        "        class_names, class_counts = np.unique(cnames, return_counts=True)\n",
        "        class_names = class_names.tolist()\n",
        "        class_counts = class_counts.tolist()\n",
        "        num_classes = len(class_names)\n",
        "        self.__class_names = class_names\n",
        "        self.__class_counts = class_counts\n",
        "        \n",
        "        # create a dictionary of text labels to integer labels\n",
        "        cname_to_label = {\n",
        "            key : val for key, val in zip(class_names, np.arange(num_classes))\n",
        "        }\n",
        "        self.__cname_to_label = cname_to_label\n",
        "\n",
        "        # convert class names to labels (their numeric equivalents)\n",
        "        labels = [cname_to_label[cname] for cname in cnames]\n",
        "        self.__train_fnames = fnames\n",
        "        self.__train_labels = labels\n",
        "\n",
        "        # create a dictionary that maps class names to labels\n",
        "        self.__class_dict = {\n",
        "            name: label \n",
        "            for (name, label) \n",
        "            in zip(class_names, range(len(class_names)))\n",
        "        }\n",
        "\n",
        "        # create a dictionary that groups fnames according to their labels\n",
        "        self.__library = {\n",
        "            key : [fname for fname, label in zip(fnames, labels) if label == key] \n",
        "            for key in range(num_classes)\n",
        "        }\n",
        "\n",
        "    @property\n",
        "    def image_root(self) -> List[str]:\n",
        "        return self.__image_root\n",
        "\n",
        "    @property\n",
        "    def class_names(self) -> List[str]:\n",
        "        return self.__class_names\n",
        "    \n",
        "    @property\n",
        "    def class_counts(self) -> List[int]:\n",
        "        return self.__class_counts\n",
        "    \n",
        "    @property\n",
        "    def class_dict(self) -> Dict[str, int]:\n",
        "        return self.__class_dict\n",
        "\n",
        "    @property\n",
        "    def train_fnames(self) -> List[str]:\n",
        "        return self.__train_fnames\n",
        "\n",
        "    @property\n",
        "    def train_labels(self) -> List[int]:\n",
        "        return self.__train_labels\n",
        "\n",
        "    @property\n",
        "    def test_fnames(self) -> List[str]:\n",
        "        return self.__test_fnames\n",
        "\n",
        "    @property\n",
        "    def library(self) -> Dict[int, List[str]]:\n",
        "          return self.__library\n",
        "\n",
        "    def create_stages(\n",
        "        self,\n",
        "        stage_info: List[Union[str, Tuple[str, List[str]]]]\n",
        "    ):\n",
        "        from queue import Queue\n",
        "        stages = []\n",
        "        queue = Queue(maxsize = len(self.__class_names))\n",
        "        queue.put(stage_info)\n",
        "        while not queue.empty():\n",
        "            names = []\n",
        "            items = queue.get()\n",
        "            mapping = [len(items)] * len(self.__class_names)\n",
        "            for idx, item in enumerate(items):\n",
        "                if isinstance(item, str):\n",
        "                    names.append(item)\n",
        "                    mapping[self.__class_dict[item]] = idx\n",
        "                else:\n",
        "                    queue.put(item[1])\n",
        "                    names.append(item[0])\n",
        "                    for name in item[1]:\n",
        "                        mapping[self.__class_dict[name]] = idx\n",
        "            stages.append((names, mapping))\n",
        "        return stages\n",
        "\n",
        "    def get_training_data(\n",
        "        self, \n",
        "        subset:bool = False,\n",
        "        random_seed:Optional[int] = None\n",
        "    ) -> KF13TrainingData:\n",
        "        if random_seed is None:\n",
        "            random_seed = self.__random_seed\n",
        "        return KF13TrainingData(\n",
        "            image_root = self.__image_root,\n",
        "            class_names = self.__class_names,\n",
        "            fnames = self.__train_fnames, \n",
        "            labels = self.__train_labels,\n",
        "            subset = subset,\n",
        "            valid_size = self.__valid_size, \n",
        "            random_seed = random_seed\n",
        "        )\n",
        "\n",
        "    def get_two_stage_training_data(\n",
        "        self,\n",
        "        stage_info: List[Union[str, Tuple[str, List[str]]]]\n",
        "    ) -> List[Tuple[str, KF13TrainingData]]:\n",
        "\n",
        "        # parse the stage information\n",
        "        stages = self.create_stages(stage_info)\n",
        "\n",
        "        # create stage names\n",
        "        names = [\"Stage1\"]\n",
        "        for idx in range(len(stages) - 1):\n",
        "            names.append(\"Stage2\" + chr(ord('a'[0]) + idx))\n",
        "\n",
        "        # loop through each stage\n",
        "        data = []\n",
        "        for name, stage in zip(names, stages):\n",
        "            data.append((\n",
        "                name,\n",
        "                KF13TrainingData(\n",
        "                    image_root = self.__image_root,\n",
        "                    class_names = stage[0],\n",
        "                    fnames = self.__train_fnames,\n",
        "                    labels = self.__train_labels,\n",
        "                    mapping = stage[1],\n",
        "                    valid_size = self.__valid_size, \n",
        "                    random_seed = self.__random_seed,\n",
        "                )\n",
        "            ))\n",
        "        \n",
        "        return data\n",
        "\n",
        "    def get_test_data_loader(\n",
        "        self,\n",
        "        transform: Iterable[Callable],\n",
        "        batch_size = 16, \n",
        "        num_workers = 2\n",
        "    ) -> DataLoader:\n",
        "        return DataLoader(\n",
        "            dataset = self.__get_test_dataset(transform),\n",
        "            batch_size = batch_size,\n",
        "            num_workers = num_workers,\n",
        "            shuffle = False\n",
        "        )\n",
        "\n",
        "    def __parse_data_file(self, data_root, file):\n",
        "        return pd.read_csv(\n",
        "            os.path.join(data_root, file), \n",
        "            delimiter=',', \n",
        "            dtype={'id': 'str'}, \n",
        "            engine='python'\n",
        "        )\n",
        "\n",
        "    def __get_test_dataset(\n",
        "        self, \n",
        "        transform: Iterable[Callable]\n",
        "    ) -> Dataset:\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        return KF13Dataset(\n",
        "            image_root = self.__image_root,\n",
        "            fnames = self.__test_fnames,\n",
        "            labels = None,\n",
        "            transform = transform\n",
        "        )"
      ]
    },
    {
      "source": [
        "### Data Imbalance\n",
        "\n",
        "The following table enumerates the number of images in each class. The most represented class, chapati, has approximately five times more images than the least represented class, kukuchoma.\n",
        "\n",
        "|Class|Images|\n",
        "|:---|:---:|\n",
        "|bhaji|632|\n",
        "|chapati|862|\n",
        "|githeri|479|\n",
        "|kachumbari|494|\n",
        "|kukuchoma|173|\n",
        "|mandazi|620|\n",
        "|masalachips|438|\n",
        "|matoke|483|\n",
        "|mukimo|212|\n",
        "|nyamachoma|784|\n",
        "|pilau|329|\n",
        "|sukumawiki|402|\n",
        "|ugali|628|\n",
        "\n",
        "This project will explore two approaches of handling this data imbalance.\n",
        "\n",
        "1. Use a weighted loss function.\n",
        "2. Use a weighted random sampler."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DIApproach(Enum):\n",
        "    NONE = auto()   \n",
        "    WEIGHTED_LOSS_FUNCTION = auto()\n",
        "    WEIGHTED_RANDOM_SAMPLER = auto()\n",
        "    MY_SAMPLER = auto()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_mean_std(data_loader=None):\n",
        "    \n",
        "    if data_loader is None:\n",
        "        \"\"\"\n",
        "        Returns the mean and standard deviation used by the pretrained\n",
        "        classification models.\n",
        "        \"\"\"\n",
        "\n",
        "        mean = [0.485, 0.456, 0.406] \n",
        "        std = [0.229, 0.224, 0.225]\n",
        "    \n",
        "    else:\n",
        "        \"\"\"\n",
        "        Computes the mean and standard deviation of the images returned\n",
        "        by the specified data loader. \n",
        "        \n",
        "        For comparision, the mean and standard deviation of the KenyanFood13\n",
        "        images using the train_dataset and preprocess transforms is as follows.\n",
        "        \n",
        "            mean = [0.5778, 0.4631, 0.3471], \n",
        "            std = [0.2380, 0.2461, 0.2464]):\n",
        "        \"\"\"\n",
        "        \n",
        "        std = 0.\n",
        "        mean = 0.\n",
        "        for images, _ in data_loader:\n",
        "            batch_samples = images.size(0)\n",
        "            images = images.view(batch_samples, images.size(1), -1)\n",
        "            std += images.std(2).sum(0)\n",
        "            mean += images.mean(2).sum(0)\n",
        "        std /= len(data_loader.dataset)\n",
        "        mean /= len(data_loader.dataset)\n",
        "\n",
        "    return mean, std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RandomGaussianNoise(object):\n",
        "    def __init__(self, p=0.5, mean=0., std=1.):\n",
        "        self.p = p\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "        \n",
        "    def __call__(self, tensor):\n",
        "        if random.random() < self.p:\n",
        "            return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
        "        return tensor\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + f\"(p={self.p}, mean={self.mean}, std={self.std})\"\n",
        "\n",
        "\n",
        "class ImageTransforms:\n",
        "    \"\"\"\n",
        "    This utility class has methods to create transforms used to train and evaluate a model as\n",
        "    well as visualize images.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "            self, \n",
        "            resize = 256, \n",
        "            crop_size = 224, \n",
        "            mean = [0.485, 0.456, 0.406], \n",
        "            std = [0.229, 0.224, 0.225],\n",
        "            config = DataAugConfig()\n",
        "        ):\n",
        "        self.__resize = resize\n",
        "        self.__crop_size = crop_size\n",
        "        self.__mean = mean\n",
        "        self.__std = std\n",
        "        self.__config = config\n",
        "\n",
        "    def preprocess(self, augment=False):\n",
        "        \"\"\"\n",
        "        These transformations convert PIL images to uniformly sized tensors whose dimensions\n",
        "        are crop_size x crop_size pixels. If the augment parameter is True, then the following\n",
        "        data augmentation transforms are applied: color jitter, horizontal flip, vertical flip,\n",
        "        rotation, translation, scaling, and erasing.\n",
        "        \"\"\"\n",
        "        return transforms.Compose(self.__create_transform_list(normalize=False, augment=augment))\n",
        "    \n",
        "    def common(self):\n",
        "        \"\"\"\n",
        "        These transformations convert PIL images to uniformly sized tensors whose dimensions\n",
        "        are crop_size x crop_size pixels and values are normalized by the mean and standard\n",
        "        deviation.\n",
        "        \"\"\"\n",
        "        return transforms.Compose(self.__create_transform_list(normalize=True, augment=False))\n",
        "    \n",
        "    def augment(self):\n",
        "        \"\"\"\n",
        "        These transformations convert PIL images to uniformly sized tensors whose dimensions\n",
        "        are crop_size x crop_size pixels and values are normalized by the mean and standard\n",
        "        deviation with the following data random augmentations: color jitter, horizontal flip,\n",
        "        vertical flip, rotation, translation, scaling, and erasing.\n",
        "        \"\"\"\n",
        "        return transforms.Compose(self.__create_transform_list(normalize=True, augment=True))\n",
        "\n",
        "    def __create_transform_list(self, normalize, augment):\n",
        "        tlist = []\n",
        "\n",
        "        # resize before data augmentation to reduce execution time\n",
        "        tlist.append(transforms.Resize(\n",
        "            size = self.__resize, \n",
        "            interpolation = PIL.Image.BILINEAR\n",
        "        ))\n",
        "\n",
        "        if augment:\n",
        "            # apply rotation before center cropping to avoid \"corner voids\"\n",
        "            tlist.extend(self.__get_color_jitter())\n",
        "            tlist.extend(self.__get_random_vertical_flip())\n",
        "            tlist.extend(self.__get_random_horizontal_flip())\n",
        "            tlist.extend(self.__get_random_affine())\n",
        "\n",
        "        tlist.append(transforms.CenterCrop(self.__crop_size))\n",
        "        tlist.append(transforms.ToTensor())\n",
        "\n",
        "        if normalize:\n",
        "            tlist.append(transforms.Normalize(self.__mean, self.__std, inplace=True))\n",
        "\n",
        "        if augment:\n",
        "            tlist.extend(self.__get_random_erasing())\n",
        "            tlist.extend(self.__get_random_noise())\n",
        "\n",
        "        return tlist\n",
        "\n",
        "    def __get_color_jitter(self):\n",
        "        tlist = []\n",
        "        if self.__config.color_enabled:\n",
        "            tlist.append(transforms.ColorJitter(\n",
        "                brightness = self.__config.color_brightness, \n",
        "                contrast = self.__config.color_contrast, \n",
        "                saturation = self.__config.color_saturation, \n",
        "                hue = self.__config.color_hue\n",
        "            ))\n",
        "        return tlist\n",
        "\n",
        "    def __get_random_vertical_flip(self):\n",
        "        tlist = []\n",
        "        if self.__config.vert_flip_prob > 0:\n",
        "            tlist.append(transforms.RandomVerticalFlip(\n",
        "                p=self.__config.vert_flip_prob\n",
        "            ))\n",
        "        return tlist \n",
        "\n",
        "    def __get_random_horizontal_flip(self):\n",
        "        tlist = []\n",
        "        if self.__config.horz_flip_prob > 0:\n",
        "            tlist.append(transforms.RandomHorizontalFlip(\n",
        "                p=self.__config.horz_flip_prob\n",
        "            ))\n",
        "        return tlist \n",
        "\n",
        "    def __get_random_affine(self):\n",
        "        tlist = []\n",
        "        if self.__config.affine_enabled:\n",
        "            tlist.append(transforms.RandomAffine(\n",
        "                degrees = self.__config.affine_rotation,\n",
        "                translate = self.__null_check(self.__config.affine_translate),\n",
        "                scale = self.__null_check(self.__config.affine_scale),\n",
        "                shear = self.__null_check(self.__config.affine_shear),\n",
        "                resample=PIL.Image.BILINEAR # ToDo: Test BICUBIC\n",
        "            ))\n",
        "        return tlist\n",
        "\n",
        "    def __get_random_erasing(self):\n",
        "        tlist = []\n",
        "        if self.__config.erasing_prob > 0:\n",
        "            tlist.append(transforms.RandomErasing(\n",
        "                p = self.__config.erasing_prob,\n",
        "                scale = self.__config.erasing_scale,\n",
        "                ratio = self.__config.erasing_ratio,\n",
        "                value = \"random\" if self.__config.erasing_random else 0,\n",
        "                inplace = True\n",
        "            ))\n",
        "        return tlist\n",
        "\n",
        "    def __get_random_noise(self):\n",
        "        tlist = []\n",
        "        if self.__config.noise_prob > 0:\n",
        "            tlist.append(RandomGaussianNoise(\n",
        "                p = self.__config.noise_prob,\n",
        "                mean = self.__config.noise_mean,\n",
        "                std = self.__config.noise_std\n",
        "            ))\n",
        "        return tlist\n",
        "\n",
        "    def __null_check(self, tuple: Tuple[float, float]) -> Optional[Tuple[float, float]]:\n",
        "        return tuple if tuple != (0.0, 0.0) else None"
      ]
    },
    {
      "source": [
        "### Data Loader Tests\n",
        "\n",
        "#### Without Weighted Random Sample\n",
        "\n",
        "Since I have not used the weighted random sampler, I wanted to run tests to explore its behavior. The `analyze_data_loader` function iterates through the data loader and counts the number of occurences of each class type. This function returns the tally as a list. The first test is on a data loader without the weighted random sampler. As expected, the tally's counts equal the number of training images for each class.\n",
        "\n",
        "|Class|Training|Loader|\n",
        "|:---|:---:|:---:|\n",
        "|bhaji|506|506|\n",
        "|chapati|690|690|\n",
        "|githeri|383|383|\n",
        "|kachumbari|395|395|\n",
        "|kukuchoma|138|138|\n",
        "|mandazi|496|496|\n",
        "|masalachips|350|350|\n",
        "|matoke|386|386|\n",
        "|mukimo|170|170|\n",
        "|nyamachoma|627|627|\n",
        "|pilau|263|263|\n",
        "|sukumawiki|322|322|\n",
        "|ugali|502|502|\n",
        "\n",
        "#### With Weighted Random Sample\n",
        "\n",
        "Repeating the previous experiment on a data loader with the random weighted sampler still had significant variations in the number of loaded images per class. Consequently, I decided to average tallies over many complete data loader cycles. These averages converged to image samples per class that were approximately equal (within 2%). The results of these experiments are documented below.\n",
        "\n",
        "|Iteration|Average Tally|\n",
        "|:---:|:---:|\n",
        "|Iteration|Average Tally|\n",
        "|:---:|:---:|\n",
        "|1|\\[492, 492, 478, 525, 546, 538, 470, 514, 535, 491, 502, 477, 476]|\n",
        "|2|\\[492, 501, 498, 519, 526, 529, 502, 515, 524, 491, 492, 479, 471]|\n",
        "|3|\\[492, 504, 512, 511, 525, 513, 498, 527, 514, 500, 482, 481, 478]|\n",
        "|4|\\[494, 509, 504, 515, 517, 512, 502, 517, 511, 495, 482, 493, 487]|\n",
        "|5|\\[491, 512, 498, 507, 514, 514, 504, 513, 523, 495, 487, 487, 491]|\n",
        "|6|\\[497, 515, 489, 502, 520, 508, 503, 513, 522, 499, 488, 486, 493]|\n",
        "|7|\\[501, 508, 494, 502, 518, 506, 504, 511, 515, 499, 494, 487, 498]|\n",
        "|8|\\[503, 509, 494, 507, 515, 508, 502, 510, 509, 497, 497, 488, 497]|\n",
        "|9|\\[500, 506, 500, 508, 517, 506, 504, 505, 508, 500, 498, 488, 496]|\n",
        "|10|\\[499, 504, 501, 509, 517, 505, 504, 505, 506, 501, 498, 488, 499]|\n",
        "|.|.|\n",
        "|.|.|\n",
        "|.|.|\n",
        "|66|\\[504, 498, 505, 502, 509, 500, 497, 505, 505, 505, 500, 501, 506]|\n",
        "|67|\\[505, 499, 505, 502, 508, 500, 497, 504, 505, 505, 500, 501, 505]|\n",
        "|68|\\[505, 499, 505, 502, 509, 501, 497, 505, 504, 506, 499, 500, 505]|\n",
        "|69|\\[505, 499, 505, 502, 508, 500, 497, 505, 505, 505, 499, 500, 505]|\n",
        "|70|\\[505, 499, 504, 502, 508, 501, 498, 505, 505, 505, 499, 500, 505]|\n",
        "\n",
        "While the above test shows that each class of images is equally represented, it does not tell whether each image is equally represented. Consequently, another test was implemented to track how many times an image was loaded. The following table enumerates the image load statistics for each class. If the images were equally represented, the minimum, maximum, and average values would be approximately equal. The Ave*CC column is the product of the average and class count, i.e., the number of images in that class.\n",
        "\n",
        "|Class|Minimum|Maximum|Average|Ave*CC|\n",
        "|:---|:---:|:---:|:---:|:---:|\n",
        "|bhaji|33|86|57|35876|\n",
        "|chapati|22|60|41|35418|\n",
        "|githeri|51|103|75|35798|\n",
        "|kachumbari|49|98|72|35649|\n",
        "|kukuchoma|172|249|209|36076|\n",
        "|mandazi|38|88|57|35551|\n",
        "|masalachips|50|109|81|35327|\n",
        "|matoke|52|103|74|35872|\n",
        "|mukimo|140|206|169|35823|\n",
        "|nyamachoma|28|74|46|35874|\n",
        "|pilau|79|141|108|35438|\n",
        "|sukumawiki|61|114|88|35519|\n",
        "|ugali|36|77|57|35835|\n",
        "\n",
        "Since the images are not equally represented, a custom sampler was developed and the above tests rerun. Each class is well represented.\n",
        "\n",
        "|Iteration|Average Tally|\n",
        "|:---:|:---:|\n",
        "|1|\\[503, 503, 503, 503, 502, 503, 503, 503, 502, 503, 502, 503, 502]|\n",
        "|2|\\[503, 503, 503, 503, 503, 503, 503, 503, 503, 503, 503, 503, 503]|\n",
        "|3|\\[503, 503, 503, 503, 503, 503, 503, 503, 503, 503, 503, 503, 503]|\n",
        "|.|.|\n",
        "|.|.|\n",
        "|.|.|\n",
        "|50|\\[503, 503, 503, 503, 503, 503, 503, 503, 503, 503, 503, 503, 503]|\n",
        "\n",
        "Futhermore, each image is well represented.\n",
        "\n",
        "|Class|Minimum|Maximum|Average|Ave*CC|\n",
        "|:---|:---:|:---:|:---:|:---:|\n",
        "|bhaji|39|40|40|25134|\n",
        "|chapati|29|30|29|25135|\n",
        "|githeri|52|53|52|25135|\n",
        "|kachumbari|50|51|51|25135|\n",
        "|kukuchoma|145|146|145|25135|\n",
        "|mandazi|40|41|41|25134|\n",
        "|masalachips|57|58|57|25134|\n",
        "|matoke|52|53|52|25135|\n",
        "|mukimo|118|119|119|25134|\n",
        "|nyamachoma|32|33|32|25135|\n",
        "|pilau|76|77|76|25135|\n",
        "|sukumawiki|62|63|63|25134|\n",
        "|ugali|40|41|40|25135|"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_data_loader(\n",
        "    data_loader:DataLoader, \n",
        "    counts:Optional[Dict[str,int]]=None\n",
        ") -> List[int]:\n",
        "    tally = [0] * 13\n",
        "    for fnames, targets in data_loader:\n",
        "        for target in targets:\n",
        "            tally[target] += 1\n",
        "        if counts is not None:\n",
        "            for fname in fnames:\n",
        "                counts[fname] += 1\n",
        "    return tally\n",
        "\n",
        "class TempDataset(Dataset):\n",
        "    def __init__(self, datastore):\n",
        "        super().__init__()\n",
        "        self.__fnames = datastore.train_fnames\n",
        "        self.__labels = datastore.train_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.__fnames)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.__fnames[idx], self.__labels[idx]\n",
        "\n",
        "\n",
        "# Test Data Loader w/o Weighted Random Sampler\n",
        "def run_data_loader_test_1():\n",
        "    datastore = KF13Datastore(data_dir)\n",
        "\n",
        "    print(\"Tally using data loader w/o weighted random sampler\")\n",
        "    print()\n",
        "    print(\"|Class|Training Images|Loader Images|\")\n",
        "    print(\"|:---|:---:|:---:|\")\n",
        "\n",
        "    tally = analyze_data_loader(DataLoader(\n",
        "        TempDataset(datastore),\n",
        "        batch_size = 128,\n",
        "        num_workers = 4,\n",
        "        shuffle = True\n",
        "    ))\n",
        "\n",
        "    class_names = datastore.class_names\n",
        "    class_counts = datastore.class_counts\n",
        "    for class_name, class_count, loader_count in zip(class_names, class_counts, tally):\n",
        "        print(f\"|{class_name}|{class_count}|{loader_count}|\")\n",
        "\n",
        "\n",
        "def run_data_loader_test_2(sampler, iterations, stop_when_converged):\n",
        "    datastore = KF13Datastore(data_dir)\n",
        "    data_loader = DataLoader(\n",
        "        TempDataset(datastore),\n",
        "        batch_size = 128,\n",
        "        num_workers = 4,\n",
        "        sampler = sampler,\n",
        "        shuffle = False\n",
        "    )\n",
        "\n",
        "    print()\n",
        "    print(\"|Iteration|Average Tally|\")\n",
        "    print(\"|:---:|:---:|\")\n",
        "    tally_prv = [0] * 13\n",
        "    tally_sum = [0] * 13\n",
        "    counts = {fname: 0 for fname in datastore.train_fnames}\n",
        "    for i in range(1, iterations + 1):\n",
        "        tally = analyze_data_loader(data_loader, counts)\n",
        "        tally_sum = [sum(counts) for counts in zip(tally_sum, tally)]\n",
        "        tally_ave = [int(count / float(i) + 0.5) for count in tally_sum]\n",
        "        if stop_when_converged and tally_ave == tally_prv:\n",
        "            break\n",
        "        tally_prv = tally_ave\n",
        "        print(f\"|{i}|\\\\{tally_ave}|\")\n",
        "\n",
        "    class_counts = [[] for _ in datastore.class_names]\n",
        "    for fname, label in zip(datastore.train_fnames, datastore.train_labels):\n",
        "        class_counts[label].append(counts[fname])\n",
        "\n",
        "    print()\n",
        "    print(\"|Class|Minimum|Maximum|Average|Ave*CC|\")\n",
        "    print(\"|:---|:---:|:---:|:---:|:---:|\")\n",
        "    for idx, class_name in enumerate(datastore.class_names):\n",
        "        min = np.amin(class_counts[idx])\n",
        "        max = np.amax(class_counts[idx])\n",
        "        ave = np.mean(class_counts[idx])\n",
        "        acc = ave * datastore.class_counts[idx]\n",
        "        print(f\"|{class_name}|{min}|{max}|{ave:.0f}|{acc:.0f}|\")\n",
        "\n",
        "\n",
        "# Test Data Loader w/ Weighted Random Sampler\n",
        "def run_data_loader_test_2a():\n",
        "    datastore = KF13Datastore(data_dir)\n",
        "    class_weights = 1. / np.array(datastore.class_counts)\n",
        "    class_weights = class_weights / np.sum(class_weights)\n",
        "    weights = [class_weights[e] for e in datastore.train_labels]\n",
        "    sampler = WeightedRandomSampler(torch.DoubleTensor(weights), len(weights))\n",
        "    run_data_loader_test_2(sampler, 100, True)\n",
        "\n",
        "\n",
        "# Test Data Loader w/ MySampler\n",
        "def run_data_loader_test_2b():\n",
        "    datastore = KF13Datastore(data_dir)\n",
        "    sampler = MySampler(datastore.train_labels)\n",
        "    run_data_loader_test_2(sampler, 50, False)\n",
        "\n",
        "\n",
        "#run_data_loader_test_1()\n",
        "#run_data_loader_test_2a()\n",
        "#run_data_loader_test_2b()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaqNIzTm3-5z"
      },
      "source": [
        "## <font style=\"color:green\">2. Configuration [5 Points]</font>\n",
        "\n",
        "Define your configuration in this section.\n",
        "\n",
        "For example,\n",
        "\n",
        "```\n",
        "@dataclass\n",
        "class TrainingConfiguration:\n",
        "    '''\n",
        "    Describes configuration of the training process\n",
        "    '''\n",
        "    batch_size: int = 10 \n",
        "    epochs_count: int = 50  \n",
        "    init_learning_rate: float = 0.1  # initial learning rate for lr scheduler\n",
        "    log_interval: int = 5  \n",
        "    test_interval: int = 1  \n",
        "    data_root: str = \"/kaggle/input/pytorch-opencv-course-classification/\" \n",
        "    num_workers: int = 2  \n",
        "    device: str = 'cuda'  \n",
        "    \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__6DBlJp3-5z"
      },
      "source": [
        "## <font style=\"color:blue\">Assignment Response</font>\n",
        "\n",
        "Since I am using the **trainer** module, I made minor modifications to the <u>configuration.py</u> file. In addition, I created a master _MasterConfig_ data class that encapsulates the individual configuration data classes. Lastly, I created helper functions to instantiate the _MasterConfig_ class with experiment-specific overrides.\n",
        "\n",
        "The following is the output of the `create_master_config` method w/o any parameter overrides.\n",
        "```\n",
        "MasterConfig(\n",
        "    system=SystemConfig(\n",
        "        proj_dir='./project2',\n",
        "        seed=42,\n",
        "        cudnn_deterministic=True,\n",
        "        cudnn_benchmark_enabled=False\n",
        "    ),\n",
        "    data_aug=DataAugConfig(\n",
        "        color_enabled=True,\n",
        "        color_brightness=(0.85, 1.15),\n",
        "        color_contrast=(0.5, 1.5),\n",
        "        color_saturation=(0.5, 2.0),\n",
        "        color_hue=(-0.03, 0.03),\n",
        "        horz_flip_prob=0.5,\n",
        "        vert_flip_prob=0.5,\n",
        "        affine_enabled=True,\n",
        "        affine_rotation=45,\n",
        "        affine_translate=(0.1, 0.1),\n",
        "        affine_scale=(0.9, 1.1),\n",
        "        erasing_prob=0.5,\n",
        "        erasing_scale=(0.02, 0.33),\n",
        "        erasing_ratio=(0.3, 3.3)\n",
        "    ),\n",
        "    dataset=DatasetConfig(\n",
        "        data_dir='./project2/data',\n",
        "        valid_size=0.2,\n",
        "        train_transforms=Compose(\n",
        "            Resize(size=256, interpolation=PIL.Image.BILINEAR)\n",
        "            ColorJitter(brightness=(0.85, 1.15), contrast=(0.5, 1.5), saturation=(0.5, 2.0), hue=(-0.03, 0.03))\n",
        "            RandomVerticalFlip(p=0.5)\n",
        "            RandomHorizontalFlip(p=0.5)\n",
        "            RandomAffine(degrees=[-45.0, 45.0], translate=(0.1, 0.1), scale=(0.9, 1.1), resample=PIL.Image.BILINEAR)\n",
        "            CenterCrop(size=(224, 224))\n",
        "            ToTensor()\n",
        "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "            RandomErasing()\n",
        "        ),\n",
        "        test_transforms=Compose(\n",
        "            Resize(size=256, interpolation=PIL.Image.BILINEAR)\n",
        "            CenterCrop(size=(224, 224))\n",
        "            ToTensor()\n",
        "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ),\n",
        "        visual_transforms=Compose(\n",
        "            Resize(size=256, interpolation=PIL.Image.BILINEAR)\n",
        "            CenterCrop(size=(224, 224))\n",
        "            ToTensor()\n",
        "        ),\n",
        "        visual_aug_transforms=Compose(\n",
        "            Resize(size=256, interpolation=PIL.Image.BILINEAR)\n",
        "            ColorJitter(brightness=(0.85, 1.15), contrast=(0.5, 1.5), saturation=(0.5, 2.0), hue=(-0.03, 0.03))\n",
        "            RandomVerticalFlip(p=0.5)\n",
        "            RandomHorizontalFlip(p=0.5)\n",
        "            RandomAffine(degrees=[-45.0, 45.0], translate=(0.1, 0.1), scale=(0.9, 1.1), resample=PIL.Image.BILINEAR)\n",
        "            CenterCrop(size=(224, 224))\n",
        "            ToTensor()\n",
        "            RandomErasing()\n",
        "        )\n",
        "    ),\n",
        "    data_loader=DataLoaderConfig(\n",
        "        batch_size=32,\n",
        "        num_workers=4\n",
        "    ),\n",
        "    optimizer=OptimizerConfig(\n",
        "        learning_rate=0.001,\n",
        "        momentum=0.9,\n",
        "        weight_decay=0.0001,\n",
        "        betas=(0.9, 0.999)\n",
        "    ),\n",
        "    scheduler=SchedulerConfig(\n",
        "        gamma=0.1,\n",
        "        step_size=10,\n",
        "        milestones=(20, 30, 40),\n",
        "        patience=10,\n",
        "        threshold=0.0001\n",
        "    ),\n",
        "    trainer=TrainerConfig(\n",
        "        device='cuda',\n",
        "        training_epochs=50,\n",
        "        weighted_loss_fn=True,\n",
        "        progress_bar=True,\n",
        "        model_dir='models',\n",
        "        model_saving_period=0,\n",
        "        visualizer_dir='runs',\n",
        "        stop_loss_epochs=0,\n",
        "        stop_acc_epochs=0,\n",
        "        stop_acc_ema_alpha=0.3,\n",
        "        stop_acc_threshold=2.0\n",
        "    )\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8clVVKLOpuwj"
      },
      "source": [
        "def get_config_value(param: str, default, config_overrides: Dict[str, Any]):\n",
        "    # ToDo: Ask Ryan why doesn't the following work.\n",
        "    # return config_overrides.get(param) if not None else default\n",
        "    value = config_overrides.get(param)\n",
        "    if value is None:\n",
        "        value = default\n",
        "    return value\n",
        "\n",
        "\n",
        "def create_system_config(**config_overrides) -> SystemConfig:\n",
        "    config = SystemConfig()\n",
        "    return SystemConfig(\n",
        "        proj_dir = proj_dir,\n",
        "        seed = get_config_value(\n",
        "            \"system_config_seed\",\n",
        "            config.seed,\n",
        "            config_overrides\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def create_data_aug_config(**config_overrides) -> DataAugConfig:\n",
        "    config = DataAugConfig()\n",
        "    return DataAugConfig(\n",
        "        color_enabled = get_config_value(\n",
        "            \"data_aug_color_enabled\", \n",
        "            config.color_enabled, \n",
        "            config_overrides\n",
        "        ),\n",
        "        color_brightness = get_config_value(\n",
        "            \"data_aug_color_brightness\",\n",
        "            config.color_brightness,\n",
        "            config_overrides\n",
        "        ),\n",
        "        color_contrast = get_config_value(\n",
        "            \"data_aug_color_contrast\",\n",
        "            config.color_contrast,\n",
        "            config_overrides\n",
        "        ),\n",
        "        color_saturation = get_config_value(\n",
        "            \"data_aug_color_saturation\",\n",
        "            config.color_saturation,\n",
        "            config_overrides\n",
        "        ),\n",
        "        color_hue = get_config_value(\n",
        "            \"data_aug_color_hue\",\n",
        "            config.color_hue,\n",
        "            config_overrides\n",
        "        ),\n",
        "        horz_flip_prob = get_config_value(\n",
        "            \"data_aug_horz_flip_prob\",\n",
        "            config.horz_flip_prob,\n",
        "            config_overrides\n",
        "        ),\n",
        "        vert_flip_prob = get_config_value(\n",
        "            \"data_aug_vert_flip_prob\",\n",
        "            config.vert_flip_prob,\n",
        "            config_overrides\n",
        "        ),\n",
        "        affine_enabled = get_config_value(\n",
        "            \"data_aug_affine_enabled\",\n",
        "            config.affine_enabled,\n",
        "            config_overrides\n",
        "        ),\n",
        "        affine_rotation = get_config_value(\n",
        "            \"data_aug_affine_rotation\",\n",
        "            config.affine_rotation,\n",
        "            config_overrides\n",
        "        ),\n",
        "        affine_translate = get_config_value(\n",
        "            \"data_aug_affine_translate\",\n",
        "            config.affine_translate,\n",
        "            config_overrides\n",
        "        ),\n",
        "        affine_scale = get_config_value(\n",
        "            \"data_aug_affine_scale\",\n",
        "            config.affine_scale,\n",
        "            config_overrides\n",
        "        ),\n",
        "        affine_shear = get_config_value(\n",
        "            \"data_aug_affine_shear\",\n",
        "            config.affine_shear,\n",
        "            config_overrides\n",
        "        ),\n",
        "        erasing_prob = get_config_value(\n",
        "            \"data_aug_erasing_prob\",\n",
        "            config.erasing_prob,\n",
        "            config_overrides\n",
        "        ),\n",
        "        erasing_scale = get_config_value(\n",
        "            \"data_aug_erasing_scale\",\n",
        "            config.erasing_scale,\n",
        "            config_overrides\n",
        "        ),\n",
        "        erasing_ratio = get_config_value(\n",
        "            \"data_aug_erasing_ratio\",\n",
        "            config.erasing_ratio,\n",
        "            config_overrides\n",
        "        ),\n",
        "        erasing_random = get_config_value(\n",
        "            \"data_aug_erasing_random\",\n",
        "            config.erasing_random,\n",
        "            config_overrides\n",
        "        ),\n",
        "        noise_prob = get_config_value(\n",
        "            \"data_aug_noise_prob\",\n",
        "            config.noise_prob,\n",
        "            config_overrides\n",
        "        ),\n",
        "        noise_mean = get_config_value(\n",
        "            \"data_aug_noise_mean\",\n",
        "            config.noise_mean,\n",
        "            config_overrides\n",
        "        ),\n",
        "        noise_std = get_config_value(\n",
        "            \"data_aug_noise_std\",\n",
        "            config.noise_std,\n",
        "            config_overrides\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def create_dataset_config(\n",
        "    resize: int = 256, \n",
        "    crop_size: int = 224,\n",
        "    data_aug_config = DataAugConfig()\n",
        ") -> DatasetConfig:\n",
        "    mean, std = get_mean_std()\n",
        "    transforms = ImageTransforms(\n",
        "        resize = resize, \n",
        "        crop_size = crop_size, \n",
        "        mean = mean, \n",
        "        std = std,\n",
        "        config = data_aug_config\n",
        "    )\n",
        "    return DatasetConfig(\n",
        "        data_dir = data_dir,\n",
        "        test_transforms = transforms.common(),\n",
        "        train_transforms = transforms.augment(),\n",
        "        visual_transforms = transforms.preprocess(augment=False),\n",
        "        visual_aug_transforms = transforms.preprocess(augment=True)\n",
        "    )\n",
        "\n",
        "\n",
        "def create_data_loader_config(**config_overrides) -> DataLoaderConfig:\n",
        "    config = DataLoaderConfig()\n",
        "    return DataLoaderConfig(\n",
        "        batch_size = get_config_value(\n",
        "            \"data_loader_batch_size\",\n",
        "            config.batch_size,\n",
        "            config_overrides\n",
        "        ),\n",
        "        num_workers = get_config_value(\n",
        "            \"data_loader_num_workers\",\n",
        "            config.num_workers,\n",
        "            config_overrides\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def create_optimizer_config(**config_overrides) -> OptimizerConfig():\n",
        "    config = OptimizerConfig()\n",
        "    return OptimizerConfig(\n",
        "        learning_rate = get_config_value(\n",
        "            \"optimizer_learning_rate\",\n",
        "            config.learning_rate,\n",
        "            config_overrides\n",
        "        ),\n",
        "        momentum = get_config_value(\n",
        "            \"optimizer_momentum\",\n",
        "            config.momentum,\n",
        "            config_overrides\n",
        "        ),\n",
        "        weight_decay = get_config_value(\n",
        "            \"optimizer_weight_decay\",\n",
        "            config.weight_decay,\n",
        "            config_overrides\n",
        "        ),\n",
        "        betas = get_config_value(\n",
        "            \"optimizer_betas\",\n",
        "            config.betas,\n",
        "            config_overrides\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def create_scheduler_config(**config_overrides) -> SchedulerConfig:\n",
        "    config = SchedulerConfig()\n",
        "    return SchedulerConfig(\n",
        "        gamma = get_config_value(\n",
        "            \"scheduler_gamma\",\n",
        "            config.gamma,\n",
        "            config_overrides\n",
        "        ),\n",
        "        step_size = get_config_value(\n",
        "            \"scheduler_step_size\",\n",
        "            config.step_size,\n",
        "            config_overrides\n",
        "        ),\n",
        "        milestones = get_config_value(\n",
        "            \"scheduler_milestones\",\n",
        "            config.milestones,\n",
        "            config_overrides\n",
        "        ),\n",
        "        patience = get_config_value(\n",
        "            \"scheduler_patience\",\n",
        "            config.patience,\n",
        "            config_overrides\n",
        "        ),\n",
        "        threshold = get_config_value(\n",
        "            \"scheduler_threshold\",\n",
        "            config.threshold,\n",
        "            config_overrides\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def create_trainer_config(**config_overrides) -> TrainerConfig:\n",
        "    config = TrainerConfig()\n",
        "    return TrainerConfig(\n",
        "        training_epochs = get_config_value(\n",
        "            \"trainer_training_epochs\",\n",
        "            config.training_epochs,\n",
        "            config_overrides\n",
        "        ),\n",
        "        model_saving_period = get_config_value(\n",
        "            \"trainer_model_saving_period\",\n",
        "            config.model_saving_period,\n",
        "            config_overrides\n",
        "        ),\n",
        "        stop_loss_epochs = get_config_value(\n",
        "            \"trainer_stop_loss_epochs\",\n",
        "            config.stop_loss_epochs,\n",
        "            config_overrides\n",
        "        ),\n",
        "        stop_acc_epochs = get_config_value(\n",
        "            \"trainer_stop_acc_epochs\",\n",
        "            config.stop_acc_epochs,\n",
        "            config_overrides\n",
        "        ),\n",
        "        stop_acc_ema_alpha = get_config_value(\n",
        "            \"trainer_stop_acc_ema_alpha\",\n",
        "            config.stop_acc_ema_alpha,\n",
        "            config_overrides\n",
        "        ),\n",
        "        stop_acc_threshold = get_config_value(\n",
        "            \"trainer_stop_acc_threshold\",\n",
        "            config.stop_acc_threshold,\n",
        "            config_overrides\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class MasterConfig:\n",
        "    system: SystemConfig = create_system_config()\n",
        "    data_aug: DataAugConfig = create_data_aug_config()\n",
        "    dataset: DatasetConfig = create_dataset_config()\n",
        "    data_loader: DataLoaderConfig = create_data_loader_config()\n",
        "    optimizer: OptimizerConfig = create_optimizer_config()\n",
        "    scheduler: SchedulerConfig = create_scheduler_config()\n",
        "    trainer: TrainerConfig = create_trainer_config()\n",
        "\n",
        "\n",
        "def create_master_config(\n",
        "    transform_resize: int = 256,\n",
        "    transform_crop_size: int = 224,\n",
        "    **config_overrides\n",
        ") -> MasterConfig:\n",
        "    # used to initialize MasterConfig data class and as a parameter to the\n",
        "    # create_data_config function\n",
        "    data_aug_config = create_data_aug_config(**config_overrides)\n",
        "    dataset_config = create_dataset_config(\n",
        "        transform_resize, \n",
        "        transform_crop_size, \n",
        "        data_aug_config\n",
        "    )\n",
        "    return MasterConfig(\n",
        "        system = create_system_config(**config_overrides),\n",
        "        data_aug = data_aug_config,\n",
        "        dataset = dataset_config,\n",
        "        data_loader = create_data_loader_config(**config_overrides),\n",
        "        optimizer = create_optimizer_config(**config_overrides),\n",
        "        scheduler = create_scheduler_config(**config_overrides),\n",
        "        trainer = create_trainer_config(**config_overrides)\n",
        "    ) "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYVGcNyX3-50"
      },
      "source": [
        "## <font style=\"color:green\">3. Evaluation Metric [10 Points]</font>\n",
        "\n",
        "Define methods or classes that will be used in model evaluation, for example, accuracy, f1-score, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJg9CPFB3-50"
      },
      "source": [
        "### Loss Function\n",
        "\n",
        "I will use [torch.nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) loss function with and without passing a tensor of rescaling weights. The weight corresponding to each class are inversely proportional to the number images in that class. The weight tensor is normalized, as shown below.\n",
        "\n",
        "```\n",
        "    weight = tensor([0.0499, 0.0366, 0.0658, 0.0638, 0.1823, \n",
        "                    0.0509, 0.0720, 0.0653, 0.1487, 0.0402, \n",
        "                    0.0958, 0.0784, 0.0502], dtype=torch.float64)\n",
        "```\n",
        "\n",
        "### Metric Function\n",
        "\n",
        "I am using the **trainer** module's `AccuracyEstimator` class from the <u>metrics.py</u> file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sSIXHXg3-51"
      },
      "source": [
        "## <font style=\"color:green\">4. Train and Validation [5 Points]</font>\n",
        "\n",
        "Write the methods or classes that will be used for training and validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhF41hAH3-51"
      },
      "source": [
        "## Assignment Response\n",
        "\n",
        "Since I am using the **trainer** module, I made the following modifications to the `trainer.py` file.\n",
        "* Added the ability to save the model only when the test loss reaches a new minimum.\n",
        "* Added the ability to terminate training after a specified number of epochs where the test loss is not further reduced.\n",
        "* Added the ability to terminate training after a specified number of epochs where the exponential moving average of the test loss does not significantly increase.\n",
        "\n",
        "I made the following modifications to the `visualizer.py` and `tensorboard_visualizer.py` files.\n",
        "* Added an <code>add_image(self, tag, image)</code> method to visualize the dataset.\n",
        "* Added an <code>add_figure(elf, tag, figure, close=True)</code> method to visulize matplotlib figures, e.g., confusion matrices.\n",
        "* Added an <code>add_graph(self, model, images)</code> method to document the model.\n",
        "* Added an <code>add_pr_curves(self, classes, pred_probs, targets)</code> method to document the precision-recall curves of the fully trained model for each class type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mxi77QT13-51"
      },
      "source": [
        "class Optimizer(Enum):\n",
        "    SGD = auto()\n",
        "    ADAM = auto()\n",
        "\n",
        "\n",
        "def get_optimizer(\n",
        "    model: nn.Module,\n",
        "    optimizer: Optimizer = Optimizer.SGD,\n",
        "    config: OptimizerConfig = OptimizerConfig()\n",
        "):\n",
        "    \"\"\"\n",
        "    Gets the specified optimizer.\n",
        "    \"\"\"\n",
        "    \n",
        "    if optimizer == Optimizer.SGD:\n",
        "        return optim.SGD(\n",
        "            model.parameters(),\n",
        "            lr = config.learning_rate,\n",
        "            momentum = config.momentum,\n",
        "            weight_decay = config.weight_decay\n",
        "        )\n",
        "    \n",
        "    elif optimizer == Optimizer.ADAM:\n",
        "        # Note: The weight_decay parameter was added after Group C experiments.\n",
        "        return optim.Adam(\n",
        "            model.parameters(),\n",
        "            lr = config.learning_rate,\n",
        "            betas = config.betas,\n",
        "            weight_decay = config.weight_decay\n",
        "        )\n",
        "    \n",
        "    else:\n",
        "        raise SystemExit(\"Invalid optimizer value.\")\n",
        "\n",
        "\n",
        "class LrScheduler(Enum):\n",
        "    STEP = auto()\n",
        "    MULTI_STEP = auto()\n",
        "    EXPONENTIAL = auto()\n",
        "    REDUCE_ON_PLATEAU = auto()\n",
        "\n",
        "    \n",
        "def get_scheduler(\n",
        "    optimizer: optim.Optimizer,\n",
        "    scheduler: LrScheduler = LrScheduler.STEP,\n",
        "    config: SchedulerConfig = SchedulerConfig()\n",
        "):\n",
        "    \"\"\"\n",
        "    Gets the specified LR scheduler.\n",
        "    \"\"\"\n",
        "\n",
        "    if scheduler == LrScheduler.STEP:\n",
        "        return optim.lr_scheduler.StepLR(\n",
        "            optimizer,\n",
        "            step_size = config.step_size,\n",
        "            gamma = config.gamma\n",
        "        )\n",
        "    \n",
        "    elif scheduler == LrScheduler.MULTI_STEP:\n",
        "        return optim.lr_scheduler.MultiStepLR(\n",
        "            optimizer, \n",
        "            milestones = config.milestones, \n",
        "            gamma = config.gamma\n",
        "        )\n",
        "    \n",
        "    elif scheduler == LrScheduler.EXPONENTIAL:\n",
        "        return optim.lr_scheduler.ExponentialLR(\n",
        "            optimizer, \n",
        "            gamma = config.gamma\n",
        "        )\n",
        "    \n",
        "    \n",
        "    elif scheduler == LrScheduler.REDUCE_ON_PLATEAU:\n",
        "        return optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, \n",
        "            factor = config.gamma,\n",
        "            patience = config.patience,\n",
        "            threshold = config.threshold\n",
        "        )\n",
        "    \n",
        "    else:\n",
        "        raise SystemExit(\"Invalid scheduler value.\")    "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlaX2uwY3-51"
      },
      "source": [
        "def predict_batch(model, data, max_prob=True):\n",
        "    \"\"\"\n",
        "    Get prediction for a batch of data. This function assumes the model and data\n",
        "    have be sent to the appropriate device and the model is in evaluation mode.\n",
        "    \"\"\"\n",
        "\n",
        "    output = model(data)\n",
        "\n",
        "    # get probability score using softmax\n",
        "    prob = F.softmax(output, dim=1)\n",
        "    \n",
        "    if max_prob:\n",
        "        # get the max probability\n",
        "        pred_prob = prob.data.max(dim=1)[0]\n",
        "    else:\n",
        "        # return all probabilties\n",
        "        pred_prob = prob.data\n",
        "    \n",
        "    # get the index of the max probability\n",
        "    pred_index = prob.data.max(dim=1)[1]\n",
        "    \n",
        "    return pred_index.cpu().numpy(), pred_prob.cpu().numpy()\n",
        "\n",
        "\n",
        "def get_pred_and_pred_probs(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Get prediction probabilities.\n",
        "    \"\"\"\n",
        "    \n",
        "    model.to(device)  # send model to cpu or cuda\n",
        "    model.eval()      # set model to evaluation mode\n",
        "\n",
        "    pred_probs = []\n",
        "\n",
        "    for data, target in dataloader:\n",
        "        _, probs = predict_batch(model, data.to(device), max_prob=False)       \n",
        "        pred_probs.append(probs)\n",
        "\n",
        "    pred_probs = np.concatenate(pred_probs, axis=0)\n",
        "    predictions = np.argmax(pred_probs, axis = 1)\n",
        "    \n",
        "    return predictions, pred_probs\n",
        "\n",
        "\n",
        "def get_targets_and_pred_probs(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Get targets and prediction probabilities.\n",
        "    \"\"\"\n",
        "    \n",
        "    model.to(device)  # send model to cpu or cuda\n",
        "    model.eval()      # set model to evaluation mode\n",
        "\n",
        "    targets = []\n",
        "    pred_probs = []\n",
        "\n",
        "    for data, target in dataloader:\n",
        "        _, probs = predict_batch(model, data.to(device), max_prob=False)       \n",
        "        pred_probs.append(probs)\n",
        "        targets.append(target.numpy())\n",
        "\n",
        "    targets = np.concatenate(targets).astype(int)\n",
        "    pred_probs = np.concatenate(pred_probs, axis=0)\n",
        "    \n",
        "    return targets, pred_probs\n",
        "\n",
        "\n",
        "def predict_valid_data(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Predict the class of the validation data.\n",
        "    \"\"\"\n",
        "\n",
        "    model.to(device)  # send model to cpu or cuda\n",
        "    model.eval()      # set model to evaluation mode\n",
        "\n",
        "    targets = []\n",
        "    preds = []\n",
        "\n",
        "    for data, target in dataloader:\n",
        "        pred, _ = predict_batch(model, data.to(device), max_prob=True)       \n",
        "        targets.append(target)\n",
        "        preds.append(pred)\n",
        "        \n",
        "    targets = np.concatenate(targets)\n",
        "    preds = np.concatenate(preds).astype(int)\n",
        "    \n",
        "    return targets, preds\n",
        "\n",
        "\n",
        "def predict_test_data(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Predict the class of the test data.\n",
        "    \"\"\"\n",
        "    model.to(device)  # send model to cpu or cuda\n",
        "    model.eval()      # set model to evaluation mode\n",
        "\n",
        "    fnames = []\n",
        "    preds = []\n",
        "\n",
        "    for _, (data, fname) in enumerate(dataloader):\n",
        "        pred, _ = predict_batch(model, data.to(device), max_prob=True)       \n",
        "        fnames.append(fname)\n",
        "        preds.append(pred)\n",
        "        \n",
        "    fnames = np.concatenate(fnames)\n",
        "    preds = np.concatenate(preds).astype(int)\n",
        "    \n",
        "    return fnames, preds"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajr4pCOz3-51"
      },
      "source": [
        "## <font style=\"color:green\">5. Model [5 Points]</font>\n",
        "\n",
        "Define your model in this section.\n",
        "\n",
        "**You are allowed to use any pre-trained model.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02C_Nwdo7SBK"
      },
      "source": [
        "## Assignment Response\n",
        "\n",
        "Since my approach is to explore transfer learning in numerous pretrained models, I created classes\n",
        "to easily set the \"tuning level\" of the ResNet, VGG, and DenseNet family of TorchVision models. I\n",
        "also want to see how the model I developed for Project 1 performs, so I created as a class for it as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18LAfbDDpuwn"
      },
      "source": [
        "TuningParam = namedtuple(\"TuningParam\", [\"level\", \"block\", \"layers\"])\n",
        "\n",
        "class TorchVisionModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Base class for TorchVision models, which provides a method to freeze network\n",
        "    layers allowing fine tuning. This class does change the network's output layer.\n",
        "    Derived classes must do this!\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, network: nn.Module):\n",
        "        super().__init__()\n",
        "        self._network = network\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self._network(x)\n",
        "    \n",
        "    def _freeze_layers(\n",
        "        self, \n",
        "        tuning_params: List[TuningParam], \n",
        "        pretrained:bool, \n",
        "        tuning_level:int\n",
        "    ):\n",
        "        # exit if not using a pretrained model\n",
        "        if not pretrained:\n",
        "            return\n",
        "            \n",
        "        # otherwise, 1) freeze the entire network\n",
        "        self._set_requires_grad(self._network, False)\n",
        "        \n",
        "        # and 2) unfreeze blocks/layers based on tuning_level\n",
        "        for param in tuning_params:\n",
        "            if param.level <= tuning_level:\n",
        "                block = getattr(self._network, param.block)\n",
        "                if param.layers is None:\n",
        "                    self._set_requires_grad(block, True)\n",
        "                else:\n",
        "                    for layer in param.layers:\n",
        "                        if isinstance(layer, int):\n",
        "                            self._set_requires_grad(block[layer], True)\n",
        "                        else:\n",
        "                            self._set_requires_grad(getattr(block, layer), True)\n",
        "            \n",
        "    def _set_requires_grad(self, block, value):\n",
        "        for param in block.parameters():\n",
        "            param.requires_grad = value\n",
        "            \n",
        "    def _inclusive_range(self, start:int, stop:int) -> List[int]:\n",
        "        return list(range(start, stop + 1))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DonDtN5MBBU1"
      },
      "source": [
        "class ResNetBase(TorchVisionModel):\n",
        "    \"\"\"\n",
        "    Base class for ResNet models that may be pretrained and fine tuned. The\n",
        "    tuning_level parameter controls the degree of fine tuning as depicted in\n",
        "    the table below.\n",
        "        \n",
        "        ResNet     tuning_level\n",
        "        -------    ------------\n",
        "        conv1          >= 5        \n",
        "        bn1            >= 5\n",
        "        relu           >= 5\n",
        "        maxpool        >= 5\n",
        "        layer1         >= 4\n",
        "        layer2         >= 3\n",
        "        layer3         >= 2\n",
        "        layer4         >= 1\n",
        "        avgpool        >= 1\n",
        "        fc             >= 0\n",
        "        \n",
        "    If tuning_level = 0, then only the classifier layer is trained.\n",
        "    If tuning_level = 5, then the entire network is trained.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model_fn: Callable, pretrained=True, tuning_level=0):\n",
        "        super().__init__(model_fn(pretrained=pretrained))\n",
        "\n",
        "        # change the output layer\n",
        "        last_layer_in = self._network.fc.in_features\n",
        "        self._network.fc = nn.Linear(last_layer_in, 13)\n",
        "\n",
        "        # ToDo: Omit layer types that do not have trainable parameters\n",
        "        tuning_params = [\n",
        "            TuningParam(0, \"fc\", None),\n",
        "            TuningParam(1, \"avgpool\", None),\n",
        "            TuningParam(1, \"layer4\", None),\n",
        "            TuningParam(2, \"layer3\", None),\n",
        "            TuningParam(3, \"layer2\", None),\n",
        "            TuningParam(4, \"layer1\", None),\n",
        "            TuningParam(5, \"maxpool\", None),\n",
        "            TuningParam(5, \"relu\", None),\n",
        "            TuningParam(5, \"bn1\", None),\n",
        "            TuningParam(5, \"conv1\", None)\n",
        "        ]\n",
        "\n",
        "        self._freeze_layers(tuning_params, pretrained, tuning_level)\n",
        "\n",
        "    \n",
        "class ResNet18(ResNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.resnet18, pretrained, tuning_level)\n",
        "\n",
        "class ResNet34(ResNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.resnet34, pretrained, tuning_level)\n",
        "\n",
        "class ResNet50(ResNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.resnet50, pretrained, tuning_level)\n",
        "\n",
        "class ResNet101(ResNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.resnet101, pretrained, tuning_level)\n",
        "\n",
        "class ResNet152(ResNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.resnet152, pretrained, tuning_level)\n",
        "\n",
        "class ResNeXt50(ResNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.resnext50_32x4d, pretrained, tuning_level)\n",
        "\n",
        "class ResNeXt101(ResNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.resnext101_32x8d, pretrained, tuning_level)\n",
        "\n",
        "class WideResNet50(ResNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.wide_resnet50_2, pretrained, tuning_level)\n",
        "\n",
        "class WideResNet101(ResNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.wide_resnet101_2, pretrained, tuning_level)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e70wQoxzkgh"
      },
      "source": [
        "class VGGBase(TorchVisionModel):\n",
        "    \"\"\"\n",
        "    Base class for ResNet models that may be pretrained and fine tuned.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model_fn: Callable, pretrained=True):\n",
        "        super().__init__(model_fn(pretrained=pretrained))\n",
        "\n",
        "        last_layer_in = self._network.classifier[6].in_features\n",
        "        self._network.classifier[6] = nn.Linear(last_layer_in, 13)\n",
        "\n",
        "\n",
        "class VGG11BN(VGGBase):\n",
        "    \"\"\"\n",
        "    VGG11BN model that may be pretrained and fine tuned. The tuning_level\n",
        "    parameter controls the degree of fine tuning as depicted in the table\n",
        "    below.\n",
        "    \n",
        "        VGG11_BN            tuning_level\n",
        "        ----------------    ------------\n",
        "        features\n",
        "          [00-02] CNR           >= 5\n",
        "          [03] MaxPool2d        >= 5\n",
        "          [04-06] CNR           >= 4\n",
        "          [07] MaxPool2d        >= 4\n",
        "          [08-10] CNR           >= 3\n",
        "          [11-13] CNR           >= 3\n",
        "          [14] MaxPool2d        >= 3\n",
        "          [15-17] CNR           >= 2\n",
        "          [18-20] CNR           >= 2\n",
        "          [21] MaxPool2d        >= 2\n",
        "          [22-24] CNR           >= 1\n",
        "          [25-27] CNR           >= 1\n",
        "          [28] MaxPool2d        >= 1\n",
        "        avgpool                 >= 1\n",
        "        classifier              \n",
        "          [00-02] LRD           >= 0\n",
        "          [03-05] LRD           >= 0\n",
        "          [06] Linear           >= 0\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.vgg11_bn, pretrained)\n",
        "            \n",
        "        # ToDo: Omit layer types that do not have trainable parameters\n",
        "        tuning_params = [\n",
        "            TuningParam(0, \"classifier\", None),\n",
        "            TuningParam(1, \"avgpool\", None),\n",
        "            TuningParam(1, \"features\", self._inclusive_range(22, 28)),\n",
        "            TuningParam(2, \"features\", self._inclusive_range(15, 21)),\n",
        "            TuningParam(3, \"features\", self._inclusive_range(8, 14)),\n",
        "            TuningParam(4, \"features\", self._inclusive_range(4, 7)),\n",
        "            TuningParam(5, \"features\", self._inclusive_range(0, 3))\n",
        "        ]\n",
        "\n",
        "        self._freeze_layers(tuning_params, pretrained, tuning_level)\n",
        "\n",
        "\n",
        "class VGG13BN(VGGBase):\n",
        "    \"\"\"\n",
        "    VGG13BN model that may be pretrained and fine tuned. The tuning_level\n",
        "    parameter controls the degree of fine tuning as depicted in the table\n",
        "    below.\n",
        "    \n",
        "        VGG13_BN            tuning_level\n",
        "        ----------------    ------------\n",
        "        features\n",
        "          [00-02] CNR           >= 5\n",
        "          [03-05] CNR           >= 5\n",
        "          [06] MaxPool2d        >= 5\n",
        "          [07-09] CNR           >= 4\n",
        "          [10-12] CNR           >= 4\n",
        "          [13] MaxPool2d        >= 4\n",
        "          [14-16] CNR           >= 3\n",
        "          [17-19] CNR           >= 3\n",
        "          [20] MaxPool2d        >= 3\n",
        "          [21-23] CNR           >= 2\n",
        "          [24-26] CNR           >= 2\n",
        "          [27] MaxPool2d        >= 2\n",
        "          [28-30] CNR           >= 1\n",
        "          [31-33] CNR           >= 1\n",
        "          [34] MaxPool2d        >= 1\n",
        "        avgpool                 >= 1\n",
        "        classifier\n",
        "          [00-02] LRD           >= 0\n",
        "          [03-05] LRD           >= 0\n",
        "          [06] Linear           >= 0\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.vgg13_bn, pretrained)\n",
        "            \n",
        "        # ToDo: Omit layer types that do not have trainable parameters\n",
        "        tuning_params = [\n",
        "            TuningParam(0, \"classifier\", None),\n",
        "            TuningParam(1, \"avgpool\", None),\n",
        "            TuningParam(1, \"features\", self._inclusive_range(28, 34)),\n",
        "            TuningParam(2, \"features\", self._inclusive_range(21, 27)),\n",
        "            TuningParam(3, \"features\", self._inclusive_range(14, 20)),\n",
        "            TuningParam(4, \"features\", self._inclusive_range(7, 13)),\n",
        "            TuningParam(5, \"features\", self._inclusive_range(0, 6))\n",
        "        ]\n",
        "\n",
        "        self._freeze_layers(tuning_params, pretrained, tuning_level)\n",
        "\n",
        "\n",
        "class VGG16BN(VGGBase):\n",
        "    \"\"\"\n",
        "    VGG16BN model that may be pretrained and fine tuned. The tuning_level\n",
        "    parameter controls the degree of fine tuning as depicted in the table\n",
        "    below.\n",
        "    \n",
        "        VGG16_BN            tuning_level\n",
        "        ----------------    ------------\n",
        "        features\n",
        "          [00-02] CNR           >= 5\n",
        "          [03-05] CNR           >= 5\n",
        "          [06] MaxPool2d        >= 5\n",
        "          [07-09] CNR           >= 4\n",
        "          [10-12] CNR           >= 4\n",
        "          [13] MaxPool2d        >= 4\n",
        "          [14-16] CNR           >= 3\n",
        "          [17-19] CNR           >= 3\n",
        "          [20-22] CNR           >= 3\n",
        "          [23] MaxPool2d        >= 3\n",
        "          [24-26] CNR           >= 2\n",
        "          [27-29] CNR           >= 2\n",
        "          [30-32] CNR           >= 2\n",
        "          [33] MaxPool2d        >= 2\n",
        "          [34-36] CNR           >= 1\n",
        "          [37-39] CNR           >= 1\n",
        "          [40-42] CNR           >= 1\n",
        "          [43] MaxPool2d        >= 1\n",
        "        avgpool                 >= 1\n",
        "        classifier              \n",
        "          [00-02] LRD           >= 0\n",
        "          [03-05] LRD           >= 0\n",
        "          [06] Linear           >= 0\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.vgg16_bn, pretrained)\n",
        "            \n",
        "        # ToDo: Omit layer types that do not have trainable parameters\n",
        "        tuning_params = [\n",
        "            TuningParam(0, \"classifier\", None),\n",
        "            TuningParam(1, \"avgpool\", None),\n",
        "            TuningParam(1, \"features\", self._inclusive_range(34, 43)),\n",
        "            TuningParam(2, \"features\", self._inclusive_range(24, 33)),\n",
        "            TuningParam(3, \"features\", self._inclusive_range(14, 23)),\n",
        "            TuningParam(4, \"features\", self._inclusive_range(7, 13)),\n",
        "            TuningParam(5, \"features\", self._inclusive_range(0, 6))\n",
        "        ]\n",
        "\n",
        "        self._freeze_layers(tuning_params, pretrained, tuning_level)\n",
        "\n",
        "\n",
        "class VGG19BN(VGGBase):\n",
        "    \"\"\"\n",
        "    VGG19BN model that may be pretrained and fine tuned. The tuning_level\n",
        "    parameter controls the degree of fine tuning as depicted in the table\n",
        "    below.\n",
        "\n",
        "        VGG11_BN            tuning_level\n",
        "        ----------------    ------------\n",
        "        features\n",
        "          [00-02] CNR           >= 5\n",
        "          [03-05] CNR           >= 5\n",
        "          [06] MaxPool2d        >= 5\n",
        "          [07-09] CNR           >= 4\n",
        "          [10-12] CNR           >= 4\n",
        "          [13] MaxPool2d        >= 4\n",
        "          [14-16] CNR           >= 3\n",
        "          [17-19] CNR           >= 3\n",
        "          [20-22] CNR           >= 3\n",
        "          [23-25] CNR           >= 3\n",
        "          [26] MaxPool2d        >= 3\n",
        "          [27-29] CNR           >= 2\n",
        "          [30-32] CNR           >= 2\n",
        "          [33-35] CNR           >= 2\n",
        "          [36-38] CNR           >= 2\n",
        "          [39] MaxPool2d        >= 2\n",
        "          [40-42] CNR           >= 1\n",
        "          [43-45] CNR           >= 1\n",
        "          [46-48] CNR           >= 1\n",
        "          [49-51] CNR           >= 1\n",
        "          [52] MaxPool2d        >= 1\n",
        "        avgpool                 >= 1\n",
        "        classifier              \n",
        "          [00-02] LRD           >= 0\n",
        "          [03-05] LRD           >= 0\n",
        "          [06] Linear           >= 0\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.vgg19_bn, pretrained)\n",
        "                # ToDo: Omit layer types that do not have trainable parameters\n",
        "        tuning_params = [\n",
        "            TuningParam(0, \"classifier\", None),\n",
        "            TuningParam(1, \"avgpool\", None),\n",
        "            TuningParam(1, \"features\", self._inclusive_range(40, 52)),\n",
        "            TuningParam(2, \"features\", self._inclusive_range(27, 39)),\n",
        "            TuningParam(3, \"features\", self._inclusive_range(14, 26)),\n",
        "            TuningParam(4, \"features\", self._inclusive_range(7, 13)),\n",
        "            TuningParam(5, \"features\", self._inclusive_range(0, 6))\n",
        "        ]\n",
        "\n",
        "        self._freeze_layers(tuning_params, pretrained, tuning_level)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aun0yxTfpuwq"
      },
      "source": [
        "class DenseNetBase(TorchVisionModel):\n",
        "    \"\"\"\n",
        "    Base class for DenseNet models that may be pretrained and fine tuned. The\n",
        "    tuning_level parameter controls the degree of fine tuning as depicted in\n",
        "    the table below.\n",
        "        \n",
        "        DenseNet          tuning_level\n",
        "        -------------     ------------\n",
        "        features\n",
        "          conv0               >= 5\n",
        "          norm0               >= 5\n",
        "          relu0               >= 5\n",
        "          pool0               >= 5\n",
        "          denseblock1         >= 4\n",
        "          transition1         >= 4\n",
        "          denseblock2         >= 3\n",
        "          transition2         >= 3\n",
        "          denseblock3         >= 2\n",
        "          transition3         >= 2\n",
        "          denseblock4         >= 1\n",
        "          norm5               >= 1\n",
        "        classifier            >= 0\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_fn: Callable, pretrained=True, tuning_level=0):\n",
        "        super().__init__(model_fn(pretrained=pretrained))\n",
        "\n",
        "        # change the output layer\n",
        "        last_layer_in = self._network.classifier.in_features\n",
        "        self._network.classifier = nn.Linear(last_layer_in, 13)\n",
        "\n",
        "        # ToDo: Omit layer types that do not have trainable parameters\n",
        "        tuning_params = [\n",
        "            TuningParam(0, \"classifier\", None),\n",
        "            TuningParam(1, \"features\", [\"denseblock4\", \"norm5\"]),\n",
        "            TuningParam(2, \"features\", [\"denseblock3\", \"transition3\"]),\n",
        "            TuningParam(3, \"features\", [\"denseblock2\", \"transition2\"]),\n",
        "            TuningParam(4, \"features\", [\"denseblock1\", \"transition1\"]),\n",
        "            TuningParam(5, \"features\", [\"conv0\", \"norm0\", \"relu0\", \"pool0\"])\n",
        "        ]\n",
        "\n",
        "        self._freeze_layers(tuning_params, pretrained, tuning_level)\n",
        "\n",
        "    \n",
        "class DenseNet121(DenseNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.densenet121, pretrained, tuning_level)\n",
        "\n",
        "class DenseNet169(DenseNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.densenet169, pretrained, tuning_level)\n",
        "\n",
        "class DenseNet201(DenseNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.densenet201, pretrained, tuning_level)\n",
        "\n",
        "class DenseNet161(DenseNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.densenet161, pretrained, tuning_level)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EfficientNetBase(TorchVisionModel):\n",
        "    \"\"\"\n",
        "    Base class for EfficientNet models that may be pretrained and fine tuned. The\n",
        "    tuning_level parameter controls the degree of fine tuning as depicted in\n",
        "    the table below.\n",
        "        \n",
        "        EfficienteNet     tuning_level\n",
        "        -------------     ------------\n",
        "        _conv_stem            >= 5\n",
        "        _bn0                  >= 5\n",
        "        _swish\n",
        "        _blocks               >= 5\n",
        "          [0] MBConvBlock     >= 5\n",
        "          [1] MBConvBlock     >= 5\n",
        "          [2] MBConvBlock     >= 5\n",
        "           ...                \n",
        "          [N-1] MBConvBlock   >= 1\n",
        "        _conv_head            >= 1\n",
        "        _bn1                  >= 1\n",
        "        _swish\n",
        "        _avg_pooling          >= 0\n",
        "        _dropout              >= 0\n",
        "        _fc                   >= 0\n",
        "\n",
        "    The number of MBConvBlock depends upon the depth as is given by the table below.\n",
        "    Each tuning level greater than zero, unfreezes 20% of the blocks.\n",
        "\n",
        "        Depth   Blocks\n",
        "        -----   ------\n",
        "          0       16\n",
        "          1       23\n",
        "          2       23\n",
        "          3       26\n",
        "          4       32\n",
        "          5       39\n",
        "          6       45\n",
        "          7       55\n",
        "\n",
        "    Note: The resulting blocks/layers from printing the model do not correspond to flow\n",
        "          of forward method. Does the the Pytorch EfficientNet implementation need to \n",
        "          override the nn.Module's \"iterator\" methods? For some reason, the _swish layer\n",
        "          appears after the fully connected _fc layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, depth=0, pretrained=True, tuning_level=0):\n",
        "        super(EfficientNetBase, self).__init__(self.__get_model(depth, pretrained))\n",
        "            \n",
        "        blocks = len(self._network._blocks)\n",
        "        levels = self.__compute_levels(blocks)\n",
        "\n",
        "        # ToDo: Omit layer types that do not have trainable parameters\n",
        "        tuning_params = [\n",
        "            TuningParam(0, \"_fc\", None),\n",
        "            TuningParam(0, \"_dropout\", None),\n",
        "            TuningParam(0, \"_avg_pooling\", None),\n",
        "            TuningParam(1, \"_bn1\", None),\n",
        "            TuningParam(1, \"_conv_head\", None),\n",
        "            TuningParam(1, \"_blocks\", self._inclusive_range(levels[4][0], levels[4][1])),\n",
        "            TuningParam(2, \"_blocks\", self._inclusive_range(levels[3][0], levels[3][1])),\n",
        "            TuningParam(3, \"_blocks\", self._inclusive_range(levels[2][0], levels[2][1])),\n",
        "            TuningParam(4, \"_blocks\", self._inclusive_range(levels[1][0], levels[1][1])),\n",
        "            TuningParam(5, \"_blocks\", self._inclusive_range(levels[0][0], levels[0][1])),\n",
        "            TuningParam(5, \"_bn0\", None),\n",
        "            TuningParam(5, \"_conv_stem\", None)\n",
        "        ]\n",
        "\n",
        "        self._freeze_layers(tuning_params, pretrained, tuning_level)\n",
        "\n",
        "    def __get_model(self, depth:int, pretrained:bool) -> nn.Module:\n",
        "        name = f\"efficientnet-b{depth}\"\n",
        "        if not pretrained:\n",
        "            return EfficientNet.from_name(name, num_classes=13)\n",
        "        else:\n",
        "            return EfficientNet.from_pretrained(name, num_classes=13)\n",
        "\n",
        "    def __compute_levels(self, blocks:int) -> List[Tuple[int, int]]:\n",
        "        div = int(blocks / 5)\n",
        "        rem = blocks - 5 * div\n",
        "        levels = []\n",
        "\n",
        "        s = 0\n",
        "        for i in range(1,6):\n",
        "            e = s + div - (0 if i <= rem else 1)\n",
        "            levels.append((s, e))\n",
        "            s = e + 1\n",
        "\n",
        "        return levels\n",
        "\n",
        "\n",
        "class EfficientNetB0(EfficientNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(0, pretrained, tuning_level)\n",
        "\n",
        "class EfficientNetB1(EfficientNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(1, pretrained, tuning_level)\n",
        "\n",
        "class EfficientNetB2(EfficientNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(2, pretrained, tuning_level)\n",
        "\n",
        "class EfficientNetB3(EfficientNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(3, pretrained, tuning_level)\n",
        "\n",
        "class EfficientNetB4(EfficientNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(4, pretrained, tuning_level)\n",
        "\n",
        "class EfficientNetB5(EfficientNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(5, pretrained, tuning_level)\n",
        "\n",
        "class EfficientNetB6(EfficientNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(6, pretrained, tuning_level)\n",
        "\n",
        "class EfficientNetB7(EfficientNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(7, pretrained, tuning_level)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHRk6r0s3-51"
      },
      "source": [
        "class Project1Model(nn.Module):\n",
        "    \"\"\"\n",
        "    Modified the last layer to output 13, rather than 3, features.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Convolution layers\n",
        "        self._body = nn.Sequential(\n",
        "            # input 3 x 224 x 224\n",
        "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=7, padding=3),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "            # input 24 * 112 * 112\n",
        "            nn.Conv2d(in_channels=16, out_channels=24, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(24),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "            # input 36 * 56 * 56\n",
        "            nn.Conv2d(in_channels=24, out_channels=36, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(36),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "            #input 54 * 28 * 28\n",
        "            nn.Conv2d(in_channels=36, out_channels=54, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(54),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "            #input 81 * 14 * 14\n",
        "            nn.Conv2d(in_channels=54, out_channels=81, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(81),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "\n",
        "        # Fully connected layers\n",
        "        self._head = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(in_features=81*7*7, out_features=1024), \n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(in_features=1024, out_features=256), \n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(in_features=256, out_features=13)            \n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self._body(x)\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        x = self._head(x)\n",
        "        return x"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XjG813k3-52"
      },
      "source": [
        "## <font style=\"color:green\">6. Utils [5 Points]</font>\n",
        "\n",
        "Define your methods or classes which are not covered in the above sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ToDo: Put into a static class (if Python supports them).\n",
        "\n",
        "def get_tuning_suffix(pretrained:bool, tuning_level:int) -> str:\n",
        "    return f\"_PT{tuning_level}\" if pretrained else \"\"\n",
        "\n",
        "def get_learning_rate_suffix(learning_rate: float) -> str:\n",
        "    return f\"_LR{learning_rate:.0E}\".replace(\"E-0\", \"E-\")\n",
        "    \n",
        "model_params = {\n",
        "    ResNet101:      (224, 74),\n",
        "    ResNet152:      (224, 52),\n",
        "    ResNeXt50:      (224, 88),\n",
        "    ResNeXt101:     (224, 35),\n",
        "    WideResNet50:   (224, 81),\n",
        "    WideResNet101:  (224, 48),\n",
        "    VGG13BN:        (224, 66),\n",
        "    VGG16BN:        (224, 58),\n",
        "    VGG19BN:        (224, 54),\n",
        "    DenseNet121:    (224, 76),\n",
        "    DenseNet169:    (224, 61),\n",
        "    DenseNet201:    (224, 47),\n",
        "    DenseNet161:    (224, 39),\n",
        "    EfficientNetB0: (224, 94),\n",
        "    EfficientNetB1: (240, 57),\n",
        "    EfficientNetB2: (260, 45),\n",
        "    EfficientNetB3: (300, 26),\n",
        "    EfficientNetB4: (380, 12),\n",
        "    EfficientNetB5: (456,  6),\n",
        "    EfficientNetB6: (528,  3),\n",
        "    EfficientNetB7: (600,  2)\n",
        "}\n",
        "\n",
        "def get_image_size(model_type: TorchVisionModel) -> int:\n",
        "    try:\n",
        "        return model_params[model_type][0]\n",
        "    except:\n",
        "        return 224\n",
        "\n",
        "def get_batch_size(model_type: TorchVisionModel) -> int:\n",
        "    try:\n",
        "        return model_params[model_type][1]\n",
        "    except:\n",
        "        return 96"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyWhT4sIpuwr"
      },
      "source": [
        "def compute_accuracy(targets:List[int], preds:List[int]):\n",
        "    right = sum(x == y for x, y in zip(targets, preds))\n",
        "    count = len(targets)\n",
        "    return float(right) / count   \n",
        "\n",
        "\n",
        "def create_confusion_matrix(cm, classes, model_name=None):\n",
        "    \"\"\"\n",
        "    Creates and returns a confusion matrix figure that can be saved to a file or .\n",
        "    \"\"\"\n",
        "\n",
        "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "    # compute accuracy, normalized confusion matrix\n",
        "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
        "    cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    # initialize the plot tick marks and title\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    title = \"Confusion Matrix\"\n",
        "    if model_name is not None:\n",
        "        title = title + \" ({})\".format(model_name)\n",
        "    \n",
        "    # plot the confusion matrix\n",
        "    plt.style.use('default')\n",
        "    fig = plt.figure(figsize=(11,10), tight_layout=True)\n",
        "    im = plt.imshow(cm_norm, interpolation=\"nearest\", cmap=plt.cm.Blues, vmin=0., vmax=1.)\n",
        "\n",
        "    plt.title(title + \"\\n\")\n",
        "    plt.xticks(tick_marks, classes, rotation=22.5)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(\n",
        "            j, i,\n",
        "            format(cm[i, j], \"d\") + \"\\n\" + format(cm_norm[i, j], \".2f\"), \n",
        "            horizontalalignment=\"center\",\n",
        "            verticalalignment=\"center\",\n",
        "            color=\"white\" if cm_norm[i, j] > 0.5 else \"black\"\n",
        "        )\n",
        "\n",
        "    plt.ylabel(\"Target Labels\")\n",
        "    plt.xlabel(\"Predicted Labels\\nAccuracy={:0.4f}\".format(accuracy))\n",
        "    \n",
        "    # plot the color bar\n",
        "    divider = make_axes_locatable(plt.gca())\n",
        "    cax = divider.append_axes(\"right\", size=0.3, pad=0.2)\n",
        "    plt.colorbar(im, cax=cax)\n",
        "\n",
        "    # close the plot and return the figure\n",
        "    plt.close()\n",
        "    return fig"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xth5ypVipuwr"
      },
      "source": [
        "### Output the architecture of several pretrained PyTorch models.\n",
        "\n",
        "The following models all have the same high level ResNet architecture.\n",
        "* ResNet-18\n",
        "* ResNet-34\n",
        "* ResNet-50\n",
        "* ResNet-101\n",
        "* ResNet-152\n",
        "* ResNeXt-50-32x4d\n",
        "* Wide ResNet-50-2\n",
        "* Wide ResNet-101-2\n",
        "\n",
        "The following models all have he same high level DenseNet architecture.\n",
        "* Densenet-121\n",
        "* Densenet-169\n",
        "* Densenet-201\n",
        "* Densenet-161\n",
        "\n",
        "The following models all have he same high level Efficient architecture except for the number of MBConvBlock blocks. Printing these models do not show the correct placement of the swish activation layers (even though the implementation of the forward method is correct).\n",
        "* EfficienteNet-B0\n",
        "* EfficienteNet-B1\n",
        "* EfficienteNet-B2\n",
        "* EfficienteNet-B3\n",
        "* EfficienteNet-B4\n",
        "* EfficienteNet-B5\n",
        "* EfficienteNet-B6\n",
        "* EfficienteNet-B7\n",
        "\n",
        "```\n",
        "print_top_level_model_blocks(models.resnet18(), False)\n",
        "print_top_level_model_blocks(models.densenet121(), True)\n",
        "print_top_level_model_blocks(models.vgg11_bn(), True)\n",
        "print_top_level_model_blocks(models.vgg13_bn(), True)\n",
        "print_top_level_model_blocks(models.vgg16_bn(), True)\n",
        "print_top_level_model_blocks(models.vgg19_bn(), True)\n",
        "```\n",
        "\n",
        "The (formatted) output the `print_top_level_model_blocks` statements yields the following.\n",
        "\n",
        "Note:\n",
        "* Groups of Conv2d, BatchNorm, and ReLU layers have been condensed to CNR\n",
        "* Groups of Linear, ReLU, and Dropout layers have been condensed to LRD\n",
        "* EfficienteNet added for comparision\n",
        "\n",
        "```\n",
        "ResNet  | DenseNet      | VGG11_BN       | VGG13_BN       | VGG16_BN       | VGG19_BN         | EfficienteNet\n",
        "------- | ------------- | -------------- | -------------- | -------------- | ---------------- | -------------\n",
        "conv1   | features      | features       | features       | features       | features         | _conv_stem\n",
        "bn1     |   conv0       |   [00-02] CNR  |   [00-02] CNR  |   [00-02] CNR  |   [00-02] CNR    | _bn0\n",
        "relu    |   norm0       |                |   [03-05] CNR  |   [03-05] CNR  |   [03-05] CNR    | _swish\n",
        "maxpool |   relu0       |   [03] MaxPool |   [06] MaxPool |   [06] MaxPool |   [06] MaxPool2d | _blocks\n",
        "layer1  |   pool0       |   [04-06] CNR  |   [07-09] CNR  |   [07-09] CNR  |   [07-09] CNR    |   [0] MBConvBlock\n",
        "layer2  |   denseblock1 |                |   [10-12] CNR  |   [10-12] CNR  |   [10-12] CNR    |   [2] MBConvBlock\n",
        "layer3  |   transition1 |   [07] MaxPool |   [13] MaxPool |   [13] MaxPool |   [13] MaxPool2d |   [3] MBConvBlock\n",
        "layer4  |   denseblock2 |   [08-10] CNR  |   [14-16] CNR  |   [14-16] CNR  |   [14-16] CNR    |    .       .\n",
        "avgpool |   transition2 |   [11-13] CNR  |   [17-19] CNR  |   [17-19] CNR  |   [17-19] CNR    |    .       .\n",
        "fc      |   denseblock3 |                |                |   [20-22] CNR  |   [20-22] CNR    |    .       .\n",
        "        |   transition3 |                |                |                |   [23-25] CNR    |   [N-1] MBConvBlock\n",
        "        |   denseblock4 |   [14] MaxPool |   [20] MaxPool |   [23] MaxPool |   [26] MaxPool2d | _conv_head\n",
        "        |   norm5       |   [15-17] CNR  |   [21-23] CNR  |   [24-26] CNR  |   [27-29] CNR    | _bn1\n",
        "        | classifier    |   [18-20] CNR  |   [24-26] CNR  |   [27-29] CNR  |   [30-32] CNR    | _swish\n",
        "        |               |                |                |   [30-32] CNR  |   [33-35] CNR    | _avg_pooling\n",
        "        |               |                |                |                |   [36-38] CNR    | _dropout\n",
        "        |               |   [21] MaxPool |   [27] MaxPool |   [33] MaxPool |   [39] MaxPool2d | _fc\n",
        "        |               |   [22-24] CNR  |   [28-30] CNR  |   [34-36] CNR  |   [40-42] CNR    | \n",
        "        |               |   [25-27] CNR  |   [31-33] CNR  |   [37-39] CNR  |   [43-45] CNR    | \n",
        "        |               |                |                |   [40-42] CNR  |   [46-48] CNR    | \n",
        "        |               |                |                |                |   [49-51] CNR    | \n",
        "        |               |   [28] MaxPool |   [34] MaxPool |   [43] MaxPool |   [52] MaxPool2d | \n",
        "        |               | avgpool        | avgpool        | avgpool        | avgpool          | \n",
        "        |               | classifier     | classifier     | classifier     | classifier       | \n",
        "        |               |   [00-02] LRD  |   [00-02] LRD  |   [00-02] LRD  |   [00-02] LRD    | \n",
        "        |               |   [03-05] LRD  |   [03-05] LRD  |   [03-05] LRD  |   [03-05] LRD    | \n",
        "        |               |   [06] Linear  |   [06] Linear  |   [06] Linear  |   [06] Linear    | \n",
        "```\n",
        "\n",
        "The `print_top_level_model_blocks` function was also used to test whether I properly implemented the fine tuning code. For example,\n",
        "\n",
        "```\n",
        "model = ResNet18(pretrained=True, tuning_level=0)\n",
        "print_top_level_model_blocks(model._network, include_grandchildren=False, display_requires_grad=True)\n",
        "\n",
        "    ResNet, requires_grad=Mixed\n",
        "      conv1, requires_grad=False\n",
        "      bn1, requires_grad=False\n",
        "      relu, requires_grad=N/A\n",
        "      maxpool, requires_grad=N/A\n",
        "      layer1, requires_grad=False\n",
        "      layer2, requires_grad=False\n",
        "      layer3, requires_grad=False\n",
        "      layer4, requires_grad=False\n",
        "      avgpool, requires_grad=N/A\n",
        "      fc, requires_grad=True#\n",
        "\n",
        "model = ResNet18(pretrained=True, tuning_level=1)\n",
        "print_top_level_model_blocks(model._network, include_grandchildren=False, display_requires_grad=True)\n",
        "\n",
        "    ResNet, requires_grad=Mixed\n",
        "      conv1, requires_grad=False\n",
        "      bn1, requires_grad=False\n",
        "      relu, requires_grad=N/A\n",
        "      maxpool, requires_grad=N/A\n",
        "      layer1, requires_grad=False\n",
        "      layer2, requires_grad=False\n",
        "      layer3, requires_grad=False\n",
        "      layer4, requires_grad=True\n",
        "      avgpool, requires_grad=N/A\n",
        "      fc, requires_grad=True\n",
        "\n",
        "...\n",
        "\n",
        "model = ResNet18(pretrained=True, tuning_level=5)\n",
        "print_top_level_model_blocks(model._network, include_grandchildren=False, display_requires_grad=True)\n",
        "\n",
        "    ResNet, requires_grad=True\n",
        "      conv1, requires_grad=True\n",
        "      bn1, requires_grad=True\n",
        "      relu, requires_grad=N/A\n",
        "      maxpool, requires_grad=N/A\n",
        "      layer1, requires_grad=True\n",
        "      layer2, requires_grad=True\n",
        "      layer3, requires_grad=True\n",
        "      layer4, requires_grad=True\n",
        "      avgpool, requires_grad=N/A\n",
        "      fc, requires_grad=True\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUG75Y54puwr"
      },
      "source": [
        "def get_requires_grad_status(block) -> str:\n",
        "    params = list(block.parameters())\n",
        "    if not params:\n",
        "        return \"N/A\"\n",
        "    \n",
        "    or_of_params = False\n",
        "    and_of_params = True\n",
        "    for param in params:\n",
        "        or_of_params = or_of_params or param.requires_grad\n",
        "        and_of_params = and_of_params and param.requires_grad\n",
        "    if or_of_params and and_of_params:\n",
        "        return \"True\"\n",
        "    elif not or_of_params and not and_of_params:\n",
        "        return \"False\"\n",
        "    else:\n",
        "        return \"Mixed\"\n",
        "\n",
        "\n",
        "def print_top_level_model_blocks(\n",
        "    model:nn.Module, \n",
        "    include_grandchildren:bool = False, \n",
        "    display_requires_grad = False\n",
        "):\n",
        "    status = \"\"\n",
        "    if display_requires_grad:\n",
        "        status = f\", requires_grad={get_requires_grad_status(model)}\"\n",
        "    print(f\"{type(model).__name__}{status}\")\n",
        "    for child in model.named_children():\n",
        "        if display_requires_grad:\n",
        "            status = f\", requires_grad={get_requires_grad_status(child[1])}\"\n",
        "        print(f\"  {child[0]}{status}\")\n",
        "        if include_grandchildren:\n",
        "            for grandchild in child[1].named_children():\n",
        "                if display_requires_grad:\n",
        "                    status = f\", requires_grad={get_requires_grad_status(grandchild[1])}\"\n",
        "                if not grandchild[0].isnumeric():\n",
        "                    print(f\"    { grandchild[0]}{status}\")\n",
        "                else:\n",
        "                    print(f\"    [{grandchild[0]}] {type(grandchild[1]).__name__}{status}\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxmMNijlAb9v"
      },
      "source": [
        "def create_submission_file(path, class_names, fnames, preds):\n",
        "    # create a dictionary of numeric labels to text labels\n",
        "    label_dict = {key: value \n",
        "        for key, value in zip(range(len(class_names)), class_names)}\n",
        "\n",
        "    # convert the numeric labels to their text equivalents\n",
        "    labels = [label_dict[pred] for pred in preds]\n",
        "\n",
        "    # create a pandas data frame and write it to a CSV file\n",
        "    data_frame = pd.DataFrame({\"id\": fnames, \"class\": labels})\n",
        "    data_frame.to_csv(path, index=False)\n",
        "\n",
        "def create_submission_file_from_experiment(path, exp, dataloader):\n",
        "    # get predictions for the test data using the trained model            \n",
        "    fnames, preds = predict_test_data(exp.trained_model, dataloader, exp.device)\n",
        "\n",
        "    # create the submission file\n",
        "    create_submission_file(path, exp.class_names, fnames, preds)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssjl7-QG3-52"
      },
      "source": [
        "## <font style=\"color:green\">7. Experiment [5 Points]</font>\n",
        "\n",
        "Choose your optimizer and LR-scheduler and use the above methods and classes to train your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exu5ZBHV-U-G"
      },
      "source": [
        "### Base Experiment Classes\n",
        "\n",
        "The following base classes facilitate rapid experiment creation.\n",
        "* Experiment - Base class for the following classes.\n",
        "* VisualExperiment - Conduct data visualization experiments.\n",
        "* ModelExperiment - Conduct model training experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_datastore() -> KF13Datastore:\n",
        "    config_system = create_system_config()\n",
        "    config_dataset = create_dataset_config()\n",
        "    setup_system(config_system)\n",
        "    return KF13Datastore(\n",
        "        data_root = config_dataset.data_dir,\n",
        "        valid_size = config_dataset.valid_size,\n",
        "        random_seed = config_system.seed\n",
        "    )\n",
        "\n",
        "datastore = create_datastore()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQXZGHy0CzTZ"
      },
      "source": [
        "class Experiment(ABC):\n",
        "    def __init__(\n",
        "        self,\n",
        "        abbr: Optional[str] = None,\n",
        "        transform_resize: int = 256,\n",
        "        transform_crop_size: int = 224,\n",
        "        **config_overrides\n",
        "    ):\n",
        "        \"\"\"\n",
        "        This base class for data visualization and model training experiment does the following.\n",
        "        \n",
        "            - Creates the master configuration instance accomodating constructor overrides\n",
        "            - Sets up the system, e.g., ensures reproducibility, enables CUDA acceleration, etc.\n",
        "            - Configures experiment visualization \n",
        "        \"\"\"\n",
        "\n",
        "        # the experient abbreviation is not specified, use the class name removing the prefix\n",
        "        # \"Exp\" if present\n",
        "        if abbr is None:\n",
        "            name = type(self).__name__\n",
        "            self._abbr = name\n",
        "            if name.startswith(\"Exp\"):\n",
        "                self._abbr = name[3:]\n",
        "        else:\n",
        "            self._abbr = abbr\n",
        "        \n",
        "        # validate configuration overrides\n",
        "        unknown_overrides = np.setdiff1d(list(config_overrides.keys()), [\n",
        "            \"system_config_seed\",\n",
        "            \"data_aug_color_enabled\",\n",
        "            \"data_aug_color_brightness\",\n",
        "            \"data_aug_color_contrast\",\n",
        "            \"data_aug_color_saturation\",\n",
        "            \"data_aug_color_hue\",\n",
        "            \"data_aug_horz_flip_prob\",\n",
        "            \"data_aug_vert_flip_prob\",\n",
        "            \"data_aug_affine_enabled\",\n",
        "            \"data_aug_affine_rotation\",\n",
        "            \"data_aug_affine_translate\",\n",
        "            \"data_aug_affine_scale\",\n",
        "            \"data_aug_affine_shear\",\n",
        "            \"data_aug_erasing_prob\",\n",
        "            \"data_aug_erasing_scale\",\n",
        "            \"data_aug_erasing_ratio\",\n",
        "            \"data_aug_erasing_random\",\n",
        "            \"data_aug_noise_prob\",\n",
        "            \"data_aug_noise_mean\",\n",
        "            \"data_aug_noise_std\",\n",
        "            \"data_loader_batch_size\",\n",
        "            \"data_loader_num_workers\",\n",
        "            \"optimizer_learning_rate\",\n",
        "            \"optimizer_momentum\",\n",
        "            \"optimizer_weight_decay\",\n",
        "            \"optimizer_betas\",\n",
        "            \"scheduler_gamma\",\n",
        "            \"scheduler_step_size\",\n",
        "            \"scheduler_milestones\",\n",
        "            \"scheduler_patience\",\n",
        "            \"scheduler_threshold\",\n",
        "            \"trainer_training_epochs\",\n",
        "            \"trainer_model_saving_period\",\n",
        "            \"trainer_stop_loss_epochs\",\n",
        "            \"trainer_stop_acc_epochs\",\n",
        "            \"trainer_stop_acc_ema_alpha\",\n",
        "            \"trainer_stop_acc_threshold\"\n",
        "        ]).tolist()\n",
        "        if len(unknown_overrides) > 0:\n",
        "            raise Exception(f\"Unknown configuration overrides: {unknown_overrides}\")\n",
        "\n",
        "        # ToDo: Apply patch if CUDA is not available.\n",
        "        self._resize = transform_resize\n",
        "        self._crop_size = transform_crop_size\n",
        "        self._config = create_master_config(\n",
        "            transform_resize,\n",
        "            transform_crop_size,\n",
        "            **config_overrides\n",
        "        )\n",
        "        \n",
        "        setup_system(self._config.system)\n",
        "                \n",
        "        self.__visualizer = None\n",
        "        \n",
        "    @abstractproperty\n",
        "    def class_names(self):\n",
        "        pass\n",
        "\n",
        "    @abstractproperty\n",
        "    def _visualizer_name(self) -> str:\n",
        "        pass\n",
        "\n",
        "    def _open_visualizer(self):\n",
        "        if self.__visualizer is None:\n",
        "            self.__visualizer = TensorBoardVisualizer(os.path.join(\n",
        "                self._config.system.proj_dir,\n",
        "                self._config.trainer.visualizer_dir, \n",
        "                self._visualizer_name\n",
        "            ))\n",
        "        return self.__visualizer\n",
        "\n",
        "    def _close_visualizer(self):\n",
        "        if self.__visualizer is not None:\n",
        "            self.__visualizer.close_tensorboard()\n",
        "            self.__visualizer = None\n",
        "\n",
        "\n",
        "class VisualExperiment(Experiment):\n",
        "    \"\"\"\n",
        "    This is the base class for data visualization experiments.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        abbr: Optional[str] = None,\n",
        "        log_originals: bool = True,\n",
        "        log_augmentations:bool = True,\n",
        "        log_augmentations_suffix: str = \"augmented\",\n",
        "        transform_resize: int = 256,\n",
        "        transform_crop_size: int = 224,\n",
        "        **config_overrides\n",
        "    ):\n",
        "        super().__init__(\n",
        "            abbr,\n",
        "            transform_resize,\n",
        "            transform_crop_size,\n",
        "            **config_overrides\n",
        "        )\n",
        "        \n",
        "        self.__library = datastore.library\n",
        "        self.__class_names = datastore.class_names\n",
        "        self.__log_originals = log_originals\n",
        "        self.__log_augmentations = log_augmentations\n",
        "        self.__log_augmentations_suffix = log_augmentations_suffix\n",
        "\n",
        "    def log_sample_images(\n",
        "        self, \n",
        "        num_of_contact_sheets: int = 1, \n",
        "        log_originals: Optional[bool] = None, \n",
        "        log_augmentations: Optional[bool] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Create a 6 x 6 grid of images for each type of food in the data and\n",
        "        log these images to the visualizer.\n",
        "        \"\"\"\n",
        "\n",
        "        if log_originals is None:\n",
        "            log_originals = self.__log_originals\n",
        "\n",
        "        if log_augmentations is None:\n",
        "            log_augmentations = self.__log_augmentations\n",
        "\n",
        "        # abort if not logging either originals or augmentations\n",
        "        if not log_originals and not log_augmentations:\n",
        "            return\n",
        "\n",
        "        visualizer = self._open_visualizer()\n",
        "\n",
        "        for food, fnames in self.__library.items():\n",
        "            \n",
        "            # since we want to visualize the same images before and after data\n",
        "            # augmentation, we need to shuffle the data ourselves rather than\n",
        "            # have the data loader do this for us\n",
        "            shuffled = random.sample(fnames, len(fnames))\n",
        "\n",
        "            # create food specific datasets\n",
        "            original_dataset = KF13Dataset(\n",
        "                image_root = datastore.image_root,\n",
        "                fnames = shuffled,\n",
        "                transform = self._config.dataset.visual_transforms\n",
        "            )\n",
        "            augmented_dataset = KF13Dataset(\n",
        "                image_root = datastore.image_root,\n",
        "                fnames = shuffled,\n",
        "                transform = self._config.dataset.visual_aug_transforms\n",
        "            )\n",
        "\n",
        "            # load 36 original and augmented images\n",
        "            original_dataloader = DataLoader(original_dataset, batch_size=36, shuffle=False)\n",
        "            original_iter = iter(original_dataloader)\n",
        "            augmented_dataloader = DataLoader(augmented_dataset, batch_size=36, shuffle=False)\n",
        "            augmented_iter = iter(augmented_dataloader)\n",
        "\n",
        "            for idx in range(num_of_contact_sheets):\n",
        "                try:\n",
        "                    original_images, _ = next(original_iter)\n",
        "                    augmented_images, _ = next(augmented_iter)\n",
        "\n",
        "                    # save images to project's image directory (needs to exist!)\n",
        "                    # if log_originals:\n",
        "                    #     original_name = f\"{self.__class_names[food]}{(idx + 1):02d}.jpg\"\n",
        "                    #     original_path = os.path.join(proj_dir, \"images\", \"data\", original_name)\n",
        "                    #     torchvision.utils.save_image(original_images, fp=original_path, nrow=6)\n",
        "                    # if log_augmentations:\n",
        "                    #     augmented_name = f\"{self.__class_names[food]}{(idx + 1):02d}_{self.__log_augmentations_suffix}.jpg\"\n",
        "                    #     augmented_path = os.path.join(proj_dir, \"images\", \"data\", augmented_name)\n",
        "                    #     torchvision.utils.save_image(augmented_images, fp=augmented_path, nrow=6)\n",
        "\n",
        "                    # add image grid to visualizer\n",
        "                    if log_originals:\n",
        "                        visualizer.add_image(\n",
        "                            tag=self.__class_names[food], \n",
        "                            image=torchvision.utils.make_grid(original_images, nrow=6)\n",
        "                        )\n",
        "                    if log_augmentations:\n",
        "                        visualizer.add_image(\n",
        "                            tag=self.__class_names[food] + f\" (self.__log_augmentations_suffix)\", \n",
        "                            image=torchvision.utils.make_grid(augmented_images, nrow=6)\n",
        "                        )\n",
        "\n",
        "                except StopIteration:\n",
        "                    break\n",
        "        \n",
        "        self._close_visualizer()\n",
        "\n",
        "    @property\n",
        "    def library(self):\n",
        "        return self.__library\n",
        "\n",
        "    @property\n",
        "    def class_names(self):\n",
        "        return self.__class_names\n",
        "\n",
        "    @property\n",
        "    def _visualizer_name(self) -> str:\n",
        "        return self._abbr + f\"_DV_RS_{self._resize}_CS_{self._crop_size}\"\n",
        "\n",
        "\n",
        "class ModelExperiment(Experiment):\n",
        "    \"\"\"\n",
        "    This is the base class for model training experiments.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        abbr: Optional[str] = None,\n",
        "        load_prev_model: str = None,\n",
        "        optimizer: Optimizer = Optimizer.SGD,\n",
        "        scheduler: LrScheduler = LrScheduler.STEP,\n",
        "        di_approach: DIApproach = DIApproach.WEIGHTED_RANDOM_SAMPLER,\n",
        "        disable_data_aug: bool = False,\n",
        "        use_data_subsets: bool = False,\n",
        "        transform_resize: int = 256,\n",
        "        transform_crop_size: int = 224,\n",
        "        **config_overrides\n",
        "    ):\n",
        "        super().__init__(\n",
        "            abbr,\n",
        "            transform_resize,\n",
        "            transform_crop_size,\n",
        "            **config_overrides\n",
        "        )\n",
        "\n",
        "        self.__use_data_subsets = use_data_subsets\n",
        "        self.__data = self._get_data()\n",
        "        self.__class_names = self.__data.class_names\n",
        "\n",
        "        test_transforms = self._config.dataset.test_transforms\n",
        "        train_transforms = self._config.dataset.train_transforms\n",
        "        if disable_data_aug:\n",
        "            train_transforms = test_transforms\n",
        "\n",
        "        loss_weight = None\n",
        "        use_my_sampler = False\n",
        "        use_random_sampler = False\n",
        "        if di_approach == DIApproach.WEIGHTED_LOSS_FUNCTION:\n",
        "            loss_weight = torch.FloatTensor(self.__data.class_weights)\n",
        "        elif di_approach == DIApproach.WEIGHTED_RANDOM_SAMPLER:\n",
        "            use_random_sampler = True\n",
        "        elif di_approach == DIApproach.MY_SAMPLER:\n",
        "            use_my_sampler = True\n",
        "\n",
        "        self.__train_loader = self.__data.get_train_data_loader(\n",
        "            transform = train_transforms,\n",
        "            batch_size = self._config.data_loader.batch_size,\n",
        "            num_workers = self._config.data_loader.num_workers,\n",
        "            use_my_sampler = use_my_sampler,\n",
        "            use_random_sampler = use_random_sampler\n",
        "        )\n",
        "        \n",
        "        self.__valid_loader = self.__data.get_valid_data_loader(\n",
        "            transform = test_transforms,\n",
        "            batch_size = self._config.data_loader.batch_size,\n",
        "            num_workers = self._config.data_loader.num_workers\n",
        "        )                \n",
        "    \n",
        "        self.__model, model_id = self._get_model()\n",
        "        self.__model_name = self._abbr + \"_\" + model_id\n",
        "        self.__model_dir = os.path.join(self._config.system.proj_dir, self._config.trainer.model_dir)\n",
        "        if load_prev_model is not None:\n",
        "            self.__load_model(load_prev_model)\n",
        "\n",
        "        self.__loss_fn = nn.CrossEntropyLoss(weight=loss_weight)\n",
        "        self.__metric_fn = AccuracyEstimator(topk=(1, )) # ToDo: Fix! (trainer.py expects a dictionary w/ 'top1' key)\n",
        "        self.__optimizer = get_optimizer(self.__model, optimizer, self._config.optimizer)\n",
        "        self.__scheduler = get_scheduler(self.__optimizer, scheduler, self._config.scheduler)\n",
        "\n",
        "    @property\n",
        "    def class_names(self):\n",
        "        return self.__class_names\n",
        "\n",
        "    @property\n",
        "    def config(self):\n",
        "        return self._config\n",
        "\n",
        "    @property\n",
        "    def data(self) -> KF13TrainingData:\n",
        "        return self.__data\n",
        "\n",
        "    @property\n",
        "    def device(self) -> torch.device:\n",
        "        return torch.device(self._config.trainer.device)\n",
        "\n",
        "    @property\n",
        "    def trained_model(self) -> nn.Module:\n",
        "        self.__load_model()\n",
        "        return self.__model\n",
        "\n",
        "    @property\n",
        "    def train_loader(self):\n",
        "        return self.__train_loader\n",
        "    \n",
        "    @property\n",
        "    def valid_loader(self):\n",
        "        return self.__valid_loader\n",
        "\n",
        "    def train(self):\n",
        "        device = self.device\n",
        "        self.__model = self.__model.to(device)\n",
        "        self.__loss_fn = self.__loss_fn.to(device)\n",
        "\n",
        "        visualizer = self._open_visualizer()\n",
        "        model_trainer = Trainer(\n",
        "            model=self.__model,\n",
        "            loader_train=self.__train_loader,\n",
        "            loader_test=self.__valid_loader,\n",
        "            loss_fn=self.__loss_fn,\n",
        "            metric_fn=self.__metric_fn,\n",
        "            optimizer=self.__optimizer,\n",
        "            lr_scheduler=self.__scheduler,\n",
        "            model_save_dir=self.__model_dir,\n",
        "            model_name=self.__model_name,\n",
        "            model_saving_period=self._config.trainer.model_saving_period,\n",
        "            stop_loss_epochs=self._config.trainer.stop_loss_epochs,\n",
        "            stop_acc_ema_alpha=self._config.trainer.stop_acc_ema_alpha,\n",
        "            stop_acc_epochs=self._config.trainer.stop_acc_epochs,\n",
        "            stop_acc_threshold=self._config.trainer.stop_acc_threshold,\n",
        "            device=device,\n",
        "            data_getter=itemgetter(0),\n",
        "            target_getter=itemgetter(1),\n",
        "            stage_progress=self._config.trainer.progress_bar,\n",
        "            visualizer=visualizer,\n",
        "            get_key_metric=itemgetter(\"top1\")\n",
        "        )\n",
        "        model_trainer.register_hook(\"end_epoch\", hooks.end_epoch_hook_classification)\n",
        "        metrics = model_trainer.fit(self._config.trainer.training_epochs)\n",
        "        self._close_visualizer()\n",
        "\n",
        "        return metrics\n",
        "    \n",
        "    def log_graph(self):\n",
        "        model = self.trained_model\n",
        "        images, _ = next(iter(self.valid_loader))\n",
        "        device = self.device\n",
        "\n",
        "        visualizer = self._open_visualizer()\n",
        "        visualizer.add_graph(model.to(device), images.to(device))\n",
        "        self._close_visualizer()\n",
        "        \n",
        "    def log_pr_curves(self):\n",
        "        targets, pred_probs = get_targets_and_pred_probs(\n",
        "            self.trained_model, \n",
        "            self.valid_loader,\n",
        "            self.device\n",
        "        )\n",
        "\n",
        "        visualizer = self._open_visualizer()\n",
        "        visualizer.add_pr_curves(self.__class_names, targets, pred_probs)\n",
        "        self._close_visualizer()\n",
        "    \n",
        "    def log_confusion_matrix(self):\n",
        "        targets, preds = predict_valid_data(\n",
        "            self.trained_model,\n",
        "            self.valid_loader,\n",
        "            self.device\n",
        "        )\n",
        "\n",
        "        visualizer = self._open_visualizer()\n",
        "        cm = confusion_matrix(targets, preds)\n",
        "        tag = f\"Confusion Matrix ({self.__model_name})\"\n",
        "        figure = create_confusion_matrix(cm, self.class_names, self.__model_name)\n",
        "        visualizer.add_figure(tag=tag, figure=figure, close=True)\n",
        "        self._close_visualizer()\n",
        "\n",
        "    \"\"\"\n",
        "    Protected methods that may or must be overridden by derived classes.\n",
        "    \"\"\"\n",
        "    \n",
        "    @property\n",
        "    def _visualizer_name(self) -> str:\n",
        "        return self.__model_name\n",
        "\n",
        "    def _get_data(self) -> KF13TrainingData:\n",
        "        return datastore.get_training_data(\n",
        "            subset = self.__use_data_subsets,\n",
        "            random_seed = self._config.system.seed\n",
        "        )\n",
        "\n",
        "    @abstractmethod\n",
        "    def _get_model(self) -> Tuple[nn.Module, str]:\n",
        "        pass\n",
        "\n",
        "    \"\"\"\n",
        "    Private methods that should only be called by this class.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __load_model(self, model_name:str=None):\n",
        "        if model_name is None:\n",
        "            model_name = self.__model_name\n",
        "        path = os.path.join(self.__model_dir, model_name + \".pt\")\n",
        "        if os.path.exists(path):\n",
        "            self.__model.load_state_dict(torch.load(path))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1yMscAgpuws"
      },
      "source": [
        "def conduct(\n",
        "    exp: Experiment,\n",
        "    train_model:bool = True, \n",
        "    log_graph:bool = False,\n",
        "    log_pr_curves:bool = False,\n",
        "    log_confusion_matrix:bool = False,\n",
        "    free_experiment:bool = True):   \n",
        "    \"\"\"\n",
        "    This method conducts an visual or model experiment. A visual experiment logs original\n",
        "    and/or augmented images in 6 x 6 contact sheets to TensorBoard. A model experiment\n",
        "    performs the following steps and returns its training metrics.\n",
        "\n",
        "    1. Logs the model's graph.\n",
        "    2. Trains the model.\n",
        "    3. Logs the precision-recall curve for each food class.\n",
        "    4. Logs the confusion matrix.\n",
        "    \n",
        "    Note: Steps 3 and 4 are performed on the validation data set using model weights that\n",
        "          achieved the lowest average loss on the validaton data set.\n",
        "    \"\"\"\n",
        "    result = None\n",
        "\n",
        "    if isinstance(exp, VisualExperiment):\n",
        "        exp.log_sample_images()\n",
        "    \n",
        "    elif isinstance(exp, ModelExperiment):\n",
        "        if train_model:\n",
        "            result = exp.train()\n",
        "        if log_graph:\n",
        "            exp.log_graph()\n",
        "        if log_pr_curves:\n",
        "            exp.log_pr_curves()\n",
        "        if log_confusion_matrix:\n",
        "            exp.log_confusion_matrix()\n",
        "        \n",
        "    if free_experiment:\n",
        "        # after running several experiments, a \"RuntimeError: CUDA out of memory\"\n",
        "        # exception was raised ... trying to see whether explicitly deleting the\n",
        "        # experiment helps\n",
        "        del exp\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return result"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3KRsyK6puws"
      },
      "source": [
        "def is_outlier(points, thresh=3.5):\n",
        "    \"\"\"\n",
        "    Returns a boolean array with True if points are outliers and False \n",
        "    otherwise.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "        points : An numobservations by numdimensions array of observations\n",
        "        thresh : The modified z-score to use as a threshold. Observations with\n",
        "            a modified z-score (based on the median absolute deviation) greater\n",
        "            than this value will be classified as outliers.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "        mask : A numobservations-length boolean array.\n",
        "\n",
        "    References:\n",
        "    ----------\n",
        "        Boris Iglewicz and David Hoaglin (1993), \"Volume 16: How to Detect and\n",
        "        Handle Outliers\", The ASQC Basic References in Quality Control:\n",
        "        Statistical Techniques, Edward F. Mykytka, Ph.D., Editor. \n",
        "    \"\"\"\n",
        "    if len(points.shape) == 1:\n",
        "        points = points[:,None]\n",
        "        \n",
        "    median = np.median(points, axis=0)\n",
        "    diff = np.sum((points - median)**2, axis=-1)\n",
        "    diff = np.sqrt(diff)\n",
        "    med_abs_deviation = np.median(diff)\n",
        "\n",
        "    modified_z_score = 0.6745 * diff / med_abs_deviation\n",
        "\n",
        "    return modified_z_score > thresh\n",
        "\n",
        "\n",
        "class AnalyzeRuns(ABC):\n",
        "    \"\"\"\n",
        "    A utility class to do the following for a given TensorBoard run:\n",
        "    \n",
        "        - return a dictionary of scalars\n",
        "        - return accuracy at epoch where loss is lowest\n",
        "        - return overfitting metric\n",
        "        \n",
        "    Note: The overfitting metric is the slope of the test loss divided\n",
        "          by the train loss. A value of zero indicates no overfitting.\n",
        "    \"\"\"\n",
        "    def __init__(self, visualizer_dir):\n",
        "        self._visualizer_dir = visualizer_dir\n",
        "        self.__all_runs = os.listdir(os.path.join(visualizer_dir))\n",
        "\n",
        "    def _get_runs(self, filter:str) -> List[str]:\n",
        "        import re\n",
        "        runs = [run for run in self.__all_runs \n",
        "                            if re.search(filter, run) is not None]\n",
        "        runs.sort()\n",
        "        return runs\n",
        "\n",
        "    def get_scalars(self, run: str):\n",
        "        from tensorboard.backend.event_processing import event_accumulator\n",
        "        path = os.path.join(self._visualizer_dir, run)\n",
        "        event_acc = event_accumulator.EventAccumulator(path)\n",
        "        event_acc.Reload()\n",
        "\n",
        "        scalars = {}\n",
        "        for tag in sorted(event_acc.Tags()[\"scalars\"]):\n",
        "            x, y = [], []\n",
        "            for scalar_event in event_acc.Scalars(tag):\n",
        "                x.append(scalar_event.step)\n",
        "                y.append(scalar_event.value)\n",
        "            scalars[tag] = (np.asarray(x), np.asarray(y))\n",
        "        return scalars\n",
        "\n",
        "    @classmethod\n",
        "    def _get_min_loss(cls, scalars) -> float:\n",
        "        index = np.argmin(scalars[\"data/test_loss\"][1])\n",
        "        return scalars[\"data/test_loss\"][1][index].item()\n",
        "\n",
        "    @classmethod\n",
        "    def _get_auc_loss(cls, scalars) -> float:\n",
        "        return np.sum(scalars[\"data/test_loss\"][1])\n",
        "\n",
        "    @classmethod\n",
        "    def _get_accuracy(cls, scalars) -> float:\n",
        "        index = np.argmin(scalars[\"data/test_loss\"][1])\n",
        "        return scalars[\"data/test_metric:top1\"][1][index].item()\n",
        "\n",
        "    @classmethod\n",
        "    def _get_overfitting_metric(cls, scalars, alpha:float=0.3) -> float:\n",
        "        from numpy.polynomial.polynomial import polyfit\n",
        "        loss_tst = scalars[\"data/test_loss\"][1]\n",
        "        loss_trn = scalars[\"data/train_loss\"][1]\n",
        "        ratio = [tst / trn for tst, trn in zip(loss_tst, loss_trn)]   \n",
        "        _, m = polyfit(list(range(len(ratio))), ratio, 1)\n",
        "        return m\n",
        "\n",
        "    @classmethod\n",
        "    def _ema_smoothing(cls, data, alpha=0.3) -> List[float]:\n",
        "        data_ema = []\n",
        "        last = data[0]\n",
        "        for datum in data:\n",
        "            last = alpha * datum + (1 - alpha) * last\n",
        "            data_ema.append(last)\n",
        "        return data_ema\n",
        "\n",
        "\n",
        "class AnalyzeRunsForOverfitting(AnalyzeRuns):\n",
        "    \"\"\"\n",
        "    A utility class to do the following:\n",
        "    \n",
        "        - return a list of runs matching the filter\n",
        "        - return a dictionary of scalars for a given run\n",
        "        - return accuracy at epoch where loss is lowest for a given run\n",
        "        - return overfitting metric for a given run\n",
        "        - return a summary of all runs\n",
        "        - return a figure for a given run\n",
        "        \n",
        "    Note: The overfitting metric is the slope of the test loss divided\n",
        "          by the train loss. A value of zero indicates no overfitting.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, visualizer_dir:str, filter:str=\"^.*\"):\n",
        "        super().__init__(visualizer_dir)\n",
        "        self.__runs = self._get_runs(filter) \n",
        "\n",
        "    @property\n",
        "    def runs(self) -> List[str]:\n",
        "        return self.__runs\n",
        "    \n",
        "    def get_summary(self) -> List[Tuple[str, float, float]]:\n",
        "        summary = []\n",
        "        for run in self.runs:\n",
        "            scalars = self.get_scalars(run)\n",
        "            if len(scalars.keys()) == 4:\n",
        "                accuracy = AnalyzeRuns._get_accuracy(scalars)\n",
        "                test_loss = AnalyzeRuns._get_min_loss(scalars)\n",
        "                overfitting = AnalyzeRuns._get_overfitting_metric(scalars)\n",
        "                summary.append((run, test_loss, accuracy, overfitting))\n",
        "        return summary\n",
        "\n",
        "    @classmethod\n",
        "    def get_plot(cls, scalars, title, alpha=0.3):\n",
        "        from numpy.polynomial.polynomial import polyfit\n",
        "        accuracy = AnalyzeRuns._get_accuracy(scalars)\n",
        "        loss_tst = scalars[\"data/test_loss\"][1]\n",
        "        loss_trn = scalars[\"data/train_loss\"][1]\n",
        "        loss_tst_ema = AnalyzeRuns._ema_smoothing(loss_tst, alpha)\n",
        "        loss_trn_ema = AnalyzeRuns._ema_smoothing(loss_trn, alpha)\n",
        "\n",
        "        x = list(range(len(loss_tst_ema)))\n",
        "        ratio = [tst / trn for tst, trn in zip(loss_tst, loss_trn)]       \n",
        "        b, m = polyfit(x, ratio, 1)\n",
        "        ratio_bf = [m * x1 + b for x1 in x] \n",
        "\n",
        "        fig = plt.figure(figsize=(8,4))\n",
        "        plt.suptitle(f\"{title} ({accuracy:.2f}%)\")\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.title(\"loss\")\n",
        "        plt.plot(x, loss_trn, color=\"#ffa0a0\")\n",
        "        plt.plot(x, loss_trn_ema, color=\"#ff0000\", label=\"test\")\n",
        "        plt.plot(x, loss_tst, color=\"#a0a0ff\")\n",
        "        plt.plot(x, loss_tst_ema, color=\"#0000ff\", label=\"train\")\n",
        "        plt.legend()\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.title(f\"loss (overfit: {m:.3f})\")\n",
        "        plt.gca().set_ylim([0.0, 4.0])\n",
        "        plt.plot(x, ratio, color=\"#a0ffa0\")\n",
        "        plt.plot(x, ratio_bf, color=\"#00ff00\", label=\"test/train\")\n",
        "        plt.legend()\n",
        "        plt.close()\n",
        "        return fig\n",
        "\n",
        "\n",
        "class AnalyzeRunsForLearningRate(AnalyzeRuns):\n",
        "    \"\"\"\n",
        "    A utility class to do the following:\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, visualizer_dir, filters:List[str]):\n",
        "        super().__init__(visualizer_dir)\n",
        "        self.__sets = [self._get_runs(filter) for filter in filters] \n",
        "\n",
        "    @property\n",
        "    def sets(self) -> List[List[str]]:\n",
        "        return self.__sets\n",
        "    \n",
        "    def get_summary(self, runs: List[str]) -> List[Tuple[str, float, float, float, float]]:\n",
        "        summary = []\n",
        "        for run in runs:\n",
        "            scalars = self.get_scalars(run)\n",
        "            if len(scalars.keys()) == 4:\n",
        "                lr = scalars[\"data/learning_rate\"][1][0].item()\n",
        "                min_loss = AnalyzeRuns._get_min_loss(scalars)\n",
        "                auc_loss = AnalyzeRuns._get_auc_loss(scalars)\n",
        "                accuracy = AnalyzeRuns._get_accuracy(scalars)\n",
        "                summary.append((run, lr, min_loss, auc_loss, accuracy))\n",
        "        return summary\n",
        "\n",
        "    @classmethod\n",
        "    def get_name(cls, summary) -> str:\n",
        "        import re\n",
        "        try:\n",
        "            run = summary[0][0]\n",
        "            name = list(re.split(\"_LR[1-9]E[\\-+]]?[0-9]+\", run)[0])\n",
        "            name[2] = '_'\n",
        "            return \"\".join(name)\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    @classmethod\n",
        "    def get_plot(cls, summary, title:Optional[str]=None):\n",
        "        # convert tuples to list and transpose 2D list\n",
        "        summary = list(map(list, zip(*[list(t) for t in summary])))\n",
        "\n",
        "        # extract arrays to plot\n",
        "        lr = np.array(summary[1])\n",
        "        min_loss = np.array(summary[2])\n",
        "        auc_loss = np.array(summary[3])\n",
        "        accuracy = np.array(summary[4])\n",
        "        min_loss_flt = min_loss[~is_outlier(min_loss)]\n",
        "        auc_loss_flt = auc_loss[~is_outlier(auc_loss)]\n",
        "\n",
        "        # since outliers are eliminated, explicitly set axis limits\n",
        "        xlim = [0.9 * lr[0], 1.1 * lr[-1]]\n",
        "        ylim_min_loss = [0.9 * np.min(min_loss_flt), 1.1 * np.max(min_loss_flt)]\n",
        "        ylim_auc_loss = [0.9 * np.min(auc_loss_flt), 1.1 * np.max(auc_loss_flt)]\n",
        "\n",
        "        fig = plt.figure(figsize=(12,4))\n",
        "        if title is not None:\n",
        "            plt.suptitle(f\"{title}\")\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.title(\"min loss\")\n",
        "        plt.gca().set_xlim(xlim)\n",
        "        plt.gca().set_ylim(ylim_min_loss)\n",
        "        plt.plot(lr, min_loss, \"r\")\n",
        "        plt.xscale('log')\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.title(\"auc loss\")\n",
        "        plt.gca().set_xlim(xlim)\n",
        "        plt.gca().set_ylim(ylim_auc_loss)\n",
        "        plt.plot(lr, auc_loss, \"g\")\n",
        "        plt.xscale('log')\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.title(\"accuracy\")\n",
        "        plt.gca().set_xlim(xlim)\n",
        "        plt.plot(lr, accuracy, \"b\")\n",
        "        plt.xscale('log')\n",
        "        plt.close()\n",
        "        return fig\n",
        "\n",
        "def analyze_runs_for_overfitting(filter: str = \"^.*\"):\n",
        "    config = create_trainer_config()\n",
        "    analyze = AnalyzeRunsForOverfitting(os.path.join(proj_dir, config.visualizer_dir), filter)\n",
        "\n",
        "    # create a markdown table\n",
        "    print(\"|Experiment|Test Loss|Accuracy|Overfitting Metric|\")\n",
        "    print(\"|:---|:---:|:---:|:---:|\")\n",
        "    for item in analyze.get_summary():\n",
        "        print(f\"|{item[0]}|{item[1]:.3f}|{item[2]:.2f}|{item[3]:.3f}|\")\n",
        "\n",
        "    # save loss figures ... destination directory must exist!\n",
        "    for run in analyze.runs:\n",
        "        try:\n",
        "            path = os.path.join(proj_dir, \"images\", \"loss\", run + \".png\")\n",
        "            scalars = analyze.get_scalars(run)\n",
        "            if len(scalars.keys()) == 4:\n",
        "                fig = AnalyzeRunsForOverfitting.get_plot(scalars, run)\n",
        "                fig.savefig(path, facecolor=\"#ffffff\")\n",
        "                plt.close(fig)\n",
        "        except:\n",
        "            print(f\"Could not plot '{run}' for overfitting analysis.\")\n",
        "\n",
        "def analyze_runs_for_learning_rate(filters: List[str]):\n",
        "    config = create_trainer_config()\n",
        "    analyze = AnalyzeRunsForLearningRate(os.path.join(proj_dir, config.visualizer_dir), filters)\n",
        "    sets = analyze.sets\n",
        "\n",
        "    for (filter, exp_set) in zip(filters, sets):\n",
        "        # create a markdown tables \n",
        "        print(f\"**Experiment Set: {filter}**\")\n",
        "        print()\n",
        "        print(\"|Experiment|Learning Rate|Min Loss|AUC Loss|Accuracy|\")\n",
        "        print(\"|:---|:---:|:---:|:---:|:---:|\")\n",
        "        summary = analyze.get_summary(exp_set)\n",
        "        for item in summary:\n",
        "            print(f\"|{item[0]}|{item[1]:.2e}|{item[2]:.2f}|{item[3]:.2f}|{item[4]:.2f}|\")\n",
        "        print()\n",
        "\n",
        "        # save learning rate figures ... destination directory must exist\n",
        "        name = AnalyzeRunsForLearningRate.get_name(summary)\n",
        "        path = os.path.join(proj_dir, \"images\", \"lr\", name + \".png\")\n",
        "        try:\n",
        "            fig = AnalyzeRunsForLearningRate.get_plot(summary, f\"Experiment Filter: {filter}\")\n",
        "            fig.savefig(path, facecolor=\"#ffffff\")\n",
        "            plt.close(fig)\n",
        "        except:\n",
        "            print(f\"Could not plot '{filter}' for learning rate analysis.\")"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLi5W6s06tZO"
      },
      "source": [
        "## Experiment Group A: Data Visualization and Training Pipeline Check\n",
        "\n",
        "### Introduction\n",
        "\n",
        "This first set of experiments logs contact sheets, 6 x 6 grids of images, of each food type to the visualizer with and without data augmentation.\n",
        "\n",
        "The second set of experiments train only the classifier layer (fc) of the pre-trained Resnet18 model using a subset of the data without augmentation to check the training pipeline. In the first experiment, training will stop after 100 epochs. In the second experiment, training will stop after 100 epochs or when the smoothed accuracy (computed by [expontential moving average](https://towardsdatascience.com/moving-averages-in-python-16170e20f6c#144e) with an alpha = 0.3) does not decrease by 2% within 10 epochs.\n",
        "\n",
        "The third set of experiments varies the number of data loader worker threads to determine the optimal number for future experiments. These experiments stop after 11 epochs. The time between logging the first and eleventh epochs' test metrics divided by 10 will be used to evaluate data loading efficiency. Since the purpose to evaluate data loading, not the forward/back-propogation training cycle, a smaller model, ResNet18, was used. Furthermore, saving the model's state is disabled to eliminate its time contribution.\n",
        "\n",
        "### Results\n",
        "\n",
        "I used the data augmentation transforms I created for project 1. The data validation experiment revealed issues with these transforms. First, the color jitter transform dramatically changed the image's color. While this was not detrimental to classifying cats, dogs, and pandas; I suspect it may reduce accuracy on the KenyanFood13 dataset. Second, the amount of translation and scaling was too agressive.\n",
        "\n",
        "To properly set the data augmentation transform parameters, I ran several experiments not shown in this notebook. These experiments disabled all but one type of augmentation in order to \"tune\" it. For example, to properly set the hue parameter of the color jitter transform, I disabled the horizontal/vertical flips, affine, and erase transforms. Furthermore, I set the color jitter's brightness, contrast, and saturation to the values that would produce the original image. I then found acceptable minimum and maximum values for the hue parameter. After conducting all of these data augmentation tuning experiments, I updated the configuration file and re-ran the data visualization experiment. I visualized the entire dataset to external files, but only logged the following 6 x 6 contact sheets to Tensorboard.\n",
        "\n",
        "* ExpAAA - Original Images\n",
        "* ExpAAA - \"Tuned\" Augmented Images\n",
        "* ExpAAB - \"Untuned\" Augmented Images\n",
        "\n",
        "The training pipeline check experiment performed as expected. The training and test loss decreased and the accuracy increased to approximtely 60%.\n",
        "\n",
        "The results of the data loader experiments are shown below. For a small number of data loader threads, adding additional threads significantly increases the data loader's efficiency thereby allowing it to keep up with the GPU. However, past six or seven threads, the impact is minimal on a small model.\n",
        "\n",
        "**Table A1.** The impact of the number of data loader worker threads on a training cycle.\n",
        "\n",
        "|Experiment|Workers|Time/Epoch|\n",
        "|:---:|:---:|:---:|\n",
        "|ACA|1|02:23|\n",
        "|ACB|2|01:16|\n",
        "|ACC|3|00:53|\n",
        "|ACD|4|00:40|\n",
        "|ACE|5|00:33|\n",
        "|ACF|6|00:28|\n",
        "|ACG|7|00:25|\n",
        "|ACH|8|00:22|\n",
        "|ACI|9|00:22|\n",
        "|ACJ|10|00:22|\n",
        "|ACK|11|00:21|\n",
        "|ACL|12|00:21|\n",
        "|ACM|13|00:20|\n",
        "|ACN|14|00:19|\n",
        "|ACO|15|00:18|\n",
        "|ACP|16|00:18|\n",
        "\n",
        "### Conclusions\n",
        "\n",
        "The data augmentation looks reasonable. The training pipeline appears to work as expected. Since I am training models on a workstation with 8 hyper-threaded cores for a total of 16 CPUs and GPU acceleration, I dedicated 8 to 12 worker threads to the data loader. As the model complexity increases, the impact of fewer worker threads lessens because each worker has more time to load and transform images before the GPU requires the next batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nCwE0ULBBU6"
      },
      "source": [
        "class ExpAAA(VisualExperiment):\n",
        "    \"\"\"\n",
        "    Data Visualization Experiment\n",
        "    Log original and augmented images using \"tuned\" parameters.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "class ExpAAB(VisualExperiment):\n",
        "    \"\"\"\n",
        "    Data Visualization Experiment\n",
        "    Log augmented images using Project 1's \"untuned\" parameters.\n",
        "    \"\"\"   \n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            log_originals = False,\n",
        "            log_augmentations = True,\n",
        "            data_aug_color_enabled = True,\n",
        "            data_aug_color_brightness = (0.75, 1.25),\n",
        "            data_aug_color_contrast = (0.75, 1.25),\n",
        "            data_aug_color_saturation = (0.75, 1.25),\n",
        "            data_aug_color_hue = (-0.25, 0.25),\n",
        "            data_aug_horz_flip_prob = 0.5,\n",
        "            data_aug_vert_flip_prob = 0.5,\n",
        "            data_aug_affine_enabled = True,\n",
        "            data_aug_affine_rotation = 45,\n",
        "            data_aug_affine_translate = (0.2, 0.2),\n",
        "            data_aug_affine_scale = (0.8, 1.2),\n",
        "            data_aug_affine_shear = (0.0, 0.0),\n",
        "            data_aug_erasing_prob = 0.5,\n",
        "            data_aug_erasing_scale = (0.02, 0.33),\n",
        "            data_aug_erasing_ratio = (0.3, 3.3)\n",
        "        )\n",
        "\n",
        "class ExpABA(ModelExperiment):\n",
        "    \"\"\"\n",
        "    Training Pipeline Check\n",
        "    Stop after 40 epoochs\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            use_data_subsets = True,\n",
        "            disable_data_aug = True,\n",
        "            trainer_training_epochs = 100,\n",
        "            scheduler_step_size = 20\n",
        "        )\n",
        "    def _get_model(self):\n",
        "        return ResNet18(pretrained=True, tuning_level=0), \"ResNet18-PT0\"\n",
        "\n",
        "class ExpABB(ModelExperiment):\n",
        "    \"\"\"\n",
        "    Training Pipeline Check\n",
        "    Stop after 40 epochs or where the accuracy increases little.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            use_data_subsets = True,\n",
        "            disable_data_aug = True,\n",
        "            trainer_training_epochs = 100, \n",
        "            scheduler_step_size = 20,\n",
        "            trainer_stop_acc_epochs = 10,\n",
        "            trainer_stop_acc_ema_alpha = 0.3,\n",
        "            trainer_stop_acc_threshold = 2.0\n",
        "        )\n",
        "    def _get_model(self):\n",
        "        return ResNet18(pretrained=True, tuning_level=0), \"ResNet18-PT0\"\n",
        "\n",
        "class ExpAC_(ModelExperiment):\n",
        "    \"\"\"\n",
        "    Data Loader Optimization\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        exp_id: str, # expects a single uppercase letter\n",
        "        data_loader_num_workers: int\n",
        "    ):\n",
        "        super().__init__(\n",
        "            abbr = \"AC\" + exp_id,\n",
        "            scheduler_step_size = 20,\n",
        "            trainer_training_epochs = 11,\n",
        "            trainer_model_saving_period = -1, # disable\n",
        "            data_loader_num_workers = data_loader_num_workers\n",
        "        )\n",
        "    def _get_model(self):\n",
        "        return ResNet18(pretrained=True, tuning_level=0), \"ResNet18-PT0\"\n",
        "\n",
        "def conduct_group_A():\n",
        "    # set 1 - visualize data\n",
        "    conduct(ExpAAA())\n",
        "    conduct(ExpAAB())\n",
        "    # set 2 - check trainer pipeline\n",
        "    conduct(ExpABA())\n",
        "    conduct(ExpABB())\n",
        "    # set 3 - optimizer data loader\n",
        "    conduct(ExpAC_('A', 1))\n",
        "    conduct(ExpAC_('B', 2))\n",
        "    conduct(ExpAC_('C', 3))\n",
        "    conduct(ExpAC_('D', 4))\n",
        "    conduct(ExpAC_('E', 5))\n",
        "    conduct(ExpAC_('F', 6))\n",
        "    conduct(ExpAC_('G', 7))\n",
        "    conduct(ExpAC_('H', 8))\n",
        "    conduct(ExpAC_('I', 9))\n",
        "    conduct(ExpAC_('J', 10))\n",
        "    conduct(ExpAC_('K', 11))\n",
        "    conduct(ExpAC_('L', 12))\n",
        "    conduct(ExpAC_('M', 13))\n",
        "    conduct(ExpAC_('N', 14))\n",
        "    conduct(ExpAC_('O', 15))\n",
        "    conduct(ExpAC_('P', 16))       "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "source": [
        "## Experiment Group B: Learning Rate\n",
        "\n",
        "### Introduction\n",
        "\n",
        "The purpose of these experiments is to explore the impact of learning rate on training the following pretrained models.\n",
        "\n",
        "* EfficientNet-B0 (Set A)\n",
        "* EfficientNet-B2 (Set B)\n",
        "* EfficientNet-B4 (Set C)\n",
        "\n",
        "The entire network was trained, i.e., no layers were frozen. Each model was trained for 10 epochs with learning rates that varied between 1.00e-06 and 1.00e-02 with 4 learning rates per decade.\n",
        "\n",
        "### Results\n",
        "\n",
        "For each set of experiments, the following metrics were plotted as a function of learning rate in Figures B1, B2, and B3.\n",
        "\n",
        "* Minimum test loss (min loss)\n",
        "* Area under the test loss curve (auc loss)\n",
        "* Test accuracy at the epoch of minimum test loss (accuracy)\n",
        "\n",
        "![EfficientNet-B0 learning rate results](https://media.githubusercontent.com/media/blazingcayenne/deep_learning_with_pytorch_project2/main/images/BA__EfficientNetB0_PT5.png?raw=true)<br>\n",
        "**Figure B1.** The impact of learning rate on the EfficientNet-B0 model.\n",
        "\n",
        "![EfficientNet-B2 learning rate results](https://media.githubusercontent.com/media/blazingcayenne/deep_learning_with_pytorch_project2/main/images/BB__EfficientNetB2_PT5.png?raw=true)<br>\n",
        "**Figure B2.** The impact of learning rate on the EfficientNet-B2 model.\n",
        "\n",
        "![EfficientNet-B4 learning rate results](https://media.githubusercontent.com/media/blazingcayenne/deep_learning_with_pytorch_project2/main/images/BC__EfficientNetB4_PT5.png?raw=true)<br>\n",
        "**Figure B3.** The impact of learning rate on the EfficientNet-B4 model.\n",
        "\n",
        "Table B1 enumerates the aforementioned metrics for each set's experiment with the lowest test loss.\n",
        "\n",
        "**Table B1.** The impact of learning rate on the EfficientNet models.\n",
        "\n",
        "|Experiment|Learning Rate|Min Loss|AUC Loss|Accuracy|\n",
        "|:---|:---:|:---:|:---:|:---:|\n",
        "|BAK_EfficientNetB0_PT5_LR3E-4|3.16e-04|0.76|9.69|75.22|\n",
        "|BBI_EfficientNetB2_PT5_LR1E-4|1.00e-04|0.69|8.51|77.63|\n",
        "|BCH_EfficientNetB4_PT5_LR6E-5|5.62e-05|0.57|6.80|81.88|\n",
        "\n",
        "### Conclusions\n",
        "\n",
        "The optimal learning rate appears to slightly decrease as the model complexity increases. Hence, a maximum learning rate of 1.00e-04 will be used in future EfficientNet experiments."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ExpB__(ModelExperiment):\n",
        "    def __init__(\n",
        "        self,\n",
        "        set_id: str, # expects a single uppercase letter\n",
        "        exp_id: str, # expects a single uppercase letter\n",
        "        pretrained: bool,\n",
        "        tuning_level: int,\n",
        "        learning_rate: float,\n",
        "        model_type: TorchVisionModel,\n",
        "        model_abbr: str\n",
        "    ):\n",
        "        image_size = get_image_size(model_type)\n",
        "        batch_size = get_batch_size(model_type)\n",
        "        tuning_suffix = get_tuning_suffix(pretrained, tuning_level)\n",
        "        learning_rate_suffix = get_learning_rate_suffix(learning_rate)\n",
        "\n",
        "        self.__pretrained = pretrained\n",
        "        self.__tuning_level = tuning_level\n",
        "        self.__model_type = model_type\n",
        "        self.__model_abbr = model_abbr + tuning_suffix + learning_rate_suffix\n",
        "\n",
        "        super().__init__(\n",
        "            abbr = 'B' + set_id + exp_id,\n",
        "            transform_resize = int(image_size * 16 / 14),\n",
        "            transform_crop_size = image_size,\n",
        "            data_loader_num_workers = 12,\n",
        "            data_loader_batch_size = batch_size,\n",
        "            optimizer = Optimizer.ADAM,\n",
        "            optimizer_learning_rate = learning_rate,\n",
        "            scheduler_step_size = 200, # disable scheduler\n",
        "            trainer_training_epochs = 10\n",
        "        )\n",
        "\n",
        "    def _get_model(self):\n",
        "        return self.__model_type(self.__pretrained, self.__tuning_level), self.__model_abbr\n",
        "\n",
        "def conduct_group_B():\n",
        "    set_id = 'A'\n",
        "    for model_type in [EfficientNetB0, EfficientNetB2, EfficientNetB4]:\n",
        "        model_abbr = model_type.__name__\n",
        "        exp_id = 'A'\n",
        "        for idx in range(17):\n",
        "            learning_rate = 10**((idx - 24) / 4.0)\n",
        "            conduct(ExpB__(set_id, exp_id, True, 5, learning_rate, model_type, model_abbr))\n",
        "            exp_id = chr(ord(exp_id[0]) + 1)\n",
        "        set_id = chr(ord(set_id[0]) + 1)"
      ]
    },
    {
      "source": [
        "## Experiment Group C: Model Size\n",
        "\n",
        "### Introduction\n",
        "\n",
        "The purpose of these experiments is to explore the impact of EfficientNet model size on bias and variance. If the model is too small for the dataset, then the accuracy will suffer (high bias). However, if the model is too large for the dataset, then it will overfit the data (high variance). Hence, it is important to select the appropriately sized model for the KenyanFood13 dataset. The following pretrained EfficientNet models will be evaluated.\n",
        "\n",
        "* EfficientNet-B3 (Set A)\n",
        "* EfficientNet-B4 (Set B)\n",
        "* EfficientNet-B5 (Set C)\n",
        "* EfficientNet-B6 (Set D)\n",
        "\n",
        "Each of the models under test will be trained using the following approaches to test the strategies given by [Marcelino](https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751) and [Gupta](https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/).\n",
        "\n",
        "* Only the classifier is trained (\\_PT0).\n",
        "* The classifier and ~ 40% of convolutional base is trained (\\_PT2).\n",
        "* The entire network is trained (\\_PT5).\n",
        "\n",
        "The models were trained for 35 epochs. The learning rate started at 1.00E-04 and was multipied by the $\\sqrt{0.1}$ every 10 epochs.\n",
        "\n",
        "### Results\n",
        "\n",
        "The test accuracy at the epoch where the test loss is lowest is used to compare model bias. An overfitting metric was developed to compare model variance. The overfitting metric is computed as follows.\n",
        "\n",
        "* The test loss is divided by the training loss at each epoch.\n",
        "* The loss ratios are fitted to a line (polynomial of order 1).\n",
        "* The overfitting metric is the slope of the best fit line.\n",
        "\n",
        "The overfitting metric is zero is the test loss decreases at the same rate as the training loss. It increases as the test loss decreases at a slower rate than the training loss.\n",
        "\n",
        "Tables C1, C2, and C3 enumerate the minimum test loss, test accuracy at the epoch with minimum test less, and overfitting metric for the three aforementioned transfer learning strategies. Figures C1, C2, C3 are loss plots from three sample runs. The first sample does not depict any overfitting. The second and third samples do show some overfitting.\n",
        "\n",
        "**Table C1:** Analysis of runs where only the classifier is trained.\n",
        "\n",
        "|Experiment|Test Loss|Accuracy|Overfitting Metric|\n",
        "|:---|:---:|:---:|:---:|\n",
        "|CAA_EfficientNetB3_PT0|1.546|54.96|-0.001|\n",
        "|CBA_EfficientNetB4_PT0|1.456|55.43|-0.001|\n",
        "|CCA_EfficientNetB5_PT0|1.503|55.12|-0.001|\n",
        "|CDA_EfficientNetB6_PT0|1.529|54.13|-0.002|\n",
        "<br>\n",
        "\n",
        "**Table C2:** Analysis of runs where classifier and ~ 40% of convolutional base is trained.\n",
        "\n",
        "|Experiment|Test Loss|Accuracy|Overfitting Metric|\n",
        "|:---|:---:|:---:|:---:|\n",
        "|CAB_EfficientNetB3_PT2|0.673|78.11|0.075|\n",
        "|CBB_EfficientNetB4_PT2|0.619|80.89|0.118|\n",
        "|CCB_EfficientNetB5_PT2|0.640|80.05|0.130|\n",
        "|CDB_EfficientNetB6_PT2|0.744|79.51|0.142|\n",
        "<br>\n",
        "\n",
        "**Table C3:** Analysis of runs where the entire network is trained.\n",
        "\n",
        "|Experiment|Test Loss|Accuracy|Overfitting Metric|\n",
        "|:---|:---:|:---:|:---:|\n",
        "|CAC_EfficientNetB3_PT5|0.615|81.26|0.125|\n",
        "|CBC_EfficientNetB4_PT5|0.601|82.03|0.195|\n",
        "|CCC_EfficientNetB5_PT5|0.576|82.11|0.208|\n",
        "|CDC_EfficientNetB6_PT5|0.605|81.88|0.203|\n",
        "<br>\n",
        "\n",
        "![EfficientNet-B4 classifier only training loss plots](https://media.githubusercontent.com/media/blazingcayenne/deep_learning_with_pytorch_project2/main/images/CBA_EfficientNetB4_PT0.png?raw=true)<br>\n",
        "**Figure C1:** The loss plots of training only the classifier of a pretrained EfficientNet-B4 model.<br>\n",
        "\n",
        "![EfficientNet-B4 entire network training loss plots](https://media.githubusercontent.com/media/blazingcayenne/deep_learning_with_pytorch_project2/main/images/CBC_EfficientNetB4_PT5.png?raw=true)<br>\n",
        "**Figure C2:** The loss plots of training the entire network of a pretrained EfficientNet-B4 model.<br>\n",
        "\n",
        "![EfficientNet-B6 entire network training loss plots](https://media.githubusercontent.com/media/blazingcayenne/deep_learning_with_pytorch_project2/main/images/CDC_EfficientNetB6_PT5.png?raw=true)<br>\n",
        "**Figure C3:** The loss plots of training the entire network of a pretrained EfficientNet-B6 model.\n",
        "\n",
        "### Conclusions\n",
        "\n",
        "Training only the classifier of the EfficientNet models has a higher bias (lower accuracy) than training part of all of the convolutional base. Surprisingly, training the entire convolutional base yields slightly better accuracy than training the last 40% of the convolutional base. Regarding model complexity, the EfficientNet-B4 model seems appropriately sized."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVC8J7OXpuwv"
      },
      "source": [
        " class ExpC__(ModelExperiment):\n",
        "    def __init__(\n",
        "        self,\n",
        "        set_id: str, # expects a single uppercase letter\n",
        "        exp_id: str, # expects a single uppercase letter\n",
        "        pretrained: bool,\n",
        "        tuning_level: int,\n",
        "        model_type: TorchVisionModel,\n",
        "        model_abbr: str\n",
        "    ):\n",
        "        image_size = get_image_size(model_type)\n",
        "        batch_size = get_batch_size(model_type)\n",
        "        tuning_suffix = get_tuning_suffix(pretrained, tuning_level)\n",
        "\n",
        "        self.__pretrained = pretrained\n",
        "        self.__tuning_level = tuning_level\n",
        "        self.__model_type = model_type\n",
        "        self.__model_abbr = model_abbr + tuning_suffix\n",
        "\n",
        "        super().__init__(\n",
        "            abbr = 'C' + set_id + exp_id,\n",
        "            transform_resize = int(image_size * 16 / 14),\n",
        "            transform_crop_size = image_size,\n",
        "            data_loader_num_workers = 12,\n",
        "            data_loader_batch_size = batch_size,\n",
        "            optimizer = Optimizer.ADAM,\n",
        "            optimizer_learning_rate = 1.0E-04,\n",
        "            scheduler = LrScheduler.MULTI_STEP,\n",
        "            scheduler_gamma = math.sqrt(0.1),\n",
        "            scheduler_milestones = [10, 20, 30],\n",
        "            trainer_training_epochs = 35\n",
        "        )\n",
        "\n",
        "    def _get_model(self):\n",
        "        return self.__model_type(self.__pretrained, self.__tuning_level), self.__model_abbr\n",
        "\n",
        "def conduct_group_C():\n",
        "    set_id = 'A'\n",
        "    for model_type in [EfficientNetB3, EfficientNetB4, EfficientNetB5, EfficientNetB6]:\n",
        "        model_abbr = model_type.__name__\n",
        "        conduct(ExpC__(set_id, 'A', True, 0, model_type, model_abbr))\n",
        "        conduct(ExpC__(set_id, 'B', True, 2, model_type, model_abbr))\n",
        "        conduct(ExpC__(set_id, 'C', True, 5, model_type, model_abbr))\n",
        "        set_id = chr(ord(set_id[0]) + 1)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "source": [
        "## Experiment Group D: Increased Regularization and Dataset Imbalance Approaches\n",
        "\n",
        "### Introduction\n",
        "\n",
        "This group of experiments explores the impact of increased regularization and dataset imbalance approaches. Because this project explores pretrained models, regularization options are limited. Gathering more data is not an option due to time constraints and lack of familarity with Kenyan food. Feasible regularization options include additional data augmentation and L2 / Ridge Regression.\n",
        "\n",
        "This group has four sets.\n",
        "\n",
        "* Set A explores increased data augmentation.\n",
        "* Set B explores increased L2 regularization.\n",
        "* Set C explores data imbalance approaches.\n",
        "* Set D combines the \"best\" of the above.\n",
        "\n",
        "Data augmentation was increased by adding shear, random color erasing, and Gaussian noise. Figures D1, D2, and D3 depict nine samples of chapati with no data augmentation, default data augmentation, and enhanced data augmentation respecitivity.\n",
        "\n",
        "![3x3 grid of chapati with no data augmentations](https://media.githubusercontent.com/media/blazingcayenne/deep_learning_with_pytorch_project2/main/images/chapati01_.jpg?raw=true)<br>\n",
        "**Figure D1:** Nine samples of chapati with no data augmentation.\n",
        "\n",
        "![3x3 grid of chapati with default data augmentations](https://media.githubusercontent.com/media/blazingcayenne/deep_learning_with_pytorch_project2/main/images/chapati01_aug.jpg?raw=true)<br>\n",
        "**Figure D2:** Nine samples of chapati with default data augmentation.\n",
        "\n",
        "![3x3 grid of chapati with enhanced data augmentations](https://media.githubusercontent.com/media/blazingcayenne/deep_learning_with_pytorch_project2/main/images/chapati01_enh.jpg?raw=true)<br>\n",
        "**Figure D3:** Nine samples of chapati with enhanced data augmentation.\n",
        "\n",
        "### Results\n",
        "\n",
        "Table D1 enumerates the results of the Group D experiments. Both the default and enhanced data augmentation reduced overfitting and increased the model's accuracy. The enhanced data augmentation reduced overfitting better than the default data augmentation at the expense of accuracy. L2 regularization likewise reduced overfitting at the expense of accuracy. Igoring dataset imbalance or using a weighted loss function slightly reduced accuracy. Combining a small increase in L2 regularization along with enhanced data augmentation slighly decreased accuracy.\n",
        "\n",
        "**Table D1:** Analysis of Group D Experiments.\n",
        "\n",
        "|Experiment|Test Loss|Accuracy|Overfitting Metric|\n",
        "|:---|:---:|:---:|:---:|\n",
        "|DAA_EfficientNetB4_DA_NON|0.722|77.37|1.238|\n",
        "|DAB_EfficientNetB4_DA_DEF|0.551|82.87|0.076|\n",
        "|DAC_EfficientNetB4_DA_ENH|0.584|80.50|0.056|\n",
        "|DBA_EfficientNetB4_WD_E-3|0.551|81.80|0.080|\n",
        "|DBB_EfficientNetB4_WD-E-2|0.622|79.74|0.042|\n",
        "|DBC_EfficientNetB4_WD_E-1|1.060|68.12|0.006|\n",
        "|DBD_EfficientNetB4_WD_E-0|2.005|36.09|0.004|\n",
        "|DCA_EfficientNetB4_DI_NON|0.528|81.88|0.068|\n",
        "|DCB_EfficientNetB4_DI_WLF|0.556|81.19|0.065|\n",
        "|DDA_EfficientNetB4_AS_DEF|0.551|81.80|0.059|\n",
        "|DDB_EfficientNetB4_AS_ENH|0.575|81.80|0.042|\n",
        "\n",
        "### Conclusions\n",
        "\n",
        "None of the experiments in this group improved the model's accuracy. Surprisingly, increasing regularization had a small detrimental impact on accuracy."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_aug_enhanced = {\n",
        "    \"data_aug_affine_shear\": (-20., 20.),\n",
        "    \"data_aug_erasing_random\": True,\n",
        "    \"data_aug_noise_prob\": 0.5,\n",
        "    \"data_aug_noise_std\": 0.1\n",
        "}\n",
        "\n",
        "\n",
        "class DataAugMode(Enum):\n",
        "    NONE = auto()\n",
        "    DEFAULT = auto()\n",
        "    ENHANCED = auto()\n",
        "\n",
        "\n",
        "class ExpDV_(VisualExperiment):\n",
        "    def __init__(\n",
        "        self,\n",
        "        exp_id: str, # expects a single uppercase letter\n",
        "        aug_mode: DataAugMode = DataAugMode.DEFAULT\n",
        "    ):\n",
        "        config_overrides = {}\n",
        "        if aug_mode == DataAugMode.NONE:\n",
        "            suffix = \"aug\"\n",
        "            data_aug = False\n",
        "        elif aug_mode == DataAugMode.DEFAULT:\n",
        "            suffix = \"aug\"\n",
        "            data_aug = True\n",
        "        elif aug_mode == DataAugMode.ENHANCED:\n",
        "            suffix = \"enh\"\n",
        "            data_aug = True\n",
        "            config_overrides.update(data_aug_enhanced)\n",
        "\n",
        "        super().__init__(\n",
        "            abbr = 'DV' + exp_id,\n",
        "            log_originals = not data_aug,\n",
        "            log_augmentations = data_aug,\n",
        "            log_augmentations_suffix = suffix,\n",
        "            **config_overrides\n",
        "        )\n",
        "\n",
        "\n",
        "class ExpD__(ModelExperiment):\n",
        "    def __init__(\n",
        "        self,\n",
        "        set_id: str, # expects a single uppercase letter\n",
        "        exp_id: str, # expects a single uppercase letter\n",
        "        model_suffix: str,\n",
        "        aug_mode: DataAugMode = DataAugMode.DEFAULT,\n",
        "        di_approach: DIApproach = DIApproach.WEIGHTED_RANDOM_SAMPLER,\n",
        "        weight_decay: float = 0.0001,\n",
        "        alt_scheduler: bool = False\n",
        "    ):\n",
        "        pretrained = True\n",
        "        tuning_level = 5\n",
        "        model_type = EfficientNetB4\n",
        "        model_abbr = model_type.__name__\n",
        "        image_size = get_image_size(model_type)\n",
        "        batch_size = get_batch_size(model_type)\n",
        "\n",
        "        self.__pretrained = pretrained\n",
        "        self.__tuning_level = tuning_level\n",
        "        self.__model_type = model_type\n",
        "        self.__model_abbr = model_abbr + model_suffix\n",
        "\n",
        "        config_overrides = {\n",
        "            \"data_loader_num_workers\": 12,\n",
        "            \"data_loader_batch_size\": batch_size,\n",
        "            \"optimizer\": Optimizer.ADAM,\n",
        "            \"optimizer_learning_rate\": 1.0E-04 * math.sqrt(0.1),\n",
        "            \"optimizer_weight_decay\": weight_decay,\n",
        "        }\n",
        "\n",
        "        if not alt_scheduler:\n",
        "            scheduler = LrScheduler.STEP\n",
        "            config_overrides.update({\n",
        "                \"scheduler_step_size\": 200, # disable scheduler\n",
        "                \"trainer_training_epochs\": 20\n",
        "            })\n",
        "        else:\n",
        "            scheduler = LrScheduler.MULTI_STEP\n",
        "            config_overrides.update({\n",
        "                \"scheduler_gamma\": math.sqrt(0.1),\n",
        "                \"scheduler_milestones\": (20, 30, 40, 50),\n",
        "                \"trainer_training_epochs\": 60\n",
        "            })\n",
        "\n",
        "        if aug_mode == DataAugMode.NONE:\n",
        "            disable_data_aug = True\n",
        "        elif aug_mode == DataAugMode.DEFAULT:\n",
        "            disable_data_aug = False\n",
        "        elif aug_mode == DataAugMode.ENHANCED:\n",
        "            disable_data_aug = False\n",
        "            config_overrides.update(data_aug_enhanced)\n",
        "\n",
        "        super().__init__(\n",
        "            abbr = 'D' + set_id + exp_id,\n",
        "            scheduler = scheduler,\n",
        "            transform_resize = int(image_size * 16 / 14),\n",
        "            transform_crop_size = image_size,\n",
        "            disable_data_aug = disable_data_aug,\n",
        "            di_approach = di_approach,\n",
        "            **config_overrides\n",
        "        )\n",
        "\n",
        "    def _get_model(self):\n",
        "        return self.__model_type(self.__pretrained, self.__tuning_level), self.__model_abbr\n",
        "\n",
        "def conduct_group_D():\n",
        "    conduct(ExpD__('A', 'A', \"_DA_NON\", aug_mode=DataAugMode.NONE))\n",
        "    conduct(ExpD__('A', 'B', \"_DA_DEF\", aug_mode=DataAugMode.DEFAULT))\n",
        "    conduct(ExpD__('A', 'C', \"_DA_ENH\", aug_mode=DataAugMode.ENHANCED))\n",
        "    conduct(ExpD__('B', 'A', \"_WD_E-3\", weight_decay=0.001))\n",
        "    conduct(ExpD__('B', 'B', \"_WD-E-2\", weight_decay=0.01))\n",
        "    conduct(ExpD__('B', 'C', \"_WD_E-1\", weight_decay=0.1))\n",
        "    conduct(ExpD__('B', 'D', \"_WD_E-0\", weight_decay=1.))\n",
        "    conduct(ExpD__('C', 'A', \"_DI_NON\", di_approach=DIApproach.NONE))\n",
        "    conduct(ExpD__('C', 'B', \"_DI_WLF\", di_approach=DIApproach.WEIGHTED_LOSS_FUNCTION))\n",
        "    conduct(ExpD__('D', 'A', \"_AS_DEF\", aug_mode=DataAugMode.DEFAULT, weight_decay=0.001, alt_scheduler=True))\n",
        "    conduct(ExpD__('D', 'B', \"_AS_ENH\", aug_mode=DataAugMode.ENHANCED, weight_decay=0.001, alt_scheduler=True))\n",
        "    #conduct(ExpDV_('A', aug_mode=DataAugMode.NONE))\n",
        "    #conduct(ExpDV_('B', aug_mode=DataAugMode.DEFAULT))\n",
        "    #conduct(ExpDV_('C', aug_mode=DataAugMode.ENHANCED))    "
      ]
    },
    {
      "source": [
        "## Experiment Group E: Two Stage Model\n",
        "\n",
        "### Introduction\n",
        "\n",
        "The confusion matrix from the most accurate models was re-ordered (in another notebook) using a greedy method so that the largest number of misclassifications were closest to the diagnoal. From this analysis, it was determined that a modest accuracy improvement was possible if models could more accurately classify images belonging to two or three classes rather than 13. The experiments in this group explore whether any of the theoretical improvements can be realized.\n",
        "\n",
        "A two stage model with three second stages was implemented. The stage definitions are as follows. The numbers in backets are labels and the numbers after the colon are the number of samples in each class.\n",
        "\n",
        "```\n",
        "Stage1:\n",
        "  [00] githeri: 479\n",
        "  [01] mandazi: 620\n",
        "  [02] masalachips: 438\n",
        "  [03] matoke: 483\n",
        "  [04] mukimo: 212\n",
        "  [05] pilau: 329\n",
        "  [06] group1: 1494\n",
        "  [07] group2: 1451\n",
        "  [08] group3: 1030\n",
        "\n",
        "Stage2a:\n",
        "  [00] bhaji: 632\n",
        "  [01] chapati: 862\n",
        "\n",
        "Stage2b:\n",
        "  [00] kachumbari: 494\n",
        "  [01] kukuchoma: 173\n",
        "  [02] nyamachoma: 784\n",
        "\n",
        "Stage2c:\n",
        "  [00] sukumawiki: 402\n",
        "  [01] ugali: 628\n",
        "```\n",
        "\n",
        "### Results\n",
        "\n",
        "Table E1 enumerates the results of each stage as well as entire two-stage model. The first stage's accuracy was better than the best single stage model. Likewise, Stage2a accurately classified bhaji and chapati samples. Stage2b and Stage2c's accuracies were slighly lower than the best single stage model. Disappointingly, the two-stage model's accuracy was slightly lower than the best single stage model.\n",
        "\n",
        "**Table E1:** The analysis of the two stage model.\n",
        "|Experiment|Test Loss|Accuracy|Overfitting Metric|\n",
        "|:---|:---:|:---:|:---:|\n",
        "|EAA_EfficientNetB4_Stage1|0.354|89.14|0.027|\n",
        "|EAB_EfficientNetB4_Stage2a|0.170|94.00|0.019|\n",
        "|EAC_EfficientNetB4_Stage2b|0.520|80.00|0.034|\n",
        "|EAD_EfficientNetB4_Stage2c|0.543|80.56|0.051|\n",
        "|EAE_EfficientNetB4_TwoStage||81.73||\n",
        "\n",
        "The confusion matrices of each stage are shown below in non-graphical form.\n",
        "\n",
        "```\n",
        "Two Stage - Accuracy: 81.72782874617737\n",
        "\n",
        "[[ 95   7   0   2   0   4   1   2   2   1   5   1   6]\n",
        " [  8 157   0   1   0   0   0   1   0   0   1   1   3]\n",
        " [  1   2  91   0   0   0   1   0   0   0   0   0   1]\n",
        " [  2   1   2  68   3   2   0   0   0   9   4   2   6]\n",
        " [  0   0   0   1  20   0   1   1   0  12   0   0   0]\n",
        " [  1   0   0   0   1 121   0   0   0   0   0   0   1]\n",
        " [  3   0   0   1   1   0  82   0   0   0   0   0   1]\n",
        " [  6   2   1   0   0   1   1  79   1   1   1   1   3]\n",
        " [  0   0   1   0   0   1   2   1  34   0   0   1   2]\n",
        " [  2   5   1  11  10   2   1   0   0 117   4   0   4]\n",
        " [  0   0   0   0   0   0   0   1   0   0  63   2   0]\n",
        " [  0   3   4   3   0   1   0   3   0   1   1  43  21]\n",
        " [  0   1   0   5   0   0   0   3   0   3   0  15  99]]\n",
        "\n",
        "[126 172  96  99  35 124  88  97  42 157  66  80 126]\n",
        "\n",
        "1069 / 1308 = 0.8172782874617737\n",
        "\n",
        "|Experiment|Test Loss|Accuracy|Overfitting Metric|\n",
        "|:---|:---:|:---:|:---:|\n",
        "|EAA_EfficientNetB4_Stage1|0.354|89.14|0.027|\n",
        "|EAB_EfficientNetB4_Stage2a|0.170|94.00|0.019|\n",
        "|EAC_EfficientNetB4_Stage2b|0.520|80.00|0.034|\n",
        "|EAD_EfficientNetB4_Stage2c|0.543|80.56|0.051|\n",
        "\n",
        "stage_results[0]\n",
        "\n",
        "[[ 91   0   1   0   0   0   3   0   1]\n",
        " [  0 121   0   0   0   0   1   1   1]\n",
        " [  0   0  82   0   0   0   3   2   1]\n",
        " [  1   1   1  79   1   1   8   1   4]\n",
        " [  1   1   2   1  34   0   0   0   3]\n",
        " [  0   0   0   1   0  63   0   0   2]\n",
        " [  0   4   1   3   2   6 267   4  11]\n",
        " [  3   4   2   1   0   8  10 251  12]\n",
        " [  4   1   0   6   0   1   4  12 178]]\n",
        "\n",
        "1166 / 1308 = 0.8914373088685015\n",
        "\n",
        "stage_results[1]\n",
        "\n",
        "[[ 95   7   0]\n",
        " [  8 157   0]\n",
        " [ 15  14   0]]\n",
        "\n",
        "252 / 296 = 0.8513513513513513\n",
        "\n",
        "stage_results[2]\n",
        "\n",
        "[[ 68   3   9   0]\n",
        " [  1  20  12   0]\n",
        " [ 11  10 117   0]\n",
        " [ 12   2   6   0]]\n",
        "\n",
        "205 / 271 = 0.7564575645756457\n",
        "\n",
        "stage_results[3]\n",
        "\n",
        "[[43 21  0]\n",
        " [15 99  0]\n",
        " [ 8 27  0]]\n",
        "\n",
        "142 / 213 = 0.6666666666666666\n",
        "```\n",
        "\n",
        "### Conclusions\n",
        "\n",
        "The two-stage model's accuracy was slightly lower than the best single stage model's accuracy."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage1:\n  [00] githeri: 479\n  [01] mandazi: 620\n  [02] masalachips: 438\n  [03] matoke: 483\n  [04] mukimo: 212\n  [05] pilau: 329\n  [06] group1: 1494\n  [07] group2: 1451\n  [08] group3: 1030\n\nStage2a:\n  [00] bhaji: 632\n  [01] chapati: 862\n\nStage2b:\n  [00] kachumbari: 494\n  [01] kukuchoma: 173\n  [02] nyamachoma: 784\n\nStage2c:\n  [00] sukumawiki: 402\n  [01] ugali: 628\n"
          ]
        }
      ],
      "source": [
        "                                                            # original\n",
        "                                                            # label(s)\n",
        "stage_info = [                                              # --------\n",
        "    'githeri',                                              #  2\n",
        "    'mandazi',                                              #  5\n",
        "    'masalachips',                                          #  6\n",
        "    'matoke',                                               #  7\n",
        "    'mukimo',                                               #  8\n",
        "    'pilau',                                                #  10\n",
        "    ('group1', ['bhaji', 'chapati']),                       #  0, 1\n",
        "    ('group2', ['kachumbari', 'kukuchoma', 'nyamachoma']),  #  3, 4, 9\n",
        "    ('group3', ['sukumawiki', 'ugali'])                     #  11, 12\n",
        "]\n",
        "\n",
        "def print_stages(stages):\n",
        "    if isinstance(stages, KF13TrainingData):\n",
        "        for index, (cname, ccount) in enumerate(zip(stages.class_names, stages.class_counts)):\n",
        "            print(f\"[{index:02d}] {cname}: {ccount}\")\n",
        "\n",
        "    else:\n",
        "        first = True\n",
        "        for stage in stages:\n",
        "            if not first: print()\n",
        "            print(f\"{stage[0]}:\")\n",
        "            for index, (cname, ccount) in enumerate(zip(stage[1].class_names, stage[1].class_counts)):\n",
        "                print(f\"  [{index:02d}] {cname}: {ccount}\")\n",
        "            first = False\n",
        "\n",
        "classifier_stages = datastore.get_two_stage_training_data(stage_info)\n",
        "print_stages(classifier_stages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TwoStageModel:\n",
        "    def __init__(\n",
        "        self, \n",
        "        stage_exp: List[ModelExperiment],\n",
        "        stage_info: List[Union[str, Tuple[str, List[str]]]],\n",
        "        class_names: List[str],\n",
        "        fnames: List[str],\n",
        "        labels: List[int] = None\n",
        "    ):\n",
        "        self.__stage_exp = stage_exp\n",
        "        self.__stage_info = stage_info\n",
        "        self.__class_names = class_names\n",
        "        self.__fnames = fnames\n",
        "        self.__labels = labels\n",
        "\n",
        "    def inference(self) -> Dict[str, List[int]]:\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        stage_results = []\n",
        "        model_results = {}\n",
        "        stage_inf = [self.__stage_info]\n",
        "        stage_idx = [list(range(len(self.__fnames)))]\n",
        "        stages = datastore.create_stages(self.__stage_info)\n",
        "\n",
        "        for idx in range(len(self.__stage_exp)):\n",
        "            exp = self.__stage_exp[idx]\n",
        "            model = exp.trained_model\n",
        "            config = exp.config\n",
        "            device = exp.device\n",
        "            mapping = stages[idx][1]\n",
        "\n",
        "            dataset = KF13IndexedDataset(\n",
        "                datastore.image_root,\n",
        "                stage_idx[idx],\n",
        "                self.__fnames,\n",
        "                [mapping[label] for label in self.__labels],\n",
        "                config.dataset.test_transforms\n",
        "            )\n",
        "\n",
        "            data_loader = DataLoader(\n",
        "                dataset = dataset,\n",
        "                batch_size = config.data_loader.batch_size,\n",
        "                num_workers = config.data_loader.num_workers,\n",
        "                shuffle = False\n",
        "            )\n",
        "\n",
        "            results, targets, preds = TwoStageModel.__predict_two_stage_valid_data(\n",
        "                model, \n",
        "                data_loader, \n",
        "                device, \n",
        "                len(exp.class_names)\n",
        "            )\n",
        "\n",
        "            stage_results.append((targets, preds))\n",
        "            for stage, result in zip(stage_inf[idx], results):\n",
        "                if isinstance(stage, str):\n",
        "                    model_results[stage] = result\n",
        "                else:\n",
        "                    stage_inf.append(stage[1])\n",
        "                    stage_idx.append(result)\n",
        "\n",
        "            del exp\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        return model_results, stage_results\n",
        "        \n",
        "    def get_targets_and_preds(\n",
        "        self, \n",
        "        results: Dict[str, List[int]], \n",
        "        preserve_order: bool = False\n",
        "    ) -> Tuple[List[int], List[int]]:\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        class_dict = datastore.class_dict\n",
        "        if not preserve_order:\n",
        "            preds = []\n",
        "            targets = []\n",
        "            for (class_name, indices) in results.items():\n",
        "                preds.extend([class_dict[class_name]] * len(indices))\n",
        "                targets.extend([self.__labels[index] for index in indices])\n",
        "        else:\n",
        "            count = len(self.__labels)\n",
        "            preds = [-1] * count\n",
        "            targets = [-1] * count\n",
        "            for (class_name, indices) in results.items():\n",
        "                pred = class_dict[class_name]\n",
        "                for index in indices:\n",
        "                    preds[index] = pred\n",
        "                    targets[index] = self.__labels[index]\n",
        "\n",
        "        return targets, preds\n",
        "\n",
        "    @classmethod\n",
        "    def compute_accuracy(cls, targets:List[int], preds:List[int]) -> float:\n",
        "        count = 0\n",
        "        for (pred, target) in zip(preds, targets):    \n",
        "            if pred == target:\n",
        "                count += 1\n",
        "        return float(count) / len(preds)\n",
        "\n",
        "    @classmethod\n",
        "    def __predict_two_stage_valid_data(cls, model, dataloader, device, num_classes):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        model.to(device)  # send model to cpu or cuda\n",
        "        model.eval()      # set model to evaluation mode\n",
        "        preds = []\n",
        "        targets = []\n",
        "        results = [[] for _ in range(num_classes)]\n",
        "        for idx, data, target in dataloader:\n",
        "            pred, _ = predict_batch(model, data.to(device), max_prob=True)\n",
        "            preds.extend(pred.tolist())\n",
        "            targets.extend(target.tolist())\n",
        "            for (_idx, _pred) in zip(idx.numpy(), pred):\n",
        "                results[_pred].append(_idx)       \n",
        "        return results, targets, preds"
      ]
    },
    {
      "source": [
        "### Validate TwoStageModel Class\n",
        "\n",
        "To validate the two stage model class, the predictions/targets generated by a ModelExperiment derived class will be compared to the predictions/targets generated by the TwoStageModel class (albeit using the aforementioned derived class' model as the only stage).\n",
        "\n",
        "The predictions and targets were identical. Both tests produced the following output.\n",
        "\n",
        "```\n",
        "One/Two Stage - Accuracy: 82.87461773700305\n",
        "\n",
        "[[ 94   6   1   2   2   4   2   2   2   2   4   2   3]\n",
        " [  7 151   0   2   1   2   0   1   0   0   3   2   3]\n",
        " [  0   1  95   0   0   0   0   0   0   0   0   0   0]\n",
        " [  2   0   2  79   3   1   0   1   0   6   2   2   1]\n",
        " [  0   0   0   3  21   0   0   1   0  10   0   0   0]\n",
        " [  2   0   0   0   1 120   0   0   0   0   0   0   1]\n",
        " [  3   0   0   2   1   0  80   1   0   0   0   0   1]\n",
        " [  5   1   2   1   0   0   0  80   0   3   2   0   3]\n",
        " [  0   0   1   1   0   0   1   1  36   0   0   1   1]\n",
        " [  0   7   1  11  10   0   3   1   0 115   3   3   3]\n",
        " [  1   0   0   1   0   0   0   1   0   0  62   1   0]\n",
        " [  0   1   3   1   0   0   0   1   0   2   1  60  11]\n",
        " [  0   1   1   6   1   0   0   3   0   4   1  18  91]]\n",
        "\n",
        "[126 172  96  99  35 124  88  97  42 157  66  80 126]\n",
        "\n",
        "1084 / 1308 = 0.8287461773700305\n",
        "```"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate_two_two_stage_model():\n",
        "    exp1a = ExpD__('A', 'B', \"_DA_DEF\", aug_mode=DataAugMode.DEFAULT)\n",
        "    targets1a, preds1a = predict_valid_data(\n",
        "        exp1a.trained_model, \n",
        "        exp1a.valid_loader, \n",
        "        exp1a.device\n",
        "    )\n",
        "    preds1a = preds1a.tolist()\n",
        "    targets1a = targets1a.tolist()\n",
        "\n",
        "    accuracy1a = TwoStageModel.compute_accuracy(targets1a, preds1a)\n",
        "    cm1a = confusion_matrix(targets1a, preds1a)\n",
        "    trace1a = np.trace(cm1a)\n",
        "    total1a = np.sum(cm1a)\n",
        "\n",
        "    print(f\"One Stage - Accuracy: {100 * accuracy1a}\")\n",
        "    print()\n",
        "    print(cm1a)\n",
        "    print()\n",
        "    print(np.sum(cm1a, axis=1))\n",
        "    print()\n",
        "    print(f\"{trace1a} / {total1a} = {trace1a / total1a}\")\n",
        "\n",
        "    data = datastore.get_training_data()\n",
        "    class_names = data.class_names\n",
        "\n",
        "    model1b = TwoStageModel(\n",
        "        stage_exp = [exp1a],\n",
        "        stage_info = class_names,\n",
        "        class_names = data.class_names,\n",
        "        fnames = data.valid_fnames,\n",
        "        labels = data.valid_labels\n",
        "    )\n",
        "\n",
        "    model_results1b, stage_results1b = model1b.inference()\n",
        "    targets1b, preds1b = model1b.get_targets_and_preds(model_results1b, preserve_order=True)\n",
        "\n",
        "    accuracy1b = TwoStageModel.compute_accuracy(targets1b, preds1b)\n",
        "    cm1b = confusion_matrix(targets1b, preds1b)\n",
        "    trace1b = np.trace(cm1b)\n",
        "    total1b = np.sum(cm1b)\n",
        "\n",
        "    print(f\"Two Stage - Accuracy: {100 * accuracy1b}\")\n",
        "    print()\n",
        "    print(cm1b)\n",
        "    print()\n",
        "    print(np.sum(cm1b, axis=1))\n",
        "    print()\n",
        "    print(f\"{trace1b} / {total1b} = {trace1b / total1b}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ExpEA_(ModelExperiment):\n",
        "    def __init__(self, stage:str):\n",
        "\n",
        "        pretrained = True\n",
        "        tuning_level = 5\n",
        "        model_type = EfficientNetB4\n",
        "        image_size = get_image_size(model_type)\n",
        "        batch_size = get_batch_size(model_type)\n",
        "\n",
        "        self.__pretrained = pretrained\n",
        "        self.__tuning_level = tuning_level\n",
        "        self.__model_type = model_type\n",
        "\n",
        "        if stage.lower() == \"stage1\":\n",
        "            exp_id = 'A'\n",
        "            self.__split_index = 0\n",
        "        elif stage.lower() == \"stage2a\":\n",
        "            exp_id = 'B'\n",
        "            self.__split_index = 1\n",
        "        elif stage.lower() == \"stage2b\":\n",
        "            exp_id = 'C'\n",
        "            self.__split_index = 2\n",
        "        elif stage.lower() == \"stage2c\":\n",
        "            exp_id = 'D'\n",
        "            self.__split_index = 3\n",
        "\n",
        "        super().__init__(\n",
        "            abbr = \"EA\" + exp_id,\n",
        "            transform_resize = int(image_size * 16 / 14),\n",
        "            transform_crop_size = image_size,\n",
        "            data_loader_num_workers = 12,\n",
        "            data_loader_batch_size = batch_size,\n",
        "            optimizer = Optimizer.ADAM,\n",
        "            optimizer_learning_rate = 1.0E-04 * math.sqrt(0.1),\n",
        "            scheduler_step_size = 10,\n",
        "            scheduler_gamma = math.sqrt(0.1),\n",
        "            trainer_training_epochs = 100,\n",
        "            trainer_stop_loss_epochs = 15\n",
        "        )\n",
        "\n",
        "    def _get_data(self) -> KF13TrainingData:\n",
        "        stage = classifier_stages[self.__split_index]\n",
        "        self.__model_abbr = self.__model_type.__name__ + '_' + stage[0]\n",
        "        return stage[1]\n",
        "        \n",
        "    def _get_model(self):\n",
        "        return self.__model_type(self.__pretrained, self.__tuning_level), self.__model_abbr\n",
        "\n",
        "\n",
        "def conduct_group_E():\n",
        "    conduct(ExpEA_(\"Stage1\"))\n",
        "    conduct(ExpEA_(\"Stage2A\"))\n",
        "    conduct(ExpEA_(\"Stage2B\"))\n",
        "    conduct(ExpEA_(\"Stage2C\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def conduct_two_stage_model_experiment():\n",
        "    data = datastore.get_training_data()\n",
        "    class_names = data.class_names\n",
        "\n",
        "    model2 = TwoStageModel(\n",
        "        stage_exp = [ExpEA_(\"Stage1\"), ExpEA_(\"Stage2a\"), ExpEA_(\"Stage2b\"), ExpEA_(\"Stage2c\")],\n",
        "        stage_info = stage_info,\n",
        "        class_names = data.class_names,\n",
        "        fnames = data.valid_fnames,\n",
        "        labels = data.valid_labels\n",
        "    )\n",
        "\n",
        "    model_results2, stage_results2 = model2.inference()\n",
        "    targets2, preds2 = model2.get_targets_and_preds(model_results2, preserve_order=True)\n",
        "\n",
        "    accuracy2 = TwoStageModel.compute_accuracy(targets2, preds2)\n",
        "    cm2 = confusion_matrix(targets2, preds2)\n",
        "    trace2 = np.trace(cm2)\n",
        "    total2 = np.sum(cm2)\n",
        "\n",
        "    print(f\"Two Stage - Accuracy: {100 * accuracy2}\")\n",
        "    print()\n",
        "    print(cm2)\n",
        "    print()\n",
        "    print(np.sum(cm2, axis=1))\n",
        "    print()\n",
        "    print(f\"{trace2} / {total2} = {trace2 / total2}\")\n",
        "    for idx, stage_result in enumerate(stage_results2):\n",
        "        stage_preds = stage_result[1]\n",
        "        stage_targets = stage_result[0]\n",
        "        stage_cm = confusion_matrix(stage_targets, stage_preds)\n",
        "        stage_trace = np.trace(stage_cm)\n",
        "        stage_total = np.sum(stage_cm)\n",
        "        print()\n",
        "        print(f\"stage_results[{idx}]\")\n",
        "        print()\n",
        "        print(stage_cm)\n",
        "        print()\n",
        "        print(f\"{stage_trace} / {stage_total} = {stage_trace / stage_total}\")\n",
        "\n",
        "\n",
        "#conduct_two_stage_model_experiment()"
      ]
    },
    {
      "source": [
        "## Experiment Group F: Project 1 Comparison\n",
        "\n",
        "### Introduction\n",
        "\n",
        "This group of experiments trains Project 1's model from scratch in order to compare its performance versus models developed by researchers who know a lot more than me.\n",
        "\n",
        "### Results\n",
        "\n",
        "After 160 epochs, the model achieved an accuracy of approximately 35%. When the experiment was terminated by Google Colabs, the test loss was still decreasing and the accuracy was increasing. It would be an interesting experiment to explore whether [Cyclical Learning Rates for Training Neural Networks](https://www.arxiv-vanity.com/papers/1506.01186/) significantly decreases training time."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ExpFAA(ModelExperiment):\n",
        "    def __init__(self):\n",
        "        pretrained = True\n",
        "        tuning_level = 5\n",
        "        model_type = EfficientNetB4\n",
        "        model_abbr = model_type.__name__\n",
        "        image_size = get_image_size(model_type)\n",
        "        batch_size = get_batch_size(model_type)\n",
        "\n",
        "        self.__pretrained = pretrained\n",
        "        self.__tuning_level = tuning_level\n",
        "        self.__model_type = model_type\n",
        "        self.__model_abbr = model_abbr + \"_DA_PR1\"\n",
        "\n",
        "        super().__init__(\n",
        "            transform_resize = int(image_size * 16 / 14),\n",
        "            transform_crop_size = image_size,\n",
        "            data_aug_color_enabled = True,\n",
        "            data_aug_color_brightness = (0.75, 1.25),\n",
        "            data_aug_color_contrast = (0.75, 1.25),\n",
        "            data_aug_color_saturation = (0.75, 1.25),\n",
        "            data_aug_color_hue = (-0.25, 0.25),\n",
        "            data_aug_horz_flip_prob = 0.5,\n",
        "            data_aug_vert_flip_prob = 0.5,\n",
        "            data_aug_affine_enabled = True,\n",
        "            data_aug_affine_rotation = 45,\n",
        "            data_aug_affine_translate = (0.2, 0.2),\n",
        "            data_aug_affine_scale = (0.8, 1.2),\n",
        "            data_aug_affine_shear = (0.0, 0.0),\n",
        "            data_aug_erasing_prob = 0.5,\n",
        "            data_aug_erasing_scale = (0.02, 0.33),\n",
        "            data_aug_erasing_ratio = (0.3, 3.3),\n",
        "            data_loader_num_workers = 12,\n",
        "            data_loader_batch_size = batch_size,\n",
        "            optimizer = Optimizer.ADAM,\n",
        "            optimizer_learning_rate = 1.0E-04 * math.sqrt(0.1),\n",
        "            scheduler_step_size = 10,\n",
        "            scheduler_gamma = math.sqrt(0.1),\n",
        "            trainer_training_epochs = 50,\n",
        "            trainer_stop_loss_epochs = 15\n",
        "        )\n",
        "\n",
        "    def _get_model(self):\n",
        "        return self.__model_type(self.__pretrained, self.__tuning_level), self.__model_abbr\n",
        "\n",
        "\n",
        "class ExpFBA(ModelExperiment):\n",
        "    def __init__(self):\n",
        "        model_type = Project1Model\n",
        "        model_abbr = model_type.__name__\n",
        "        image_size = get_image_size(model_type)\n",
        "        batch_size = get_batch_size(model_type)\n",
        "\n",
        "        self.__model_type = model_type\n",
        "        self.__model_abbr = model_abbr\n",
        "\n",
        "        super().__init__(\n",
        "            di_approach = DIApproach.MY_SAMPLER,\n",
        "            transform_resize = int(image_size * 16 / 14),\n",
        "            transform_crop_size = image_size,\n",
        "            data_loader_num_workers = 12,\n",
        "            data_loader_batch_size = batch_size,\n",
        "            optimizer = Optimizer.ADAM,\n",
        "            optimizer_learning_rate = 1.0E-05,\n",
        "            scheduler = LrScheduler.REDUCE_ON_PLATEAU,\n",
        "            scheduler_gamma = math.sqrt(0.1),\n",
        "            scheduler_patience = 10,\n",
        "            scheduler_threshold = 0.001,\n",
        "            trainer_training_epochs = 500,\n",
        "            trainer_stop_loss_epochs = 25\n",
        "        )\n",
        "\n",
        "    def _get_model(self):\n",
        "        return self.__model_type(), self.__model_abbr\n",
        "\n",
        "\n",
        "def conduct_group_F():\n",
        "    conduct(ExpFAA())\n",
        "    conduct(ExpFBA())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "source": [
        "## Experiment Group G: Model Comparisons\n",
        "\n",
        "### Introduction\n",
        "\n",
        "This group of experiments 1) trains the convolutional base and classifier of the following pretrained models and 2) compares their accuracies against the validation data.\n",
        "\n",
        "* Set A\n",
        "  * ResNet-18 \n",
        "  * ResNet-34\n",
        "  * ResNet-50\n",
        "  * ResNet-101\n",
        "  * ResNet-152\n",
        "* Set B\n",
        "  * ResNeXt-50\n",
        "  * ResNeXt-101\n",
        "* Set C\n",
        "  * WideResNet-50\n",
        "  * WideResNet-101\n",
        "* Set D\n",
        "  * VGG11 w/ Batch Normalization\n",
        "  * VGG13 w/ Batch Normalization\n",
        "  * VGG16 w/ Batch Normalization\n",
        "  * VGG19 w/ Batch Normalization\n",
        "* Set E\n",
        "  * DenseNet-121\n",
        "  * DenseNet-169\n",
        "  * DenseNet-201\n",
        "  * DenseNet-161\n",
        "* Set F\n",
        "  * EfficientNet-B0\n",
        "  * EfficientNet-B1\n",
        "  * EfficientNet-B2\n",
        "  * EfficientNet-B3\n",
        "  * EfficientNet-B4\n",
        "  * EfficientNet-B5\n",
        "  * EfficientNet-B6\n",
        "  * EfficientNet-B7\n",
        "\n",
        "### Results\n",
        "\n",
        "The experiments were analyzed. Table G1 enumerates their results in descending order.\n",
        "\n",
        "**Table G1:** Experiment results sorted by accuracy in descending order.\n",
        "\n",
        "|Experiment|Test Loss|Accuracy|Overfitting Metric|\n",
        "|---|---|---|---|\n",
        "|GFF_EfficientNetB5_PT5|0.543|83.18|0.044|\n",
        "|GFE_EfficientNetB4_PT5|0.553|82.34|0.037|\n",
        "|GFG_EfficientNetB6_PT5|0.644|81.5|0.059|\n",
        "|GFH_EfficientNetB7_PT5|0.746|79.59|0.108|\n",
        "|GFD_EfficientNetB3_PT5|0.628|79.52|0.023|\n",
        "|GBA_ResNeXt50_PT5|0.691|78.74|0.102|\n",
        "|GED_DenseNet161_PT5|0.665|78.71|0.078|\n",
        "|GCA_WideResNet50_PT5|0.686|78.4|0.122|\n",
        "|GEC_DenseNet201_PT5|0.672|78.23|0.067|\n",
        "|GBB_ResNeXt101_PT5|0.731|77.51|0.475|\n",
        "|GAD_ResNet101_PT5|0.729|77.42|0.101|\n",
        "|GFC_EfficientNetB2_PT5|0.703|77.41|0.01|\n",
        "|GAC_ResNet50_PT5|0.728|77.02|0.053|\n",
        "|GEA_DenseNet121_PT5|0.717|76.88|0.019|\n",
        "|GCB_WideResNet101_PT5|0.744|76.56|0.196|\n",
        "|GDC_VGG16BN_PT5|0.787|76.53|0.048|\n",
        "|GAE_ResNet152_PT5|0.736|76.04|0.129|\n",
        "|GDD_VGG19BN_PT5|0.769|75.74|0.062|\n",
        "|GEB_DenseNet169_PT5|0.72|75.63|0.047|\n",
        "|GDB_VGG13BN_PT5|0.794|74.8|0.028|\n",
        "|GFB_EfficientNetB1_PT5|0.799|73.77|0.009|\n",
        "|GAB_ResNet34_PT5|0.784|73.57|0.033|\n",
        "|GDA_VGG11BN_PT5|0.814|73.15|0.023|\n",
        "|GAA_ResNet18_PT5|0.842|71.65|0.013|\n",
        "|GFA_EfficientNetB0_PT5|0.961|69.66|0.004|\n",
        "\n",
        "### Conclusions\n",
        "\n",
        "The EfficientNet-B5 model most accurately classified the KenyanFood13 validation data."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ExpG__(ModelExperiment):\n",
        "    def __init__(\n",
        "        self,\n",
        "        set_id: str, # expects a single uppercase letter\n",
        "        exp_id: str, # expects a single uppercase letter\n",
        "        model_type: TorchVisionModel\n",
        "    ):\n",
        "        pretrained = True\n",
        "        tuning_level = 5\n",
        "        model_abbr = model_type.__name__\n",
        "        image_size = get_image_size(model_type)\n",
        "        batch_size = get_batch_size(model_type)\n",
        "\n",
        "        self.__pretrained = pretrained\n",
        "        self.__tuning_level = tuning_level\n",
        "        self.__model_type = model_type\n",
        "        self.__model_abbr = model_abbr + \"_PT5\"\n",
        "\n",
        "        super().__init__(\n",
        "            abbr = 'G' + set_id + exp_id,\n",
        "            transform_resize = int(image_size * 16 / 14),\n",
        "            transform_crop_size = image_size,\n",
        "            data_loader_num_workers = 12,\n",
        "            data_loader_batch_size = batch_size,\n",
        "            optimizer = Optimizer.ADAM,\n",
        "            optimizer_learning_rate = 1.0E-04 * math.sqrt(0.1),\n",
        "            scheduler_step_size = 10,\n",
        "            scheduler_gamma = math.sqrt(0.1),\n",
        "            trainer_training_epochs = 30,\n",
        "            trainer_stop_loss_epochs = 15\n",
        "        )\n",
        "\n",
        "    def _get_model(self):\n",
        "        return self.__model_type(self.__pretrained, self.__tuning_level), self.__model_abbr\n",
        "\n",
        "\n",
        "def conduct_group_G():\n",
        "    exp_sets = [\n",
        "        ('A', [ResNet18, ResNet34, ResNet50, ResNet101, ResNet152]),\n",
        "        ('B', [ResNeXt50, ResNeXt101]),\n",
        "        ('C', [WideResNet50, WideResNet101]),\n",
        "        ('D', [VGG11BN, VGG13BN, VGG16BN, VGG19BN]),\n",
        "        ('E', [DenseNet121, DenseNet169, DenseNet201, DenseNet161]),\n",
        "        ('F', [EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3, \n",
        "               EfficientNetB4, EfficientNetB5, EfficientNetB6, EfficientNetB7])\n",
        "    ]\n",
        "\n",
        "    for exp_set in exp_sets:\n",
        "        set_id = exp_set[0]   \n",
        "        exp_id = 'A'\n",
        "        for model_type in exp_set[1]:\n",
        "            conduct(ExpG__(set_id, exp_id, model_type))\n",
        "            exp_id = chr(ord(exp_id[0]) + 1)"
      ]
    },
    {
      "source": [
        "## Experiment Group H: Ensembles\n",
        "\n",
        "### Introduction\n",
        "\n",
        "This set of experiment explores ensembles. As a prelimary test, all models resulting from the Group G experiments who accuracy was greater than or equal to 77% were assembled into an ensemble. The ensemble used three classification approaches.\n",
        "\n",
        "1. Combine model probabilities and use highest probability as prediction (AP).\n",
        "2. Select highest probablity as prediction (MP).\n",
        "3. Majority vote wins - ties broken by model with highest accuracy (V1).\n",
        "4. Majority vote wins - ties broken by model with highest probability (V2).\n",
        "\n",
        "Next, the most accurate model, EfficientNet-B5, was trained on 15 different splits of the public data into training and validation data. Given these models have no common validation data, the models are passed to ensemble classifer without regard to their accuracy thereby rendering classification approach V1 meaningless. The test data will be classified by this ensemble using approaches AP, MP, and V2.\n",
        "\n",
        "### Results\n",
        "\n",
        "#### Preliminary Test\n",
        "\n",
        "Predictions for the validation dataset were computed on ensembles comprised of two to 12 of the most accurate models of Group G via the three classification approaches. The accuracies of these predications are summarized in Table H1. The highest accuracy was 85.40%, which is 2.22% higher than the most accurate model. Unfortunately, the most accurate ensemble only achieved an accuracy on the test set of 82.746%, which is only 0.00729% higher than the most accurate model.\n",
        "\n",
        "**Table H1:** The accuracy of the preliminary ensembles.\n",
        "\n",
        "|Models|Pred_AP|Pred_MP|Pred_V1|Pred_V2|\n",
        "|:---:|:---:|:---:|:---:|:---:|\n",
        "|02|83.56|84.02|83.18|84.02|\n",
        "|03|83.87|83.87|83.64|84.02|\n",
        "|04|84.17|83.87|84.48|84.17|\n",
        "|05|84.94|84.10|85.09|85.02|\n",
        "|06|84.56|84.02|85.17|85.17|\n",
        "|07|84.79|84.02|85.40|85.09|\n",
        "|08|84.86|83.94|84.79|84.56|\n",
        "|09|84.94|84.40|84.48|84.71|\n",
        "|10|84.63|84.40|84.33|83.94|\n",
        "|11|84.48|84.40|84.33|84.40|\n",
        "|12|83.94|84.17|83.79|83.64|\n",
        "\n",
        "#### Proper Test\n",
        "\n",
        "My earlier testing determined the data loader's WeightedRandomSampler equally represented the KenyanFood13 dataset's classes during training. I decided to write test code to determine whether the images it selected were equally represented. Somewhat surprisingly, they were not. Hence, I wrote my own sampler that not only provided equal class representation, but also equal image representation during training. I used this sampler in the Group H, Set A experiments to train EfficientNet-B5 models. In addition, each of these experiments used a different random seed value. Hence, they each partitioned the public data into different training and validation sets. These training set were analyzed by counting how many experiments used eah image in the public dataset. Table H2 enumerates count statistics while Figure H1 depicts of count histogram. In hindsight, I should have manually partitioned the public data into training and validation sets to achieve more equal training exposure of the public images.\n",
        "\n",
        "**Table H2:** The count statistics. On average each image in the public dataset is used by 11.998 experiments to train its model with a standard deviation of 1.584.\n",
        "\n",
        "|Statistic|Value|\n",
        "|:---|:---:|\n",
        "|minimum|5|\n",
        "|maximum|15|\n",
        "|median|12|\n",
        "|mean|11.998|\n",
        "|st dev|1.584|\n",
        "\n",
        "![Count Histogram](https://media.githubusercontent.com/media/blazingcayenne/deep_learning_with_pytorch_project2/main/images/SetHATrainingDataHistogram.png?raw=true)<br>\n",
        "**Figure H1:** The number of public images used by N experiments during training.\n",
        "\n",
        "Each experiment was analyzed and the results are enumerated in Table H3. The minimum, maximum, and average accuracies are 81.80%, 84.02%, and 82.84% respectively. Figure H2, H3, and H4 are the confusion matrices on the least accurate prediction, most accurate prediction, and an average prediction respectively. Note: Predictions were made of different validation datasets.\n",
        "\n",
        "**Table H3:** The results of Group H, Set A experiments.\n",
        "\n",
        "|Experiment|Test Loss|Accuracy|Overfitting Metric|\n",
        "|:---|:---:|:---:|:---:|\n",
        "|HAA_EfficientNetB5_0042|0.584|82.80|0.089|\n",
        "|HAB_EfficientNetB5_5174|0.563|83.49|0.081|\n",
        "|HAC_EfficientNetB5_2123|0.621|82.57|0.093|\n",
        "|HAD_EfficientNetB5_6993|0.548|83.94|0.086|\n",
        "|HAE_EfficientNetB5_8133|0.579|82.65|0.104|\n",
        "|HAF_EfficientNetB5_0438|0.587|81.80|0.101|\n",
        "|HAG_EfficientNetB5_3708|0.571|83.41|0.096|\n",
        "|HAH_EfficientNetB5_0888|0.597|82.57|0.104|\n",
        "|HAI_EfficientNetB5_2984|0.598|82.65|0.103|\n",
        "|HAJ_EfficientNetB5_0565|0.557|82.87|0.098|\n",
        "|HAK_EfficientNetB5_6816|0.586|82.95|0.098|\n",
        "|HAL_EfficientNetB5_2956|0.591|81.80|0.102|\n",
        "|HAM_EfficientNetB5_1713|0.569|82.49|0.099|\n",
        "|HAN_EfficientNetB5_2570|0.571|84.02|0.101|\n",
        "|HAO_EfficientNetB5_1541|0.588|82.57|0.115|\n",
        "\n",
        "![Least accurate confusion matrix](https://media.githubusercontent.com/media/blazingcayenne/deep_learning_with_pytorch_project2/main/images/HAF_Confusion_Matrix.png?raw=true)<br>\n",
        "**Figure H2:** The confusion matrix for the least accurate prediction.\n",
        "\n",
        "![Most accurate confusion matrix](https://media.githubusercontent.com/media/blazingcayenne/deep_learning_with_pytorch_project2/main/images/HAN_Confusion_Matrix.png?raw=true)<br>\n",
        "**Figure H3:** The confusion matrix for the most accurate prediction.\n",
        "\n",
        "![Average accuracy confusion matrix](https://media.githubusercontent.com/media/blazingcayenne/deep_learning_with_pytorch_project2/main/images/HAJ_Confusion_Matrix.png?raw=true?raw=true)<br>\n",
        "**Figure H4:** The confusion matrix for an average prediction.\n",
        "\n",
        "Figure H5 visualizes the voting approaches. The x-axis are the indices of the test images. The y-axis is the highest number of models that predicted the same class. For the majority of test images, all models in the ensemble predicted the same class. All images had at least five models predict the same class.\n",
        "\n",
        "![Highest vote tally plot](https://media.githubusercontent.com/media/blazingcayenne/deep_learning_with_pytorch_project2/main/images/FinalEnsembleTallies.png?raw=true)<br>\n",
        "**Figure H5:** The highest vote tallies for all test images.\n",
        "\n",
        "### Conclusions\n",
        "\n",
        "Predictions of the private data were made using the classification approaches AP, MP, and V2. They scored 82.867%,83.110%, and 82.867% respectively on Kaggle's Public Leaderboard."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_exp_result(\n",
        "    exp:TorchVisionModel, \n",
        "    on_test_data:bool=False, \n",
        "    free_experiment:bool=True\n",
        "):\n",
        "    data_loader = exp.valid_loader\n",
        "    if on_test_data:\n",
        "        config = exp.config\n",
        "        data_loader = datastore.get_test_data_loader(\n",
        "            transform = config.dataset.test_transforms,\n",
        "            batch_size = config.data_loader.batch_size,\n",
        "            num_workers = config.data_loader.num_workers\n",
        "        )\n",
        "\n",
        "    preds, probs = get_pred_and_pred_probs(\n",
        "        exp.trained_model, \n",
        "        data_loader, \n",
        "        exp.device\n",
        "    )\n",
        "\n",
        "    if free_experiment:\n",
        "        del exp\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return preds.tolist(), probs.tolist()\n",
        "\n",
        "\n",
        "def get_exp_results(\n",
        "    experiments:Iterable[TorchVisionModel], \n",
        "    on_test_data:bool=False, \n",
        "    free_experiment:bool=True\n",
        "):\n",
        "    return [get_exp_result(exp, on_test_data, free_experiment) for exp in experiments]\n",
        "\n",
        "def combine_predictions(\n",
        "    num_classes:int, \n",
        "    *results\n",
        ") -> Tuple[List[int], List[int], List[int], List[int], List[List[int]]]:\n",
        "    \"\"\"\n",
        "    Computes the ensemble predictions using three methods:\n",
        "    1) combining probabilities and selecting most probable (AP)\n",
        "    2) most probable (MP)\n",
        "    3) majority wins, select most accurate model's prediction (V1)\n",
        "    4) majority wins, select most probable prediction (V2)\n",
        "\n",
        "    results - list of predictions/probabilities sequence\n",
        "              sorted by model accuracy in descending order\n",
        "    \"\"\"\n",
        "    preds_ap = [] # approach #1\n",
        "    preds_mp = [] # approach #2\n",
        "    preds_v1 = [] # approach #3\n",
        "    preds_v2 = [] # approach #4\n",
        "    tallies = []\n",
        "\n",
        "    model_preds = []\n",
        "    model_probs = []\n",
        "    for result in results:\n",
        "        model_preds.append(result[0])\n",
        "        model_probs.append(result[1])\n",
        "\n",
        "    for preds, probs in zip(zip(*model_preds), zip(*model_probs)):\n",
        "        # combine probabilities and select the most probable\n",
        "        pred_ap = np.argmax(np.sum(probs, axis=0))\n",
        "\n",
        "        # retain each model's maximum probability        \n",
        "        probs = np.amax(probs, axis = 1)\n",
        "        \n",
        "        # find the model with the highest probability\n",
        "        index = np.argmax(probs)\n",
        "        pred_mp = preds[index]\n",
        "\n",
        "        # retain the majority\n",
        "        tally = [preds.count(label) for label in range(num_classes)]\n",
        "        indices, = np.where(tally == np.max(tally))\n",
        "        majority = [(pred, prob) for pred, prob in zip(preds, probs) if pred in indices]\n",
        "\n",
        "        # select most accurate model's prediction\n",
        "        pred_v1 = majority[0][0]\n",
        "\n",
        "        # select most probable prediction\n",
        "        preds, probs = zip(*majority)\n",
        "        pred_v4 = preds[np.argmax(probs)]\n",
        "\n",
        "        #print(f\"{indices} {tally}\")\n",
        "        #print(majority)\n",
        "        #print(pred_ap, pred_mp, pred_v1, pred_v4)\n",
        "        #print()\n",
        "\n",
        "        preds_ap.append(pred_ap)\n",
        "        preds_mp.append(pred_mp)\n",
        "        preds_v1.append(pred_v1)\n",
        "        preds_v2.append(pred_v4)\n",
        "        tallies.append(tally)\n",
        "\n",
        "    return preds_ap, preds_mp, preds_v1, preds_v2, tallies\n",
        "\n",
        "def conduct_ensemble_experiment(exp_list:Iterable[TorchVisionModel], on_test_data:bool=False):\n",
        "    results = get_exp_results(exp_list, on_test_data)\n",
        "    targets = datastore.test_fnames\n",
        "    if not on_test_data:\n",
        "        num_classes = len(datastore.class_names)\n",
        "        extra = datastore.get_training_data().valid_labels\n",
        "\n",
        "        print()\n",
        "        print(\"|Models|Pred_AP|Pred_MP|Pred_V1|Pred_V2|\")\n",
        "        print(\"|:---:|:---:|:---:|:---:|:---:|\")\n",
        "        for i in range(2, len(results) + 1):\n",
        "            preds_ap, preds_mp, preds_v1, preds_v2, _ = combine_predictions(\n",
        "                num_classes, \n",
        "                *(results[:i])\n",
        "            )\n",
        "            acc_ap = compute_accuracy(targets, preds_ap) * 100\n",
        "            acc_mp = compute_accuracy(targets, preds_mp) * 100\n",
        "            acc_v1 = compute_accuracy(targets, preds_v1) * 100\n",
        "            acc_v2 = compute_accuracy(targets, preds_v2) * 100\n",
        "            print(f\"|{i:02d}|{acc_ap:.2f}|{acc_mp:.2f}|{acc_v1:.2f}|{acc_v2:.2f}|\")\n",
        "\n",
        "    return targets, results\n",
        "\n",
        "\n",
        "def create_tallies_figure(tallies:List[List[int]]):\n",
        "    idx = list(range(len(tallies)))\n",
        "    max_votes = [np.max(tally) for tally in tallies]\n",
        "\n",
        "    plt.style.use('default')\n",
        "    fig, ax = plt.subplots(figsize=(7,4))\n",
        "    ax.bar(x=idx, height=max_votes, width=1., color=\"#0080FF80\")\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['left'].set_visible(False)\n",
        "    ax.spines['bottom'].set_color('#000000')\n",
        "    ax.tick_params(bottom=False, left=False)\n",
        "    ax.set_axisbelow(True)\n",
        "    ax.yaxis.grid(True, color='#404040')\n",
        "    ax.xaxis.grid(False)\n",
        "    ax.set_xlabel('Test Image Index', labelpad=5)\n",
        "    ax.set_ylabel('Highest Vote Tally', labelpad=5)\n",
        "    ax.set_title('Highest Vote Tallies for Test Images', pad=5, weight='bold')\n",
        "    ax.set_yticks(list(range(len(results) + 1)))\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.close()\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Note: Implementing experiments as a list causes a CUDA\n",
        "#       out of memory exception error when inferencing on\n",
        "#       the experiment's trained model.\n",
        "class PreliminaryEnsemble():\n",
        "    def __init__(self):\n",
        "        return\n",
        "    def __iter__(self):\n",
        "        self.__idx = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        self.__idx += 1\n",
        "        if self.__idx == 1:\n",
        "            return ExpG__('F', 'F', EfficientNetB5)\n",
        "        elif self.__idx == 2:\n",
        "            return ExpG__('F', 'E', EfficientNetB4)\n",
        "        elif self.__idx == 3:\n",
        "            return ExpG__('F', 'G', EfficientNetB6)\n",
        "        elif self.__idx == 4:\n",
        "            return ExpG__('F', 'D', EfficientNetB3)\n",
        "        elif self.__idx == 5:\n",
        "            return ExpG__('B', 'A', ResNeXt50)\n",
        "        elif self.__idx == 6:\n",
        "            return ExpG__('E', 'D', DenseNet161)\n",
        "        elif self.__idx == 7:\n",
        "            return ExpG__('C', 'A', WideResNet50)\n",
        "        elif self.__idx == 8:\n",
        "            return ExpG__('E', 'C', DenseNet201)\n",
        "        elif self.__idx == 9:\n",
        "            return ExpG__('B', 'B', ResNeXt101)\n",
        "        elif self.__idx == 10:\n",
        "            return ExpG__('A', 'D', ResNet101)\n",
        "        elif self.__idx == 11:\n",
        "            return ExpG__('F', 'C', EfficientNetB2)\n",
        "        elif self.__idx == 12:\n",
        "            return ExpG__('A', 'C', ResNet50)\n",
        "        else:\n",
        "            raise StopIteration\n",
        "\n",
        "\n",
        "def get_best_preliminary_ensemble_predictions(on_test_data:bool=False):\n",
        "    extra, results = conduct_ensemble_experiment(\n",
        "        PreliminaryEnsemble(), \n",
        "        on_test_data\n",
        "    )\n",
        "\n",
        "    _, _, preds, _, _ = combine_predictions(\n",
        "        len(datastore.class_names), \n",
        "        *(results[:7])\n",
        "    )\n",
        "\n",
        "    return extra, preds\n",
        "\n",
        "    \n",
        "def print_confusion_matrix(\n",
        "    exp_name: str,\n",
        "    targets: List[int],\n",
        "    preds: List[int]\n",
        "):\n",
        "    cm = confusion_matrix(targets, preds)\n",
        "    trace = np.trace(cm)\n",
        "    total = np.sum(cm)\n",
        "    print(cm)\n",
        "    print()\n",
        "    print(f\"{trace} / {total} = {trace / total}\")\n",
        "\n",
        "    path = os.path.join(proj_dir, \"images\", \"cm\", exp_name + \".png\")\n",
        "    figure = create_confusion_matrix(cm, datastore.class_names, exp_name)\n",
        "    figure.savefig(path)\n",
        "\n",
        "\n",
        "def log_confusion_matrix(\n",
        "    exp_name: str,\n",
        "    targets: List[int],\n",
        "    preds: List[int]\n",
        "):\n",
        "    config = create_master_config()\n",
        "    cm = confusion_matrix(targets, preds)\n",
        "\n",
        "    path = os.path.join(config.system.proj_dir, \"images\", \"cm\", exp_name + \".png\")\n",
        "    figure = create_confusion_matrix(cm, datastore.class_names, exp_name)\n",
        "    figure.savefig(path)\n",
        "        \n",
        "    visualizer = TensorBoardVisualizer(os.path.join(\n",
        "        config.system.proj_dir,\n",
        "        config.trainer.visualizer_dir, \n",
        "        exp_name\n",
        "    ))\n",
        "    tag = f\"Confusion Matrix ({exp_name})\"\n",
        "    figure = create_confusion_matrix(cm, datastore.class_names, exp_name)\n",
        "    visualizer.add_figure(tag=tag, figure=figure, close=True)\n",
        "    visualizer.close_tensorboard()\n",
        "    \n",
        "\n",
        "#targets, results = conduct_ensemble_experiment(PreliminaryEnsemble())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ExpHA_(ModelExperiment):\n",
        "    def __init__(\n",
        "        self,\n",
        "        exp_id: str, # expects a single uppercase letter\n",
        "        seed: int\n",
        "    ):\n",
        "        pretrained = True\n",
        "        tuning_level = 5\n",
        "        model_type = EfficientNetB5\n",
        "        model_abbr = model_type.__name__\n",
        "        image_size = get_image_size(model_type)\n",
        "        batch_size = get_batch_size(model_type)\n",
        "\n",
        "        self.__pretrained = pretrained\n",
        "        self.__tuning_level = tuning_level\n",
        "        self.__model_type = model_type\n",
        "        self.__model_abbr = model_abbr + f\"_{seed:04d}\"\n",
        "\n",
        "        super().__init__(\n",
        "            abbr = \"HA\" + exp_id,\n",
        "            system_config_seed = seed,           \n",
        "            di_approach = DIApproach.MY_SAMPLER,\n",
        "            transform_resize = int(image_size * 16 / 14),\n",
        "            transform_crop_size = image_size,\n",
        "            data_loader_num_workers = 12,\n",
        "            data_loader_batch_size = batch_size,\n",
        "            optimizer = Optimizer.ADAM,\n",
        "            optimizer_learning_rate = 1.0E-04 * math.sqrt(0.1),\n",
        "            scheduler_step_size = 10,\n",
        "            scheduler_gamma = math.sqrt(0.1),\n",
        "            trainer_training_epochs = 20,\n",
        "            trainer_stop_loss_epochs = 15\n",
        "        )\n",
        "\n",
        "    def _get_model(self):\n",
        "        return self.__model_type(self.__pretrained, self.__tuning_level), self.__model_abbr\n",
        "\n",
        "\n",
        "class FinalEnsemble():\n",
        "    def __init__(self):\n",
        "        self.__params = [\n",
        "            ('A',   42), ('B', 5174), ('C', 2123),\n",
        "            ('D', 6993), ('E', 8133), ('F',  438),\n",
        "            ('G', 3708), ('H',  888), ('I', 2984),\n",
        "            ('J',  565), ('K', 6816), ('L', 2956),\n",
        "            ('M', 1713), ('N', 2570), ('O', 1541)\n",
        "        ]\n",
        "    def __iter__(self):\n",
        "        self.__idx = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self.__idx >= len(self.__params):\n",
        "            raise StopIteration\n",
        "        exp_id, seed = self.__params[self.__idx]\n",
        "        self.__idx += 1\n",
        "        return ExpHA_(exp_id, seed)\n",
        "\n",
        "\n",
        "def conduct_group_H():\n",
        "    for exp in FinalEnsemble():\n",
        "        conduct(exp)\n",
        "\n",
        "def analyze_set_HA_training_data():\n",
        "    # count the number of times each image is used to train an \"HA\" model\n",
        "    tallies = {fname: 0 for fname in datastore.train_fnames}\n",
        "    for exp in FinalEnsemble():\n",
        "        for fname in exp.data.train_fnames:\n",
        "            tallies[fname] += 1\n",
        "        del exp\n",
        "    torch.cuda.empty_cache()\n",
        "   \n",
        "    counts = list(tallies.values())\n",
        "    min = np.amin(counts)\n",
        "    max = np.amax(counts)\n",
        "    bins = range(min, max+2)\n",
        "\n",
        "    print(\"|Statistic|Value|\")\n",
        "    print(\"|:---|:---:|\")\n",
        "    print(f\"|minimum|{min}|\")\n",
        "    print(f\"|maximum|{max}|\")\n",
        "    print(f\"|median|{np.median(counts):.0f}|\")\n",
        "    print(f\"|mean|{np.mean(counts):.3f}|\")\n",
        "    print(f\"|st dev|{np.std(counts):.3f}|\")\n",
        "\n",
        "    plt.style.use('default')\n",
        "    fig = plt.figure(figsize=(5,3), tight_layout=True)\n",
        "    plt.hist(counts, bins=bins) \n",
        "    plt.title(\"Training Data Histogram\") \n",
        "    ax = plt.gca()\n",
        "    ax.set_xlabel(\"Number of Ensemble Models That Used the Same Image\")\n",
        "    ax.set_ylabel(\"Number of Images\")\n",
        "    ax.set_xticks([x + 0.5 for x in bins[:-1]])\n",
        "    ax.set_xticklabels([str(x) for x in bins[:-1]])\n",
        "    plt.close()\n",
        "    return fig\n",
        "\n",
        "#fig = analyze_set_HA_training_data()\n",
        "#fig.savefig(os.path.join(proj_dir, \"images\", \"set_ha_hist.png\"))\n",
        "#conduct(ExpHA_('F',  438), train_model=False, log_confusion_matrix=True)\n",
        "#conduct(ExpHA_('N', 2570), train_model=False, log_confusion_matrix=True)\n",
        "#conduct(ExpHA_('J',  565), train_model=False, log_confusion_matrix=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx9Gbfds3-53"
      },
      "source": [
        "### <font style=\"color:blue\">Main Function</font>\n",
        "\n",
        "A simple function that conducts groups of experiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FFvSmzspuww"
      },
      "source": [
        "def analyze_runs():\n",
        "    # Warning: Make sure analysis destination directories exist!\n",
        "    analyze_runs_for_learning_rate(filters=[\"^BA\", \"^BB\", \"^BC\"])\n",
        "    analyze_runs_for_overfitting(filter=\"^C\")\n",
        "    analyze_runs_for_overfitting(filter=\"^D\")\n",
        "    analyze_runs_for_overfitting(filter=\"^E\")\n",
        "    analyze_runs_for_overfitting(filter=\"^F\")\n",
        "    analyze_runs_for_overfitting(filter=\"^G\")\n",
        "    analyze_runs_for_overfitting(filter=\"^H\")\n",
        "\n",
        "def create_submission_from_experiment(exp:TorchVisionModel):\n",
        "    # log its confusion matrix and precision-recall curves\n",
        "    conduct(exp, train_model=False, log_confusion_matrix=True, log_pr_curves=True)\n",
        "\n",
        "    # create the submission file that classifies the private test data\n",
        "    config = exp.config\n",
        "    create_submission_file_from_experiment(\n",
        "        path = os.path.join(proj_dir, \"submission.csv\"),\n",
        "        exp = exp,\n",
        "        dataloader = datastore.get_test_data_loader(\n",
        "            transform = config.dataset.test_transforms,\n",
        "            batch_size = config.data_loader.batch_size,\n",
        "            num_workers = config.data_loader.num_workers\n",
        "        )\n",
        "    )\n",
        "\n",
        "def create_submission_from_ensemble(ensemble:Iterable[TorchVisionModel]):\n",
        "    fnames, results = conduct_ensemble_experiment(FinalEnsemble(), on_test_data=True)\n",
        "\n",
        "    preds_ap, preds_mp, _, preds_v2, tallies = combine_predictions(\n",
        "        len(datastore.class_names), \n",
        "        *results\n",
        "    )\n",
        "\n",
        "    fig = create_tallies_figure(tallies)\n",
        "    fig.savefig(os.path.join(proj_dir, \"images\", f\"{type(ensemble).__name__}Tallies.png\"))\n",
        "\n",
        "    for preds, suffix in ((preds_ap, \"ap\"), (preds_mp, \"mp\"), (preds_v2, \"v2\")):\n",
        "        create_submission_file(\n",
        "            path = os.path.join(proj_dir, f\"submission_{suffix}.csv\"),\n",
        "            class_names = datastore.class_names,\n",
        "            fnames = fnames,\n",
        "            preds = preds\n",
        "        )\n",
        "\n",
        "# attempt01 = ExpD__('A', 'B', \"_DA_DEF\", aug_mode=DataAugMode.DEFAULT)\n",
        "# attempt02 = ... interactive coding ...\n",
        "# attempt03 = ExpG__('F', 'F', EfficientNetB5)\n",
        "# attempt04_06 = FinalEnsemble()\n",
        "#\n",
        "#                                                   Kaggle Public Score\n",
        "#                                                   -------------------------\n",
        "# create_submission_from_experiment(attempt01)   #  81.004%\n",
        "# PreliminaryEnsemble w/ top 7 models, preds_v1  #  82.746%\n",
        "# create_submission_from_experiment(attempt03)   #  82.017%\n",
        "# create_submission_from_ensemble(attempt04_06)  #  82.867%, 83.110%, 82.867%"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9Q6_bS9lDh8"
      },
      "source": [
        "def main():   \n",
        "    for group in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']:\n",
        "        if group == 'A':\n",
        "            conduct_group_A()\n",
        "        elif group == 'B':\n",
        "            conduct_group_B()\n",
        "        elif group == 'C':\n",
        "            conduct_group_C()\n",
        "        elif group == 'D':\n",
        "            conduct_group_D()\n",
        "        elif group == 'E':\n",
        "            conduct_group_E()\n",
        "        elif group == 'F':\n",
        "            conduct_group_F()\n",
        "        elif group == 'G':\n",
        "            conduct_group_G()\n",
        "        elif group == 'H':\n",
        "            conduct_group_H()\n",
        "    analyze_runs()\n",
        "    return"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4Fe0mLZ3-53",
        "scrolled": false
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    print(\"Uncomment the 'main()' function to run project.\")\n",
        "    #main()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uncomment the 'main()' function to run project.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "## Project Conclusions\n",
        "\n",
        "The non-ensemble experiment with the best accuracy is **GFF_EfficientNetB5_PT5**. This experiment fine tuned the entire convolutional base and classifier of a pretrained EfficientNet-B5 model using the default data augmentation, my custom data loader sampler, and a very small L2 regularization. Its accuracy in classifying the samples in the validation set is 83.18%. Its accuracy on the test data is 82.017% according to the public Kaggle leaderboard. Its confusion matrix is depicted in Figure 3.\n",
        "\n",
        "![The confusion matrix of the most accurate model.](https://media.githubusercontent.com/media/blazingcayenne/deep_learning_with_pytorch_project2/main/images/GFF_Confusion_Matrix.png?raw=true)<br>\n",
        "**Figure 3:** The confusion matrix of the most accurate model.\n",
        "\n",
        "My most accurate prediction on Kaggle's Public Leadership is the FinalEnsemble, most probable predictions (Preds_MP). Its accuracy on the test data is 83.110% according to the public Kaggle leaderboard.\n",
        "\n",
        "### Future Exploration\n",
        "\n",
        "The following areas warrant attention.\n",
        "\n",
        "* Investigate why increasing the regularization not only reduced overfitting, but also reduced the model's accuracy.\n",
        "* Investigate whether the accuracies of the second stages of the two-stage model could be increased, thereby increasing the overall accuracy of the two-stage model.\n",
        "\n",
        "In general, the models confused the following classes.\n",
        "\n",
        "* kukuchoma and nyamachoma\n",
        "* sukumawiki and ugali\n",
        "\n",
        "Future exploration should focus on distinguishing between these classes."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFt15GPK3-53"
      },
      "source": [
        "## <font style=\"color:green\">8. TensorBoard Dev Scalars Log Link [5 Points]</font>\n",
        "\n",
        "Share your tensorboard scalars logs link in this section. You can also share (not mandatory) your GitHub link if you have pushed this project in GitHub. \n",
        "\n",
        "You can view the project's TensorBoard scalar logs and GitHub repository via the following links.\n",
        "* [Kaggle Profile](https://www.kaggle.com/kevinkramer)\n",
        "* [TensorBoard scalar logs](https://tensorboard.dev/experiment/JIAjKMu6Q0GLAijDzP06Lw/#scalars)\n",
        "* [GitHub repository](https://github.com/blazingcayenne/deep_learning_with_pytorch_project2)\n",
        "\n",
        "The TensorBoard scalars were shared via the following command.\n",
        "```\n",
        "tensorboard dev upload --logdir runs/ --name \"Project2 Experiments\" --description \"Classification experiments on the KenyanFood13 dataset (cont. 2)\"\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh36KFmK3-53"
      },
      "source": [
        "## <font style=\"color:green\">9. Kaggle Profile Link [50 Points]</font>\n",
        "\n",
        "Share your Kaggle profile link here with us so that we can give points for the competition score. \n",
        "\n",
        "You should have a minimum accuracy of `75%` on the test data to get all points. If accuracy is less than `70%`, you will not get any points for the section. \n",
        "\n",
        "**You must have to submit `submission.csv` (prediction for images in `test.csv`) in `Submit Predictions` tab in Kaggle to get any evaluation in this section.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id,class\n9156739011499789258,nyamachoma\n2049465964503133373,kachumbari\n6446998501027132988,nyamachoma\n4194396063119815321,ugali\n9018117998187006009,kachumbari\n6246759883907852128,masalachips\n16478122708528316044,nyamachoma\n14045745760440690312,ugali\n7872954221890963019,matoke\n"
          ]
        }
      ],
      "source": [
        "# display the first 10 lines of the submission.csv file\n",
        "submission_path = os.path.join(proj_dir, \"submission_mp.csv\")\n",
        "!head {submission_path.replace(\" \", \"\\\\ \")}"
      ]
    }
  ]
}