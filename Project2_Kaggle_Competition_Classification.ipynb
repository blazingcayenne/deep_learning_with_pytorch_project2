{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0-final"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhTA9Jbl3-5s"
      },
      "source": [
        "# <font style=\"color:blue\">Project 2: Kaggle Competition - Classification</font>\n",
        "\n",
        "#### Maximum Points: 100\n",
        "\n",
        "<div>\n",
        "    <table>\n",
        "        <tr><td><h3>Sr. no.</h3></td> <td><h3>Section</h3></td> <td><h3>Points</h3></td> </tr>\n",
        "        <tr><td><h3>1</h3></td> <td><h3>Data Loader</h3></td> <td><h3>10</h3></td> </tr>\n",
        "        <tr><td><h3>2</h3></td> <td><h3>Configuration</h3></td> <td><h3>5</h3></td> </tr>\n",
        "        <tr><td><h3>3</h3></td> <td><h3>Evaluation Metric</h3></td> <td><h3>10</h3></td> </tr>\n",
        "        <tr><td><h3>4</h3></td> <td><h3>Train and Validation</h3></td> <td><h3>5</h3></td> </tr>\n",
        "        <tr><td><h3>5</h3></td> <td><h3>Model</h3></td> <td><h3>5</h3></td> </tr>\n",
        "        <tr><td><h3>6</h3></td> <td><h3>Utils</h3></td> <td><h3>5</h3></td> </tr>\n",
        "        <tr><td><h3>7</h3></td> <td><h3>Experiment</h3></td><td><h3>5</h3></td> </tr>\n",
        "        <tr><td><h3>8</h3></td> <td><h3>TensorBoard Dev Scalars Log Link</h3></td> <td><h3>5</h3></td> </tr>\n",
        "        <tr><td><h3>9</h3></td> <td><h3>Kaggle Profile Link</h3></td> <td><h3>50</h3></td> </tr>\n",
        "    </table>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sDaF3ovpuwb"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This work is the second project of the OpenCV Pytroch Course's. Its focus is image classification. Half of the score is based on implementation. The other half is based on model performance.\n",
        "\n",
        "### Data Description\n",
        "\n",
        "The dataset consists of 8,174 images in 13 Kenyan food type classes. Sample images of KenyanFood13 dataset and the number of images in each of the classes are shown below:\n",
        "\n",
        "![Sample images from the KenyanFood13 Dataset](https://media.githubusercontent.com/media/blazingcayenne/deep_learning_with_pytorch_project2/main/images/KenyanFood13.jpg?raw=true)<br>\n",
        "**Figure 1:** A sample image from each class of the KenyanFood13 dataset.\n",
        "\n",
        "The data was split into public training set and private test set which is used for evaluation of submissions. The public set can be split into training and validation subsets.\n",
        "\n",
        "### Goal\n",
        "\n",
        "To create a model that predicts a type of food represented on each image in the KenyanFood13 dataset's private test set. Pre-trained models may be used. The performance metric is accuracy. To receive any performance point, an accuracy of 70% must be achieved. To receive full points, an accuracy of 75% must be achieved.\n",
        "\n",
        "### Approach\n",
        "\n",
        "Rather than create my own CNN architecture, I will explore transfer learning of models that have been trained on the ImageNet data set. Pedro Marcelino defines three strategies for fine-tuning pretrained models in **[Transfer learning from pre-trained models](https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751)**.\n",
        "\n",
        "* Train the entire model\n",
        "* Train some layers and leave others frozen\n",
        "* Train only the classifer by freezing the convolutional base\n",
        "\n",
        "Marcelino gives guidance as to which strategy to use based on one's dataset size and similarity (see Figure 2). Marcelino defines a small dataset as one with less than 1000 images per class. According to this definition, the KenyanFood13 dataset is small. However, it is unclear whether the application of data augmentation would reclassify the size of this dataset. Hence, this project will explore training the entire pretrained model as well as training some layers and leaving others frozen. Regarding dataset similarity, Marceline states:\n",
        "\n",
        "> ...  let common sense prevail. For example, if your task is to identify cats and dogs, ImageNet would be a similar dataset because it has images of cats and dogs. However, if your task is to identify cancer cells, ImageNet canâ€™t be considered a similar dataset.\n",
        "\n",
        "Fortunately, the ImageNet dataset contains 10 food classes: apple, banana, broccoli, burger, egg, french fries, hot dog, pizza, rice, and strawberry. Unfortunately, ImageNet's food images look significantly different from the images in Kenyan13Food dataset. Hence, the dataset similarity is moderate. Consequently, this project will also explore freezing the convolutional base and training only the classifier.\n",
        "\n",
        "![Decision map](https://media.githubusercontent.com/media/blazingcayenne/deep_learning_with_pytorch_project2/main/images/TransferLearningApproaches.png?raw=true)<br>\n",
        "**Figure 2:** Decision map for fine-tuning pre-trained models.\n",
        "\n",
        "According to the decision map, pre-trained models should be fine tuned by either freezing part or all of the convolutional base. This project will test the efficiency of this guidance.\n",
        "\n",
        "Dishashree Gupta also defines strategies for fine-tuning pre-trained models in his blog post, **[Transfer learning and the art of using Pre-trained Models in Deep Learning](https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/)**. Gupta's strategies, while similar to Marcelino, have the following differences.\n",
        "\n",
        "* Large datasets with low similarity should train models from scratch.\n",
        "* Large datasets with high similarity should train the entire pre-trained model.\n",
        "\n",
        "### Implementation Overview\n",
        "\n",
        "My primary objective is to fine-tune a pretrained model to achieve a minimum of 75% accuracy on the KenyanFood13 dataset. My secondary objective is to improve my proficiency with the [Python](https://www.python.org/) computer programming language. Prior to this class, my Python expertise was almost non-existent. Consequently, I developed class hierachies that enable the rapid implementation of fine-tuning experiments on following pre-trained TorchVision and EfficientNet models using any of Marcelino or Gupta's strategies.\n",
        "\n",
        "* ResNet-18\n",
        "* ResNet-34\n",
        "* ResNet-50\n",
        "* ResNet-101\n",
        "* ResNet-152\n",
        "* ResNeXt-50-32x4d\n",
        "* ResNeXt-101-32x8d\n",
        "* Wide ResNet-50-2\n",
        "* Wide ResNet-101-2\n",
        "* VGG-11 with batch normalization\n",
        "* VGG-13 with batch normalization\n",
        "* VGG-16 with batch normalization\n",
        "* VGG-19 with batch normalization\n",
        "* DenseNet-121\n",
        "* DenseNet-169\n",
        "* DenseNet-201\n",
        "* DenseNet-161\n",
        "* EfficientNet-B0\n",
        "* EfficientNet-B1\n",
        "* EfficientNet-B2\n",
        "* EfficientNet-B3\n",
        "* EfficientNet-B4\n",
        "* EfficientNet-B5\n",
        "* EfficientNet-B6\n",
        "* EfficientNet-B7\n",
        "\n",
        "I used and modified the Trainer Pipeline module introduced in **Week 6 - Best Practicing in Deep Learning > How to structure your Project for Scale**. Modications to the trainer module include, but are not limited to, the following:\n",
        "\n",
        "* additional configuration parameters,\n",
        "* saving the model state only when the average loss on the validation set reaches a new low,\n",
        "* prematurely stopping training when either the loss or accuracy does not significantly improve over a certain number of epochs, and\n",
        "* extending the visualization base and TensorBoard classes to support the logging of images, figures, graphs, and PR curves.\n",
        "\n",
        "Experiments are identified by three uppercase capital letters per the regular expression \\(\\[A-Z\\]\\[A-Z\\]\\[A-Z\\]\\). The first and second letters designate the experiment group and experiment set respectively. The last letter designates an individual experiment. Hence, all experiments that begin with \"A\" belong to Group A, while all experiments that begin with \"AB\" belong to Group A, Set B.\n",
        "\n",
        "I implemented the following groups of experiments.\n",
        "\n",
        "* Group A to explore the data and verify the training pipeline.\n",
        "* Group B to explore the impact of learning rate on training pretrained EfficientNet models.\n",
        "* Group C to explore the impact of EfficientNet model size and transfer learning approach on bias and variance.\n",
        "* Group D to explore the impact of additional regularization and dataset imbalance approaches.\n",
        "* Group E to explore the impact of a two-stage classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LNCmWgkBNLg"
      },
      "source": [
        "# This cell initializes the notebook for execution on different hosts.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "def get_host() -> str:\n",
        "    \"\"\"\n",
        "    The get_ipython() function returns the following from different hosts.\n",
        "\n",
        "    colab:  <google.colab._shell.Shell object at 0x7f23c5e386d8>\n",
        "    brule:  <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f1990f22a50>\n",
        "    kaggle: <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f9d093aebd0>\n",
        "    \"\"\"\n",
        "    \n",
        "    if 'google.colab' in str(get_ipython()):\n",
        "        return \"colab\"\n",
        "    else:\n",
        "        # ToDo: Determine whether running on kaggle.\n",
        "        return \"brule\"\n",
        "\n",
        "def init_host(host:str):\n",
        "    if host == \"brule\":\n",
        "        # set data and project directories\n",
        "        if os.path.isdir(\"./trainer\"):\n",
        "            data_dir = \"./data\"\n",
        "            proj_dir = \"./\"\n",
        "        elif os.path.isdir(\"./project2/trainer\"):\n",
        "            data_dir = \"./project2/data\"\n",
        "            proj_dir = \"./project2\"\n",
        "        else:\n",
        "            raise SystemExit(\"Cannot locate trainer module.\")\n",
        "\n",
        "    elif host == \"colab\":\n",
        "        # mount Google Drive\n",
        "        from google.colab import drive\n",
        "        drive.mount(\"/content/gdrive\")\n",
        "\n",
        "        # set data and project directories\n",
        "        data_dir = \"/content/data\"\n",
        "        proj_dir = \"/content/gdrive/MyDrive/Colab Notebooks/project2\"\n",
        "\n",
        "        # fetching data from Google Drive is very, very slow ...\n",
        "        # hence, we will unzip the dataset to /content/data if it is not there\n",
        "        dataset = os.path.join(proj_dir, \"data\", \"pytorch-opencv-course-classification.zip\")\n",
        "        if not os.path.isdir(data_dir):\n",
        "              os.makedirs(data_dir)\n",
        "              import zipfile\n",
        "              with zipfile.ZipFile(dataset, 'r') as zip_ref:\n",
        "                  zip_ref.extractall(data_dir)              \n",
        "\n",
        "    else:\n",
        "        raise SystemExit(\"Unknown host! Cannot continue.\")\n",
        "\n",
        "    sys.path.append(proj_dir)\n",
        "    return data_dir, proj_dir\n",
        "\n",
        "data_dir, proj_dir = init_host(get_host())\n",
        "\n",
        "print(f\"data_dir: {data_dir}\")\n",
        "!ls -lh {data_dir.replace(\" \", \"\\\\ \")}\n",
        "\n",
        "print(f\"proj_dir: {proj_dir}\")\n",
        "!ls -lh {proj_dir.replace(\" \", \"\\\\ \")}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xiq8YhMC3-5w"
      },
      "source": [
        "# import organzier @ https://pypi.org/project/importanize/\n",
        "\n",
        "from abc import ABC, abstractmethod, abstractproperty\n",
        "from collections import namedtuple\n",
        "from dataclasses import dataclass, replace\n",
        "from enum import Enum, auto\n",
        "from operator import itemgetter\n",
        "from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union\n",
        "\n",
        "import itertools\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import PIL\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, Dataset, Sampler, WeightedRandomSampler\n",
        "from torchvision import models, transforms\n",
        "\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "from trainer import Trainer, configuration, hooks\n",
        "from trainer.configuration import SystemConfig\n",
        "from trainer.configuration import DataAugConfig, DatasetConfig, DataLoaderConfig\n",
        "from trainer.configuration import OptimizerConfig, SchedulerConfig, TrainerConfig\n",
        "from trainer.metrics import AccuracyEstimator\n",
        "from trainer.tensorboard_visualizer import TensorBoardVisualizer\n",
        "from trainer.utils import patch_configs, setup_system"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWi_M1eB3-5x"
      },
      "source": [
        "## <font style=\"color:green\">1. Data Loader [10 Points]</font>\n",
        "\n",
        "In this section, you have to write a class or methods that will be used to get training and validation data\n",
        "loader.\n",
        "\n",
        "You will have to write a custom dataset class to load data.\n",
        "\n",
        "**Note that there are not separate validation data, so you will have to create your validation set by dividing train data into train and validation data. Usually, in practice, we do `80:20` ratio for train and validation, respectively.** \n",
        "\n",
        "For example,\n",
        "\n",
        "```\n",
        "class KenyanFood13Dataset(Dataset):\n",
        "    \"\"\"\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, *args):\n",
        "    ....\n",
        "    ...\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "    ...\n",
        "    ...\n",
        "    \n",
        "    \n",
        "```\n",
        "\n",
        "```\n",
        "def get_data(args1, *agrs):\n",
        "    ....\n",
        "    ....\n",
        "    return train_loader, test_loader\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class KF13Dataset(Dataset):\n",
        "    \"\"\"\n",
        "    This custom PyTorch dataset contains images and classification labels from\n",
        "    Kaggle's KenyanFood13 dataset.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, image_root, fnames, labels=None, transform=None):\n",
        "        super().__init__()\n",
        "        self.__fnames = fnames\n",
        "        self.__labels = labels\n",
        "        self.__transform = transform\n",
        "        self.__image_root = image_root\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the dataset's length, i.e., the number of image/label pairs.\n",
        "        \"\"\"\n",
        "\n",
        "        return len(self.__fnames)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns the (optionally resized & preprocessed) image that corresponds to the specified index.\n",
        "        \"\"\"\n",
        "\n",
        "        # conversion needed to remove alpha channel, if present\n",
        "        path = os.path.join(self.__image_root, self.__fnames[idx] + \".jpg\")\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "        \n",
        "        if self.__transform is not None:\n",
        "            image = self.__transform(image)\n",
        "\n",
        "        if self.__labels is None:\n",
        "            extra = self.__fnames[idx]  # return file name with image\n",
        "        else:\n",
        "            extra = self.__labels[idx]  # return target with image\n",
        "\n",
        "        return image, extra\n",
        "\n",
        "\n",
        "class KF13IndexedDataset(Dataset):\n",
        "    \"\"\"\n",
        "    This custom PyTorch dataset contains images and classification labels from\n",
        "    Kaggle's KenyanFood13 dataset.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, image_root, indices, fnames, labels=None, transform=None):\n",
        "        super().__init__()\n",
        "        self.__fnames = fnames\n",
        "        self.__labels = labels\n",
        "        self.__indices = indices\n",
        "        self.__transform = transform\n",
        "        self.__image_root = image_root\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the dataset's length, i.e., the number of image/label pairs.\n",
        "        \"\"\"\n",
        "\n",
        "        return len(self.__indices)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns the (optionally resized & preprocessed) image that corresponds to the specified index.\n",
        "        \"\"\"\n",
        "        idx = self.__indices[idx]\n",
        "\n",
        "        # conversion needed to remove alpha channel, if present\n",
        "        path = os.path.join(self.__image_root, self.__fnames[idx] + \".jpg\")\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "        \n",
        "        if self.__transform is not None:\n",
        "            image = self.__transform(image)\n",
        "\n",
        "        if self.__labels is None:\n",
        "            extra = self.__fnames[idx]  # return file name with image\n",
        "        else:\n",
        "            extra = self.__labels[idx]  # return target with image\n",
        "\n",
        "        return idx, image, extra\n",
        "\n",
        "\n",
        "class KF13TrainingData:\n",
        "    \"\"\"\n",
        "    Splits the specified data into training and validation sets preserving the\n",
        "    relative ratios of the number of images of each class type.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self, \n",
        "        image_root: str,\n",
        "        class_names: List[str],\n",
        "        fnames: List[str], \n",
        "        labels: List[int],\n",
        "        mapping: Optional[List[int]] = None,\n",
        "        subset: bool = False,\n",
        "        valid_size: float = 0.2, \n",
        "        random_seed: int = 42\n",
        "    ):\n",
        "        self.__mapping = mapping\n",
        "        self.__image_root = image_root\n",
        "        self.__class_names = class_names\n",
        "        self.__valid_size = valid_size\n",
        "\n",
        "        # split the training data into training and validation sets\n",
        "        train_fnames, valid_fnames, train_labels, valid_labels = train_test_split(\n",
        "            fnames,                      # image file names w/o path or extension\n",
        "            labels,                      # image labels\n",
        "            test_size = valid_size,      # test size\n",
        "            random_state = random_seed,  # random seed for reproducibility\n",
        "            shuffle = True,              # shuffle data before splitting into training and validation sets\n",
        "            stratify = labels            # maintain equal class representation in training and validation sets\n",
        "        )\n",
        "        \n",
        "        if subset:\n",
        "            subset_size = 256.0 / len(train_fnames)\n",
        "            _, train_fnames_subset, _, train_labels_subset = train_test_split(\n",
        "                train_fnames,\n",
        "                train_labels,\n",
        "                test_size = subset_size,\n",
        "                random_state = random_seed,\n",
        "                shuffle = True,\n",
        "                stratify = train_labels\n",
        "            )\n",
        "            _, valid_fnames_subset, _, valid_labels_subset = train_test_split(\n",
        "                valid_fnames,\n",
        "                valid_labels,\n",
        "                test_size = subset_size,\n",
        "                random_state = random_seed,\n",
        "                shuffle = True,\n",
        "                stratify = valid_labels\n",
        "            )\n",
        "            #subset_labels = train_labels_subset + valid_labels_subset\n",
        "            #_, class_counts = np.unique(subset_labels, return_counts=True)\n",
        "            #self.__class_counts = class_counts.tolist()\n",
        "            train_fnames = train_fnames_subset\n",
        "            train_labels = train_labels_subset\n",
        "            valid_fnames = valid_fnames_subset\n",
        "            valid_labels = valid_labels_subset\n",
        "\n",
        "        # ToDo: filter data\n",
        "        if mapping is not None:\n",
        "            train_fnames, train_labels = self.__map_data(train_fnames, train_labels)\n",
        "            valid_fnames, valid_labels = self.__map_data(valid_fnames, valid_labels)\n",
        "\n",
        "        _, class_counts = np.unique(train_labels + valid_labels, return_counts=True)\n",
        "\n",
        "        self.__class_counts = class_counts.tolist()\n",
        "        self.__train_fnames = train_fnames\n",
        "        self.__train_labels = train_labels\n",
        "        self.__valid_fnames = valid_fnames\n",
        "        self.__valid_labels = valid_labels\n",
        "\n",
        "        class_weights = 1. / np.array(self.__class_counts)\n",
        "        class_weights = class_weights / np.sum(class_weights)\n",
        "        self.__class_weights = class_weights.tolist()\n",
        "\n",
        "    @property\n",
        "    def valid_size(self) -> float:\n",
        "        return self.__valid_size\n",
        "\n",
        "    @property\n",
        "    def class_names(self) -> List[str]:\n",
        "        return self.__class_names\n",
        "    \n",
        "    @property\n",
        "    def class_counts(self) -> List[int]:\n",
        "        return self.__class_counts\n",
        "\n",
        "    @property\n",
        "    def class_weights(self) -> List[float]:\n",
        "        return self.__class_weights\n",
        "\n",
        "    @property\n",
        "    def train_fnames(self) -> List[str]:\n",
        "        return self.__train_fnames\n",
        "\n",
        "    @property\n",
        "    def train_labels(self) -> List[int]:\n",
        "        return self.__train_labels\n",
        "\n",
        "    @property\n",
        "    def valid_fnames(self) -> List[str]:\n",
        "        return self.__valid_fnames\n",
        "\n",
        "    @property\n",
        "    def valid_labels(self) -> List[int]:\n",
        "        return self.__valid_labels\n",
        "\n",
        "    @property\n",
        "    def mapping(self) -> List[int]:\n",
        "        return self.__mapping\n",
        "\n",
        "    def get_train_data_loader(\n",
        "        self,\n",
        "        transform: Iterable[Callable],\n",
        "        batch_size = 16, \n",
        "        num_workers = 2,\n",
        "        use_random_sampler: bool = False\n",
        "    ) -> DataLoader:\n",
        "        return DataLoader(\n",
        "            dataset = self.__get_train_dataset(transform),\n",
        "            sampler = self.__get_sampler() if use_random_sampler else None,\n",
        "            batch_size = batch_size,\n",
        "            num_workers = num_workers,\n",
        "            shuffle = not use_random_sampler\n",
        "        )\n",
        "\n",
        "    def get_valid_data_loader(\n",
        "        self,\n",
        "        transform: Iterable[Callable],\n",
        "        batch_size = 16, \n",
        "        num_workers = 2\n",
        "    ) -> DataLoader:\n",
        "        return DataLoader(\n",
        "            dataset = self.__get_valid_dataset(transform),\n",
        "            batch_size = batch_size,\n",
        "            num_workers = num_workers,\n",
        "            shuffle = False\n",
        "        )\n",
        "\n",
        "    def __map_data(self, fnames, labels):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        count = len(self.__class_names)\n",
        "        labels = [self.__mapping[label] for label in labels]\n",
        "        mapflt = [label < count for label in labels]\n",
        "        labels = [label for (label, fltval) in zip(labels, mapflt) if fltval]\n",
        "        fnames = [fname for (fname, fltval) in zip(fnames, mapflt) if fltval]\n",
        "        return fnames, labels\n",
        "\n",
        "    def __get_sampler(self) -> Sampler:\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        weights = [self.__class_weights[e] for e in self.__train_labels]\n",
        "        return WeightedRandomSampler(torch.DoubleTensor(weights), len(weights))\n",
        "\n",
        "    def __get_train_dataset(\n",
        "        self, \n",
        "        transform: Iterable[Callable]\n",
        "    ) -> Dataset:\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        return KF13Dataset(\n",
        "            image_root = self.__image_root,\n",
        "            fnames = self.__train_fnames,\n",
        "            labels = self.__train_labels,\n",
        "            transform = transform\n",
        "        )\n",
        "\n",
        "    def __get_valid_dataset(\n",
        "        self, \n",
        "        transform: Iterable[Callable]\n",
        "    ) -> Dataset:\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        return KF13Dataset(\n",
        "            image_root = self.__image_root,\n",
        "            fnames = self.__valid_fnames,\n",
        "            labels = self.__valid_labels,\n",
        "            transform = transform\n",
        "        )\n",
        "\n",
        "\n",
        "class KF13Datastore:\n",
        "    \"\"\"\n",
        "    This class parses the KenyanFood13's test.csv and train.csv files. It has\n",
        "    properties to retrieve the following.\n",
        "\n",
        "          - a list of class names (classes)\n",
        "          - a list of class counts (class_counts)\n",
        "          - a list of test filenames (test_fnames)\n",
        "          - a dictionary of filenames for each class (library)\n",
        "\n",
        "    In addition, it has a method to create KF13TrainingData instances, which\n",
        "    splits the training data into training and validation sets. In addition,\n",
        "    this method can group classes together and create datasets to train a two\n",
        "    stage classifer.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, data_root, valid_size = 0.2, random_seed = 42):\n",
        "        self.__valid_size = valid_size\n",
        "        self.__random_seed = random_seed\n",
        "\n",
        "        # the root path of the images\n",
        "        image_root = os.path.join(data_root, 'images', 'images')\n",
        "        self.__image_root = image_root\n",
        "        \n",
        "        # parse the test CSV file to obtain filenames (labels are not given)\n",
        "        test_data_frame = self.__parse_data_file(data_root, 'test.csv')\n",
        "        test_fnames = test_data_frame.values[:,0].tolist()\n",
        "        self.__test_fnames = test_fnames\n",
        "        \n",
        "        # parse the train CSV file to obtain filenames and labels       \n",
        "        train_data_frame = self.__parse_data_file(data_root, 'train.csv')\n",
        "        fnames = train_data_frame.values[:,0].tolist()\n",
        "        cnames = train_data_frame.values[:,1].tolist()\n",
        "        \n",
        "        # get the classes and class counts\n",
        "        class_names, class_counts = np.unique(cnames, return_counts=True)\n",
        "        class_names = class_names.tolist()\n",
        "        class_counts = class_counts.tolist()\n",
        "        num_classes = len(class_names)\n",
        "        self.__class_names = class_names\n",
        "        self.__class_counts = class_counts\n",
        "        \n",
        "        # create a dictionary of text labels to integer labels\n",
        "        cname_to_label = {\n",
        "            key : val for key, val in zip(class_names, np.arange(num_classes))\n",
        "        }\n",
        "        self.__cname_to_label = cname_to_label\n",
        "\n",
        "        # convert class names to labels (their numeric equivalents)\n",
        "        labels = [cname_to_label[cname] for cname in cnames]\n",
        "        self.__train_fnames = fnames\n",
        "        self.__train_labels = labels\n",
        "\n",
        "        # create a dictionary that maps class names to labels\n",
        "        self.__class_dict = {\n",
        "            name: label \n",
        "            for (name, label) \n",
        "            in zip(class_names, range(len(class_names)))\n",
        "        }\n",
        "\n",
        "        # create a dictionary that groups fnames according to their labels\n",
        "        self.__library = {\n",
        "            key : [fname for fname, label in zip(fnames, labels) if label == key] \n",
        "            for key in range(num_classes)\n",
        "        }\n",
        "\n",
        "    @property\n",
        "    def image_root(self) -> List[str]:\n",
        "        return self.__image_root\n",
        "\n",
        "    @property\n",
        "    def class_names(self) -> List[str]:\n",
        "        return self.__class_names\n",
        "    \n",
        "    @property\n",
        "    def class_counts(self) -> List[int]:\n",
        "        return self.__class_counts\n",
        "    \n",
        "    @property\n",
        "    def class_dict(self) -> Dict[str, int]:\n",
        "        return self.__class_dict\n",
        "\n",
        "    @property\n",
        "    def library(self) -> Dict[int, List[str]]:\n",
        "          return self.__library\n",
        "\n",
        "    def create_stages(\n",
        "        self,\n",
        "        stage_info: List[Union[str, Tuple[str, List[str]]]]\n",
        "    ):\n",
        "        from queue import Queue\n",
        "        stages = []\n",
        "        queue = Queue(maxsize = len(self.__class_names))\n",
        "        queue.put(stage_info)\n",
        "        while not queue.empty():\n",
        "            names = []\n",
        "            items = queue.get()\n",
        "            mapping = [len(items)] * len(self.__class_names)\n",
        "            for idx, item in enumerate(items):\n",
        "                if isinstance(item, str):\n",
        "                    names.append(item)\n",
        "                    mapping[self.__class_dict[item]] = idx\n",
        "                else:\n",
        "                    queue.put(item[1])\n",
        "                    names.append(item[0])\n",
        "                    for name in item[1]:\n",
        "                        mapping[self.__class_dict[name]] = idx\n",
        "            stages.append((names, mapping))\n",
        "        return stages\n",
        "\n",
        "    def get_training_data(self, subset:bool=False) -> KF13TrainingData:\n",
        "        return KF13TrainingData(\n",
        "            image_root = self.__image_root,\n",
        "            class_names = self.__class_names,\n",
        "            fnames = self.__train_fnames, \n",
        "            labels = self.__train_labels,\n",
        "            subset = subset,\n",
        "            valid_size = self.__valid_size, \n",
        "            random_seed = self.__random_seed\n",
        "        )\n",
        "\n",
        "    def get_two_stage_training_data(\n",
        "        self,\n",
        "        stage_info: List[Union[str, Tuple[str, List[str]]]]\n",
        "    ) -> List[Tuple[str, KF13TrainingData]]:\n",
        "\n",
        "        # parse the stage information\n",
        "        stages = self.create_stages(stage_info)\n",
        "\n",
        "        # create stage names\n",
        "        names = [\"Stage1\"]\n",
        "        for idx in range(len(stages) - 1):\n",
        "            names.append(\"Stage2\" + chr(ord('a'[0]) + idx))\n",
        "\n",
        "        # loop through each stage\n",
        "        data = []\n",
        "        for name, stage in zip(names, stages):\n",
        "            data.append((\n",
        "                name,\n",
        "                KF13TrainingData(\n",
        "                    image_root = self.__image_root,\n",
        "                    class_names = stage[0],\n",
        "                    fnames = self.__train_fnames,\n",
        "                    labels = self.__train_labels,\n",
        "                    mapping = stage[1],\n",
        "                    valid_size = self.__valid_size, \n",
        "                    random_seed = self.__random_seed,\n",
        "                )\n",
        "            ))\n",
        "        \n",
        "        return data\n",
        "\n",
        "    def get_test_data_loader(\n",
        "        self,\n",
        "        transform: Iterable[Callable],\n",
        "        batch_size = 16, \n",
        "        num_workers = 2\n",
        "    ) -> DataLoader:\n",
        "        return DataLoader(\n",
        "            dataset = self.__get_test_dataset(transform),\n",
        "            batch_size = batch_size,\n",
        "            num_workers = num_workers,\n",
        "            shuffle = False\n",
        "        )\n",
        "\n",
        "    def __parse_data_file(self, data_root, file):\n",
        "        return pd.read_csv(\n",
        "            os.path.join(data_root, file), \n",
        "            delimiter=',', \n",
        "            dtype={'id': 'str'}, \n",
        "            engine='python'\n",
        "        )\n",
        "\n",
        "    def __get_test_dataset(\n",
        "        self, \n",
        "        transform: Iterable[Callable]\n",
        "    ) -> Dataset:\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        return KF13Dataset(\n",
        "            image_root = self.__image_root,\n",
        "            fnames = self.__test_fnames,\n",
        "            labels = None,\n",
        "            transform = transform\n",
        "        )"
      ]
    },
    {
      "source": [
        "### Data Imbalance\n",
        "\n",
        "The following table enumerates the number of images in each class. The most represented class, chapati, has approximately five times more images than the least represented class, kukuchoma.\n",
        "\n",
        "|Class|Images|\n",
        "|:---|:---:|\n",
        "|bhaji|632|\n",
        "|chapati|862|\n",
        "|githeri|479|\n",
        "|kachumbari|494|\n",
        "|kukuchoma|173|\n",
        "|mandazi|620|\n",
        "|masalachips|438|\n",
        "|matoke|483|\n",
        "|mukimo|212|\n",
        "|nyamachoma|784|\n",
        "|pilau|329|\n",
        "|sukumawiki|402|\n",
        "|ugali|628|\n",
        "\n",
        "This project will explore two approaches of handling this data imbalance.\n",
        "\n",
        "1. Use a weighted loss function.\n",
        "2. Use a weighted random sampler."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DIApproach(Enum):\n",
        "    NONE = auto()\n",
        "    WEIGHTED_LOSS_FUNCTION = auto()\n",
        "    WEIGHTED_RANDOM_SAMPLER = auto()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_mean_std(data_loader=None):\n",
        "    \n",
        "    if data_loader is None:\n",
        "        \"\"\"\n",
        "        Returns the mean and standard deviation used by the pretrained\n",
        "        classification models.\n",
        "        \"\"\"\n",
        "\n",
        "        mean = [0.485, 0.456, 0.406] \n",
        "        std = [0.229, 0.224, 0.225]\n",
        "    \n",
        "    else:\n",
        "        \"\"\"\n",
        "        Computes the mean and standard deviation of the images returned\n",
        "        by the specified data loader. \n",
        "        \n",
        "        For comparision, the mean and standard deviation of the KenyanFood13\n",
        "        images using the train_dataset and preprocess transforms is as follows.\n",
        "        \n",
        "            mean = [0.5778, 0.4631, 0.3471], \n",
        "            std = [0.2380, 0.2461, 0.2464]):\n",
        "        \"\"\"\n",
        "        \n",
        "        std = 0.\n",
        "        mean = 0.\n",
        "        for images, _ in data_loader:\n",
        "            batch_samples = images.size(0)\n",
        "            images = images.view(batch_samples, images.size(1), -1)\n",
        "            std += images.std(2).sum(0)\n",
        "            mean += images.mean(2).sum(0)\n",
        "        std /= len(data_loader.dataset)\n",
        "        mean /= len(data_loader.dataset)\n",
        "\n",
        "    return mean, std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RandomGaussianNoise(object):\n",
        "    def __init__(self, p=0.5, mean=0., std=1.):\n",
        "        self.p = p\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "        \n",
        "    def __call__(self, tensor):\n",
        "        if random.random() < self.p:\n",
        "            return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
        "        return tensor\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + f\"(p={self.p}, mean={self.mean}, std={self.std})\"\n",
        "\n",
        "\n",
        "class ImageTransforms:\n",
        "    \"\"\"\n",
        "    This utility class has methods to create transforms used to train and evaluate a model as\n",
        "    well as visualize images.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "            self, \n",
        "            resize = 256, \n",
        "            crop_size = 224, \n",
        "            mean = [0.485, 0.456, 0.406], \n",
        "            std = [0.229, 0.224, 0.225],\n",
        "            config = DataAugConfig()\n",
        "        ):\n",
        "        self.__resize = resize\n",
        "        self.__crop_size = crop_size\n",
        "        self.__mean = mean\n",
        "        self.__std = std\n",
        "        self.__config = config\n",
        "\n",
        "    def preprocess(self, augment=False):\n",
        "        \"\"\"\n",
        "        These transformations convert PIL images to uniformly sized tensors whose dimensions\n",
        "        are crop_size x crop_size pixels. If the augment parameter is True, then the following\n",
        "        data augmentation transforms are applied: color jitter, horizontal flip, vertical flip,\n",
        "        rotation, translation, scaling, and erasing.\n",
        "        \"\"\"\n",
        "        return transforms.Compose(self.__create_transform_list(normalize=False, augment=augment))\n",
        "    \n",
        "    def common(self):\n",
        "        \"\"\"\n",
        "        These transformations convert PIL images to uniformly sized tensors whose dimensions\n",
        "        are crop_size x crop_size pixels and values are normalized by the mean and standard\n",
        "        deviation.\n",
        "        \"\"\"\n",
        "        return transforms.Compose(self.__create_transform_list(normalize=True, augment=False))\n",
        "    \n",
        "    def augment(self):\n",
        "        \"\"\"\n",
        "        These transformations convert PIL images to uniformly sized tensors whose dimensions\n",
        "        are crop_size x crop_size pixels and values are normalized by the mean and standard\n",
        "        deviation with the following data random augmentations: color jitter, horizontal flip,\n",
        "        vertical flip, rotation, translation, scaling, and erasing.\n",
        "        \"\"\"\n",
        "        return transforms.Compose(self.__create_transform_list(normalize=True, augment=True))\n",
        "\n",
        "    def __create_transform_list(self, normalize, augment):\n",
        "        tlist = []\n",
        "\n",
        "        # resize before data augmentation to reduce execution time\n",
        "        tlist.append(transforms.Resize(\n",
        "            size = self.__resize, \n",
        "            interpolation = PIL.Image.BILINEAR\n",
        "        ))\n",
        "\n",
        "        if augment:\n",
        "            # apply rotation before center cropping to avoid \"corner voids\"\n",
        "            tlist.extend(self.__get_color_jitter())\n",
        "            tlist.extend(self.__get_random_vertical_flip())\n",
        "            tlist.extend(self.__get_random_horizontal_flip())\n",
        "            tlist.extend(self.__get_random_affine())\n",
        "\n",
        "        tlist.append(transforms.CenterCrop(self.__crop_size))\n",
        "        tlist.append(transforms.ToTensor())\n",
        "\n",
        "        if normalize:\n",
        "            tlist.append(transforms.Normalize(self.__mean, self.__std, inplace=True))\n",
        "\n",
        "        if augment:\n",
        "            tlist.extend(self.__get_random_erasing())\n",
        "            tlist.extend(self.__get_random_noise())\n",
        "\n",
        "        return tlist\n",
        "\n",
        "    def __get_color_jitter(self):\n",
        "        tlist = []\n",
        "        if self.__config.color_enabled:\n",
        "            tlist.append(transforms.ColorJitter(\n",
        "                brightness = self.__config.color_brightness, \n",
        "                contrast = self.__config.color_contrast, \n",
        "                saturation = self.__config.color_saturation, \n",
        "                hue = self.__config.color_hue\n",
        "            ))\n",
        "        return tlist\n",
        "\n",
        "    def __get_random_vertical_flip(self):\n",
        "        tlist = []\n",
        "        if self.__config.vert_flip_prob > 0:\n",
        "            tlist.append(transforms.RandomVerticalFlip(\n",
        "                p=self.__config.vert_flip_prob\n",
        "            ))\n",
        "        return tlist \n",
        "\n",
        "    def __get_random_horizontal_flip(self):\n",
        "        tlist = []\n",
        "        if self.__config.horz_flip_prob > 0:\n",
        "            tlist.append(transforms.RandomHorizontalFlip(\n",
        "                p=self.__config.horz_flip_prob\n",
        "            ))\n",
        "        return tlist \n",
        "\n",
        "    def __get_random_affine(self):\n",
        "        tlist = []\n",
        "        if self.__config.affine_enabled:\n",
        "            tlist.append(transforms.RandomAffine(\n",
        "                degrees = self.__config.affine_rotation,\n",
        "                translate = self.__null_check(self.__config.affine_translate),\n",
        "                scale = self.__null_check(self.__config.affine_scale),\n",
        "                shear = self.__null_check(self.__config.affine_shear),\n",
        "                resample=PIL.Image.BILINEAR # ToDo: Test BICUBIC\n",
        "            ))\n",
        "        return tlist\n",
        "\n",
        "    def __get_random_erasing(self):\n",
        "        tlist = []\n",
        "        if self.__config.erasing_prob > 0:\n",
        "            tlist.append(transforms.RandomErasing(\n",
        "                p = self.__config.erasing_prob,\n",
        "                scale = self.__config.erasing_scale,\n",
        "                ratio = self.__config.erasing_ratio,\n",
        "                value = \"random\" if self.__config.erasing_random else 0,\n",
        "                inplace = True\n",
        "            ))\n",
        "        return tlist\n",
        "\n",
        "    def __get_random_noise(self):\n",
        "        tlist = []\n",
        "        if self.__config.noise_prob > 0:\n",
        "            tlist.append(RandomGaussianNoise(\n",
        "                p = self.__config.noise_prob,\n",
        "                mean = self.__config.noise_mean,\n",
        "                std = self.__config.noise_std\n",
        "            ))\n",
        "        return tlist\n",
        "\n",
        "    def __null_check(self, tuple: Tuple[float, float]) -> Optional[Tuple[float, float]]:\n",
        "        return tuple if tuple != (0.0, 0.0) else None"
      ]
    },
    {
      "source": [
        "### Data Loader Tests\n",
        "\n",
        "#### Without Weighted Random Sample\n",
        "\n",
        "Since I have not used the weighted random sampler, I wanted to run tests to explore its behavior. The `analyze_data_loader` function iterates through the data loader and counts the number of occurences of each class type. This function returns the tally as a list. The first test is on a data loader without the weighted random sampler. As expected, the tally's counts equal the number of training images for each class.\n",
        "\n",
        "<center>\n",
        "\n",
        "|Class|Training|Loader|\n",
        "|:---|:---:|:---:|\n",
        "|bhaji|506|506|\n",
        "|chapati|690|690|\n",
        "|githeri|383|383|\n",
        "|kachumbari|395|395|\n",
        "|kukuchoma|138|138|\n",
        "|mandazi|496|496|\n",
        "|masalachips|350|350|\n",
        "|matoke|386|386|\n",
        "|mukimo|170|170|\n",
        "|nyamachoma|627|627|\n",
        "|pilau|263|263|\n",
        "|sukumawiki|322|322|\n",
        "|ugali|502|502|\n",
        "\n",
        "</center>\n",
        "\n",
        "#### With Weighted Random Sample\n",
        "\n",
        "Repeating the previous experiment on a data loader with the random weighted sampler still had significant variations in the number of loaded images per class. Consequently, I decided to average tallies over many complete data loader cycles. These averages converged to image samples per class that were approximately equal (within 2%). The results of these experiments are documented below.\n",
        "\n",
        "<center>\n",
        "\n",
        "|Iteration|Average Tally|\n",
        "|:---:|:---:|\n",
        "|1|\\[398, 421, 399, 374, 418, 384, 433, 384, 415, 408, 386, 402, 406]|\n",
        "|2|\\[403, 410, 420, 379, 406, 402, 398, 398, 413, 399, 401, 399, 404]|\n",
        "|3|\\[408, 411, 419, 381, 409, 403, 399, 386, 404, 393, 405, 407, 403]|\n",
        "|4|\\[401, 406, 413, 383, 416, 401, 402, 384, 410, 398, 408, 407, 401]|\n",
        "|5|\\[402, 405, 408, 381, 411, 398, 405, 390, 411, 400, 406, 404, 407]|\n",
        "|6|\\[401, 402, 406, 379, 412, 399, 406, 397, 406, 397, 405, 408, 411]|\n",
        "|7|\\[402, 399, 410, 380, 412, 400, 404, 401, 404, 397, 405, 406, 408]|\n",
        "|8|\\[399, 404, 412, 385, 410, 401, 401, 404, 406, 392, 404, 407, 405]|\n",
        "|9|\\[402, 406, 409, 387, 411, 399, 400, 404, 405, 392, 406, 406, 403]|\n",
        "|10|\\[401, 407, 409, 389, 410, 399, 400, 402, 402, 396, 404, 407, 403]|\n",
        "|.|.|\n",
        "|.|.|\n",
        "|.|.|\n",
        "|64|\\[403, 407, 404, 402, 402, 403, 399, 397, 408, 400, 401, 402, 402]|\n",
        "|65|\\[403, 408, 404, 401, 402, 402, 399, 397, 408, 400, 402, 402, 401]|\n",
        "|66|\\[403, 407, 404, 402, 402, 402, 399, 397, 407, 400, 401, 402, 401]|\n",
        "|67|\\[403, 407, 404, 402, 402, 403, 399, 397, 407, 400, 401, 402, 401]|\n",
        "|68|\\[403, 407, 404, 402, 401, 403, 399, 398, 407, 400, 401, 402, 401]|\n",
        "\n",
        "</center>"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_data_loader(data_loader: DataLoader) -> List[int]:\n",
        "    tally = [0] * 13\n",
        "    for _, targets in data_loader:\n",
        "        for target in targets:\n",
        "            tally[target] += 1\n",
        "    return tally\n",
        "\n",
        "\n",
        "# Test Data Loader w/o Weighted Random Sampler\n",
        "def run_data_loader_test_1():\n",
        "    image_transforms = ImageTransforms()\n",
        "    datastore = KF13Datastore(data_dir)\n",
        "    data = datastore.get_training_data()\n",
        "\n",
        "    print(\"Tally using data loader w/o weighted random sampler\")\n",
        "    print()\n",
        "    print(\"|Class|Training Images|Loader Images|\")\n",
        "    print(\"|:---|:---:|:---:|\")\n",
        "\n",
        "    tally = analyze_data_loader(data.get_train_data_loader(\n",
        "        transform = image_transforms.preprocess(),\n",
        "        batch_size = 32, \n",
        "        num_workers = 12,\n",
        "        use_random_sampler = False\n",
        "    ))\n",
        "\n",
        "    training_size = 1. - data.valid_size\n",
        "    class_names = data.class_names\n",
        "    class_counts = [int(x * training_size + 0.5) for x in data.class_counts]\n",
        "    for class_name, class_count, loader_count in zip(class_names, class_counts, tally):\n",
        "        print(f\"|{class_name}|{class_count}|{loader_count}|\")\n",
        "\n",
        "# Test Data Loader w/ Weighted Random Sampler\n",
        "def run_data_loader_test_2():\n",
        "    image_transforms = ImageTransforms()\n",
        "    datastore = KF13Datastore(data_dir)\n",
        "    data = datastore.get_training_data()\n",
        "\n",
        "    data_loader = data.get_train_data_loader(\n",
        "        transform = image_transforms.preprocess(),\n",
        "        batch_size = 32, \n",
        "        num_workers = 12,\n",
        "        use_random_sampler = True\n",
        "    )\n",
        "\n",
        "    print()\n",
        "    print(\"Averaging tallies using data loader w/ weighted random sampler\")\n",
        "    print()\n",
        "    print(\"|Iteration|Average Tally|\")\n",
        "    print(\"|:---:|:---:|\")\n",
        "    tally_prv = [0] * 13\n",
        "    tally_sum = [0] * 13\n",
        "    for i in range(1, 101):\n",
        "        tally = analyze_data_loader(data_loader)\n",
        "        tally_sum = [sum(counts) for counts in zip(tally_sum, tally)]\n",
        "        tally_ave = [int(count / float(i) + 0.5) for count in tally_sum]\n",
        "        if (tally_ave == tally_prv):\n",
        "            break\n",
        "        tally_prv = tally_ave\n",
        "        print(f\"|{i}|\\\\{tally_ave}|\")\n",
        "\n",
        "#run_data_loader_test_1()\n",
        "#run_data_loader_test_2()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaqNIzTm3-5z"
      },
      "source": [
        "## <font style=\"color:green\">2. Configuration [5 Points]</font>\n",
        "\n",
        "Define your configuration in this section.\n",
        "\n",
        "For example,\n",
        "\n",
        "```\n",
        "@dataclass\n",
        "class TrainingConfiguration:\n",
        "    '''\n",
        "    Describes configuration of the training process\n",
        "    '''\n",
        "    batch_size: int = 10 \n",
        "    epochs_count: int = 50  \n",
        "    init_learning_rate: float = 0.1  # initial learning rate for lr scheduler\n",
        "    log_interval: int = 5  \n",
        "    test_interval: int = 1  \n",
        "    data_root: str = \"/kaggle/input/pytorch-opencv-course-classification/\" \n",
        "    num_workers: int = 2  \n",
        "    device: str = 'cuda'  \n",
        "    \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__6DBlJp3-5z"
      },
      "source": [
        "## <font style=\"color:blue\">Assignment Response</font>\n",
        "\n",
        "Since I am using the **trainer** module, I made minor modifications to the <u>configuration.py</u> file. In addition, I created a master _MasterConfig_ data class that encapsulates the individual configuration data classes. Lastly, I created helper functions to instantiate the _MasterConfig_ class with experiment-specific overrides.\n",
        "\n",
        "The following is the output of the `create_master_config` method w/o any parameter overrides.\n",
        "```\n",
        "MasterConfig(\n",
        "    system=SystemConfig(\n",
        "        proj_dir='./project2',\n",
        "        seed=42,\n",
        "        cudnn_deterministic=True,\n",
        "        cudnn_benchmark_enabled=False\n",
        "    ),\n",
        "    data_aug=DataAugConfig(\n",
        "        color_enabled=True,\n",
        "        color_brightness=(0.85, 1.15),\n",
        "        color_contrast=(0.5, 1.5),\n",
        "        color_saturation=(0.5, 2.0),\n",
        "        color_hue=(-0.03, 0.03),\n",
        "        horz_flip_prob=0.5,\n",
        "        vert_flip_prob=0.5,\n",
        "        affine_enabled=True,\n",
        "        affine_rotation=45,\n",
        "        affine_translate=(0.1, 0.1),\n",
        "        affine_scale=(0.9, 1.1),\n",
        "        erasing_prob=0.5,\n",
        "        erasing_scale=(0.02, 0.33),\n",
        "        erasing_ratio=(0.3, 3.3)\n",
        "    ),\n",
        "    dataset=DatasetConfig(\n",
        "        data_dir='./project2/data',\n",
        "        valid_size=0.2,\n",
        "        train_transforms=Compose(\n",
        "            Resize(size=256, interpolation=PIL.Image.BILINEAR)\n",
        "            ColorJitter(brightness=(0.85, 1.15), contrast=(0.5, 1.5), saturation=(0.5, 2.0), hue=(-0.03, 0.03))\n",
        "            RandomVerticalFlip(p=0.5)\n",
        "            RandomHorizontalFlip(p=0.5)\n",
        "            RandomAffine(degrees=[-45.0, 45.0], translate=(0.1, 0.1), scale=(0.9, 1.1), resample=PIL.Image.BILINEAR)\n",
        "            CenterCrop(size=(224, 224))\n",
        "            ToTensor()\n",
        "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "            RandomErasing()\n",
        "        ),\n",
        "        test_transforms=Compose(\n",
        "            Resize(size=256, interpolation=PIL.Image.BILINEAR)\n",
        "            CenterCrop(size=(224, 224))\n",
        "            ToTensor()\n",
        "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ),\n",
        "        visual_transforms=Compose(\n",
        "            Resize(size=256, interpolation=PIL.Image.BILINEAR)\n",
        "            CenterCrop(size=(224, 224))\n",
        "            ToTensor()\n",
        "        ),\n",
        "        visual_aug_transforms=Compose(\n",
        "            Resize(size=256, interpolation=PIL.Image.BILINEAR)\n",
        "            ColorJitter(brightness=(0.85, 1.15), contrast=(0.5, 1.5), saturation=(0.5, 2.0), hue=(-0.03, 0.03))\n",
        "            RandomVerticalFlip(p=0.5)\n",
        "            RandomHorizontalFlip(p=0.5)\n",
        "            RandomAffine(degrees=[-45.0, 45.0], translate=(0.1, 0.1), scale=(0.9, 1.1), resample=PIL.Image.BILINEAR)\n",
        "            CenterCrop(size=(224, 224))\n",
        "            ToTensor()\n",
        "            RandomErasing()\n",
        "        )\n",
        "    ),\n",
        "    data_loader=DataLoaderConfig(\n",
        "        batch_size=32,\n",
        "        num_workers=4\n",
        "    ),\n",
        "    optimizer=OptimizerConfig(\n",
        "        learning_rate=0.001,\n",
        "        momentum=0.9,\n",
        "        weight_decay=0.0001,\n",
        "        betas=(0.9, 0.999)\n",
        "    ),\n",
        "    scheduler=SchedulerConfig(\n",
        "        gamma=0.1,\n",
        "        step_size=10,\n",
        "        milestones=(20, 30, 40),\n",
        "        patience=10,\n",
        "        threshold=0.0001\n",
        "    ),\n",
        "    trainer=TrainerConfig(\n",
        "        device='cuda',\n",
        "        training_epochs=50,\n",
        "        weighted_loss_fn=True,\n",
        "        progress_bar=True,\n",
        "        model_dir='models',\n",
        "        model_saving_period=0,\n",
        "        visualizer_dir='runs',\n",
        "        stop_loss_epochs=0,\n",
        "        stop_acc_epochs=0,\n",
        "        stop_acc_ema_alpha=0.3,\n",
        "        stop_acc_threshold=2.0\n",
        "    )\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8clVVKLOpuwj"
      },
      "source": [
        "def get_config_value(param: str, default, config_overrides: Dict[str, Any]):\n",
        "    # ToDo: Ask Ryan why doesn't the following work.\n",
        "    # return config_overrides.get(param) if not None else default\n",
        "    value = config_overrides.get(param)\n",
        "    if value is None:\n",
        "        value = default\n",
        "    return value\n",
        "\n",
        "\n",
        "def create_system_config() -> SystemConfig:\n",
        "    return SystemConfig(\n",
        "        proj_dir = proj_dir\n",
        "    )\n",
        "\n",
        "\n",
        "def create_data_aug_config(**config_overrides) -> DataAugConfig:\n",
        "    config = DataAugConfig()\n",
        "    return DataAugConfig(\n",
        "        color_enabled = get_config_value(\n",
        "            \"data_aug_color_enabled\", \n",
        "            config.color_enabled, \n",
        "            config_overrides\n",
        "        ),\n",
        "        color_brightness = get_config_value(\n",
        "            \"data_aug_color_brightness\",\n",
        "            config.color_brightness,\n",
        "            config_overrides\n",
        "        ),\n",
        "        color_contrast = get_config_value(\n",
        "            \"data_aug_color_contrast\",\n",
        "            config.color_contrast,\n",
        "            config_overrides\n",
        "        ),\n",
        "        color_saturation = get_config_value(\n",
        "            \"data_aug_color_saturation\",\n",
        "            config.color_saturation,\n",
        "            config_overrides\n",
        "        ),\n",
        "        color_hue = get_config_value(\n",
        "            \"data_aug_color_hue\",\n",
        "            config.color_hue,\n",
        "            config_overrides\n",
        "        ),\n",
        "        horz_flip_prob = get_config_value(\n",
        "            \"data_aug_horz_flip_prob\",\n",
        "            config.horz_flip_prob,\n",
        "            config_overrides\n",
        "        ),\n",
        "        vert_flip_prob = get_config_value(\n",
        "            \"data_aug_vert_flip_prob\",\n",
        "            config.vert_flip_prob,\n",
        "            config_overrides\n",
        "        ),\n",
        "        affine_enabled = get_config_value(\n",
        "            \"data_aug_affine_enabled\",\n",
        "            config.affine_enabled,\n",
        "            config_overrides\n",
        "        ),\n",
        "        affine_rotation = get_config_value(\n",
        "            \"data_aug_affine_rotation\",\n",
        "            config.affine_rotation,\n",
        "            config_overrides\n",
        "        ),\n",
        "        affine_translate = get_config_value(\n",
        "            \"data_aug_affine_translate\",\n",
        "            config.affine_translate,\n",
        "            config_overrides\n",
        "        ),\n",
        "        affine_scale = get_config_value(\n",
        "            \"data_aug_affine_scale\",\n",
        "            config.affine_scale,\n",
        "            config_overrides\n",
        "        ),\n",
        "        affine_shear = get_config_value(\n",
        "            \"data_aug_affine_shear\",\n",
        "            config.affine_shear,\n",
        "            config_overrides\n",
        "        ),\n",
        "        erasing_prob = get_config_value(\n",
        "            \"data_aug_erasing_prob\",\n",
        "            config.erasing_prob,\n",
        "            config_overrides\n",
        "        ),\n",
        "        erasing_scale = get_config_value(\n",
        "            \"data_aug_erasing_scale\",\n",
        "            config.erasing_scale,\n",
        "            config_overrides\n",
        "        ),\n",
        "        erasing_ratio = get_config_value(\n",
        "            \"data_aug_erasing_ratio\",\n",
        "            config.erasing_ratio,\n",
        "            config_overrides\n",
        "        ),\n",
        "        erasing_random = get_config_value(\n",
        "            \"data_aug_erasing_random\",\n",
        "            config.erasing_random,\n",
        "            config_overrides\n",
        "        ),\n",
        "        noise_prob = get_config_value(\n",
        "            \"data_aug_noise_prob\",\n",
        "            config.noise_prob,\n",
        "            config_overrides\n",
        "        ),\n",
        "        noise_mean = get_config_value(\n",
        "            \"data_aug_noise_mean\",\n",
        "            config.noise_mean,\n",
        "            config_overrides\n",
        "        ),\n",
        "        noise_std = get_config_value(\n",
        "            \"data_aug_noise_std\",\n",
        "            config.noise_std,\n",
        "            config_overrides\n",
        "        ),\n",
        "    )\n",
        "\n",
        "\n",
        "def create_dataset_config(\n",
        "    resize: int = 256, \n",
        "    crop_size: int = 224,\n",
        "    data_aug_config = DataAugConfig()\n",
        ") -> DatasetConfig:\n",
        "    mean, std = get_mean_std()\n",
        "    transforms = ImageTransforms(\n",
        "        resize = resize, \n",
        "        crop_size = crop_size, \n",
        "        mean = mean, \n",
        "        std = std,\n",
        "        config = data_aug_config\n",
        "    )\n",
        "    return DatasetConfig(\n",
        "        data_dir = data_dir,\n",
        "        test_transforms = transforms.common(),\n",
        "        train_transforms = transforms.augment(),\n",
        "        visual_transforms = transforms.preprocess(augment=False),\n",
        "        visual_aug_transforms = transforms.preprocess(augment=True)\n",
        "    )\n",
        "\n",
        "\n",
        "def create_data_loader_config(**config_overrides) -> DataLoaderConfig:\n",
        "    config = DataLoaderConfig()\n",
        "    return DataLoaderConfig(\n",
        "        batch_size = get_config_value(\n",
        "            \"data_loader_batch_size\",\n",
        "            config.batch_size,\n",
        "            config_overrides\n",
        "        ),\n",
        "        num_workers = get_config_value(\n",
        "            \"data_loader_num_workers\",\n",
        "            config.num_workers,\n",
        "            config_overrides\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def create_optimizer_config(**config_overrides) -> OptimizerConfig():\n",
        "    config = OptimizerConfig()\n",
        "    return OptimizerConfig(\n",
        "        learning_rate = get_config_value(\n",
        "            \"optimizer_learning_rate\",\n",
        "            config.learning_rate,\n",
        "            config_overrides\n",
        "        ),\n",
        "        momentum = get_config_value(\n",
        "            \"optimizer_momentum\",\n",
        "            config.momentum,\n",
        "            config_overrides\n",
        "        ),\n",
        "        weight_decay = get_config_value(\n",
        "            \"optimizer_weight_decay\",\n",
        "            config.weight_decay,\n",
        "            config_overrides\n",
        "        ),\n",
        "        betas = get_config_value(\n",
        "            \"optimizer_betas\",\n",
        "            config.betas,\n",
        "            config_overrides\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def create_scheduler_config(**config_overrides) -> SchedulerConfig:\n",
        "    config = SchedulerConfig()\n",
        "    return SchedulerConfig(\n",
        "        gamma = get_config_value(\n",
        "            \"scheduler_gamma\",\n",
        "            config.gamma,\n",
        "            config_overrides\n",
        "        ),\n",
        "        step_size = get_config_value(\n",
        "            \"scheduler_step_size\",\n",
        "            config.step_size,\n",
        "            config_overrides\n",
        "        ),\n",
        "        milestones = get_config_value(\n",
        "            \"scheduler_milestones\",\n",
        "            config.milestones,\n",
        "            config_overrides\n",
        "        ),\n",
        "        patience = get_config_value(\n",
        "            \"scheduler_patience\",\n",
        "            config.patience,\n",
        "            config_overrides\n",
        "        ),\n",
        "        threshold = get_config_value(\n",
        "            \"scheduler_threshold\",\n",
        "            config.threshold,\n",
        "            config_overrides\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def create_trainer_config(**config_overrides) -> TrainerConfig:\n",
        "    config = TrainerConfig()\n",
        "    return TrainerConfig(\n",
        "        training_epochs = get_config_value(\n",
        "            \"trainer_training_epochs\",\n",
        "            config.training_epochs,\n",
        "            config_overrides\n",
        "        ),\n",
        "        model_saving_period = get_config_value(\n",
        "            \"trainer_model_saving_period\",\n",
        "            config.model_saving_period,\n",
        "            config_overrides\n",
        "        ),\n",
        "        stop_loss_epochs = get_config_value(\n",
        "            \"trainer_stop_loss_epochs\",\n",
        "            config.stop_loss_epochs,\n",
        "            config_overrides\n",
        "        ),\n",
        "        stop_acc_epochs = get_config_value(\n",
        "            \"trainer_stop_acc_epochs\",\n",
        "            config.stop_acc_epochs,\n",
        "            config_overrides\n",
        "        ),\n",
        "        stop_acc_ema_alpha = get_config_value(\n",
        "            \"trainer_stop_acc_ema_alpha\",\n",
        "            config.stop_acc_ema_alpha,\n",
        "            config_overrides\n",
        "        ),\n",
        "        stop_acc_threshold = get_config_value(\n",
        "            \"trainer_stop_acc_threshold\",\n",
        "            config.stop_acc_threshold,\n",
        "            config_overrides\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class MasterConfig:\n",
        "    system: SystemConfig = create_system_config()\n",
        "    data_aug: DataAugConfig = create_data_aug_config()\n",
        "    dataset: DatasetConfig = create_dataset_config()\n",
        "    data_loader: DataLoaderConfig = create_data_loader_config()\n",
        "    optimizer: OptimizerConfig = create_optimizer_config()\n",
        "    scheduler: SchedulerConfig = create_scheduler_config()\n",
        "    trainer: TrainerConfig = create_trainer_config()\n",
        "\n",
        "\n",
        "def create_master_config(\n",
        "    transform_resize: int = 256,\n",
        "    transform_crop_size: int = 224,\n",
        "    **config_overrides\n",
        ") -> MasterConfig:\n",
        "    # used to initialize MasterConfig data class and as a parameter to the\n",
        "    # create_data_config function\n",
        "    data_aug_config = create_data_aug_config(**config_overrides)\n",
        "    dataset_config = create_dataset_config(\n",
        "        transform_resize, \n",
        "        transform_crop_size, \n",
        "        data_aug_config\n",
        "    )\n",
        "    return MasterConfig(\n",
        "        system = create_system_config(),\n",
        "        data_aug = data_aug_config,\n",
        "        dataset = dataset_config,\n",
        "        data_loader = create_data_loader_config(**config_overrides),\n",
        "        optimizer = create_optimizer_config(**config_overrides),\n",
        "        scheduler = create_scheduler_config(**config_overrides),\n",
        "        trainer = create_trainer_config(**config_overrides)\n",
        "    ) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYVGcNyX3-50"
      },
      "source": [
        "## <font style=\"color:green\">3. Evaluation Metric [10 Points]</font>\n",
        "\n",
        "Define methods or classes that will be used in model evaluation, for example, accuracy, f1-score, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJg9CPFB3-50"
      },
      "source": [
        "### Loss Function\n",
        "\n",
        "I will use [torch.nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) loss function with and without passing a tensor of rescaling weights. The weight corresponding to each class are inversely proportional to the number images in that class. The weight tensor is normalized, as shown below.\n",
        "\n",
        "```\n",
        "    weight = tensor([0.0499, 0.0366, 0.0658, 0.0638, 0.1823, \n",
        "                    0.0509, 0.0720, 0.0653, 0.1487, 0.0402, \n",
        "                    0.0958, 0.0784, 0.0502], dtype=torch.float64)\n",
        "```\n",
        "\n",
        "### Metric Function\n",
        "\n",
        "I am using the **trainer** module's `AccuracyEstimator` class from the <u>metrics.py</u> file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sSIXHXg3-51"
      },
      "source": [
        "## <font style=\"color:green\">4. Train and Validation [5 Points]</font>\n",
        "\n",
        "Write the methods or classes that will be used for training and validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhF41hAH3-51"
      },
      "source": [
        "## Assignment Response\n",
        "\n",
        "Since I am using the **trainer** module, I made the following modifications to the `trainer.py` file.\n",
        "* Added the ability to save the model only when the test loss reaches a new minimum.\n",
        "* Added the ability to terminate training after a specified number of epochs where the test loss is not further reduced.\n",
        "* Added the ability to terminate training after a specified number of epochs where the exponential moving average of the test loss does not significantly increase.\n",
        "\n",
        "I made the following modifications to the `visualizer.py` and `tensorboard_visualizer.py` files.\n",
        "* Added an <code>add_image(self, tag, image)</code> method to visualize the dataset.\n",
        "* Added an <code>add_figure(elf, tag, figure, close=True)</code> method to visulize matplotlib figures, e.g., confusion matrices.\n",
        "* Added an <code>add_graph(self, model, images)</code> method to document the model.\n",
        "* Added an <code>add_pr_curves(self, classes, pred_probs, targets)</code> method to document the precision-recall curves of the fully trained model for each class type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mxi77QT13-51"
      },
      "source": [
        "class Optimizer(Enum):\n",
        "    SGD = auto()\n",
        "    ADAM = auto()\n",
        "\n",
        "\n",
        "def get_optimizer(\n",
        "    model: nn.Module,\n",
        "    optimizer: Optimizer = Optimizer.SGD,\n",
        "    config: OptimizerConfig = OptimizerConfig()\n",
        "):\n",
        "    \"\"\"\n",
        "    Gets the specified optimizer.\n",
        "    \"\"\"\n",
        "    \n",
        "    if optimizer == Optimizer.SGD:\n",
        "        return optim.SGD(\n",
        "            model.parameters(),\n",
        "            lr = config.learning_rate,\n",
        "            momentum = config.momentum,\n",
        "            weight_decay = config.weight_decay\n",
        "        )\n",
        "    \n",
        "    elif optimizer == Optimizer.ADAM:\n",
        "        # Note: The weight_decay parameter was added after Group C experiments.\n",
        "        return optim.Adam(\n",
        "            model.parameters(),\n",
        "            lr = config.learning_rate,\n",
        "            betas = config.betas,\n",
        "            weight_decay = config.weight_decay\n",
        "        )\n",
        "    \n",
        "    else:\n",
        "        raise SystemExit(\"Invalid optimizer value.\")\n",
        "\n",
        "\n",
        "class LrScheduler(Enum):\n",
        "    STEP = auto()\n",
        "    MULTI_STEP = auto()\n",
        "    EXPONENTIAL = auto()\n",
        "    REDUCE_ON_PLATEAU = auto()\n",
        "\n",
        "    \n",
        "def get_scheduler(\n",
        "    optimizer: optim.Optimizer,\n",
        "    scheduler: LrScheduler = LrScheduler.STEP,\n",
        "    config: SchedulerConfig = SchedulerConfig()\n",
        "):\n",
        "    \"\"\"\n",
        "    Gets the specified LR scheduler.\n",
        "    \"\"\"\n",
        "\n",
        "    if scheduler == LrScheduler.STEP:\n",
        "        return optim.lr_scheduler.StepLR(\n",
        "            optimizer,\n",
        "            step_size = config.step_size,\n",
        "            gamma = config.gamma\n",
        "        )\n",
        "    \n",
        "    elif scheduler == LrScheduler.MULTI_STEP:\n",
        "        return optim.lr_scheduler.MultiStepLR(\n",
        "            optimizer, \n",
        "            milestones = config.milestones, \n",
        "            gamma = config.gamma\n",
        "        )\n",
        "    \n",
        "    elif scheduler == LrScheduler.EXPONENTIAL:\n",
        "        return optim.lr_scheduler.ExponentialLR(\n",
        "            optimizer, \n",
        "            gamma = config.gamma\n",
        "        )\n",
        "    \n",
        "    \n",
        "    elif scheduler == LrScheduler.REDUCE_ON_PLATEAU:\n",
        "        return optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, \n",
        "            factor = config.gamma,\n",
        "            patience = config.patience,\n",
        "            threshold = config.threshold\n",
        "        )\n",
        "    \n",
        "    else:\n",
        "        raise SystemExit(\"Invalid scheduler value.\")    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlaX2uwY3-51"
      },
      "source": [
        "def predict_batch(model, data, max_prob=True):\n",
        "    \"\"\"\n",
        "    Get prediction for a batch of data. This function assumes the model and data\n",
        "    have be sent to the appropriate device and the model is in evaluation mode.\n",
        "    \"\"\"\n",
        "\n",
        "    output = model(data)\n",
        "\n",
        "    # get probability score using softmax\n",
        "    prob = F.softmax(output, dim=1)\n",
        "    \n",
        "    if max_prob:\n",
        "        # get the max probability\n",
        "        pred_prob = prob.data.max(dim=1)[0]\n",
        "    else:\n",
        "        # return all probabilties\n",
        "        pred_prob = prob.data\n",
        "    \n",
        "    # get the index of the max probability\n",
        "    pred_index = prob.data.max(dim=1)[1]\n",
        "    \n",
        "    return pred_index.cpu().numpy(), pred_prob.cpu().numpy()\n",
        "\n",
        "\n",
        "def get_targets_and_pred_probs(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Get targets and prediction probabilities.\n",
        "    \"\"\"\n",
        "    \n",
        "    model.to(device)  # send model to cpu or cuda\n",
        "    model.eval()      # set model to evaluation mode\n",
        "\n",
        "    targets = []\n",
        "    pred_probs = []\n",
        "\n",
        "    for data, target in dataloader:\n",
        "        _, probs = predict_batch(model, data.to(device), max_prob=False)       \n",
        "        pred_probs.append(probs)\n",
        "        targets.append(target.numpy())\n",
        "        \n",
        "    targets = np.concatenate(targets).astype(int)\n",
        "    pred_probs = np.concatenate(pred_probs, axis=0)\n",
        "    \n",
        "    return targets, pred_probs\n",
        "\n",
        "\n",
        "def predict_valid_data(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Predict the class of the validation data.\n",
        "    \"\"\"\n",
        "\n",
        "    model.to(device)  # send model to cpu or cuda\n",
        "    model.eval()      # set model to evaluation mode\n",
        "\n",
        "    targets = []\n",
        "    preds = []\n",
        "\n",
        "    for data, target in dataloader:\n",
        "        pred, _ = predict_batch(model, data.to(device), max_prob=True)       \n",
        "        targets.append(target)\n",
        "        preds.append(pred)\n",
        "        \n",
        "    targets = np.concatenate(targets)\n",
        "    preds = np.concatenate(preds).astype(int)\n",
        "    \n",
        "    return targets, preds\n",
        "\n",
        "\n",
        "def predict_test_data(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Predict the class of the test data.\n",
        "    \"\"\"\n",
        "    model.to(device)  # send model to cpu or cuda\n",
        "    model.eval()      # set model to evaluation mode\n",
        "\n",
        "    fnames = []\n",
        "    preds = []\n",
        "\n",
        "    for _, (data, fname) in enumerate(dataloader):\n",
        "        pred, _ = predict_batch(model, data.to(device), max_prob=True)       \n",
        "        fnames.append(fname)\n",
        "        preds.append(pred)\n",
        "        \n",
        "    fnames = np.concatenate(fnames)\n",
        "    preds = np.concatenate(preds).astype(int)\n",
        "    \n",
        "    return fnames, preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajr4pCOz3-51"
      },
      "source": [
        "## <font style=\"color:green\">5. Model [5 Points]</font>\n",
        "\n",
        "Define your model in this section.\n",
        "\n",
        "**You are allowed to use any pre-trained model.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02C_Nwdo7SBK"
      },
      "source": [
        "## Assignment Response\n",
        "\n",
        "Since my approach is to explore transfer learning in numerous pretrained models, I created classes\n",
        "to easily set the \"tuning level\" of the ResNet, VGG, and DenseNet family of TorchVision models. I\n",
        "also want to see how the model I developed for Project 1 performs, so I created as a class for it as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18LAfbDDpuwn"
      },
      "source": [
        "TuningParam = namedtuple(\"TuningParam\", [\"level\", \"block\", \"layers\"])\n",
        "\n",
        "class TorchVisionModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Base class for TorchVision models, which provides a method to freeze network\n",
        "    layers allowing fine tuning. This class does change the network's output layer.\n",
        "    Derived classes must do this!\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, network: nn.Module):\n",
        "        super().__init__()\n",
        "        self._network = network\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self._network(x)\n",
        "    \n",
        "    def _freeze_layers(\n",
        "        self, \n",
        "        tuning_params: List[TuningParam], \n",
        "        pretrained:bool, \n",
        "        tuning_level:int\n",
        "    ):\n",
        "        # exit if not using a pretrained model\n",
        "        if not pretrained:\n",
        "            return\n",
        "            \n",
        "        # otherwise, 1) freeze the entire network\n",
        "        self._set_requires_grad(self._network, False)\n",
        "        \n",
        "        # and 2) unfreeze blocks/layers based on tuning_level\n",
        "        for param in tuning_params:\n",
        "            if param.level <= tuning_level:\n",
        "                block = getattr(self._network, param.block)\n",
        "                if param.layers is None:\n",
        "                    self._set_requires_grad(block, True)\n",
        "                else:\n",
        "                    for layer in param.layers:\n",
        "                        if isinstance(layer, int):\n",
        "                            self._set_requires_grad(block[layer], True)\n",
        "                        else:\n",
        "                            self._set_requires_grad(getattr(block, layer), True)\n",
        "            \n",
        "    def _set_requires_grad(self, block, value):\n",
        "        for param in block.parameters():\n",
        "            param.requires_grad = value\n",
        "            \n",
        "    def _inclusive_range(self, start:int, stop:int) -> List[int]:\n",
        "        return list(range(start, stop + 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DonDtN5MBBU1"
      },
      "source": [
        "class ResNetBase(TorchVisionModel):\n",
        "    \"\"\"\n",
        "    Base class for ResNet models that may be pretrained and fine tuned. The\n",
        "    tuning_level parameter controls the degree of fine tuning as depicted in\n",
        "    the table below.\n",
        "        \n",
        "        ResNet     tuning_level\n",
        "        -------    ------------\n",
        "        conv1          >= 5        \n",
        "        bn1            >= 5\n",
        "        relu           >= 5\n",
        "        maxpool        >= 5\n",
        "        layer1         >= 4\n",
        "        layer2         >= 3\n",
        "        layer3         >= 2\n",
        "        layer4         >= 1\n",
        "        avgpool        >= 1\n",
        "        fc             >= 0\n",
        "        \n",
        "    If tuning_level = 0, then only the classifier layer is trained.\n",
        "    If tuning_level = 5, then the entire network is trained.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model_fn: Callable, pretrained=True, tuning_level=0):\n",
        "        super().__init__(model_fn(pretrained=pretrained))\n",
        "\n",
        "        # change the output layer\n",
        "        last_layer_in = self._network.fc.in_features\n",
        "        self._network.fc = nn.Linear(last_layer_in, 13)\n",
        "\n",
        "        # ToDo: Omit layer types that do not have trainable parameters\n",
        "        tuning_params = [\n",
        "            TuningParam(0, \"fc\", None),\n",
        "            TuningParam(1, \"avgpool\", None),\n",
        "            TuningParam(1, \"layer4\", None),\n",
        "            TuningParam(2, \"layer3\", None),\n",
        "            TuningParam(3, \"layer2\", None),\n",
        "            TuningParam(4, \"layer1\", None),\n",
        "            TuningParam(5, \"maxpool\", None),\n",
        "            TuningParam(5, \"relu\", None),\n",
        "            TuningParam(5, \"bn1\", None),\n",
        "            TuningParam(5, \"conv1\", None)\n",
        "        ]\n",
        "\n",
        "        self._freeze_layers(tuning_params, pretrained, tuning_level)\n",
        "\n",
        "    \n",
        "class ResNet18(ResNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.resnet18, pretrained, tuning_level)\n",
        "\n",
        "class ResNet34(ResNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.resnet34, pretrained, tuning_level)\n",
        "\n",
        "class ResNet50(ResNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.resnet50, pretrained, tuning_level)\n",
        "\n",
        "class ResNet101(ResNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.resnet101, pretrained, tuning_level)\n",
        "\n",
        "class ResNet152(ResNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.resnet152, pretrained, tuning_level)\n",
        "\n",
        "class ResNeXt50(ResNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.resnext50_32x4d, pretrained, tuning_level)\n",
        "\n",
        "class ResNeXt101(ResNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.resnext101_32x8d, pretrained, tuning_level)\n",
        "\n",
        "class WideResNet50(ResNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.wide_resnet50_2, pretrained, tuning_level)\n",
        "\n",
        "class WideResNet101(ResNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.wide_resnet101_2, pretrained, tuning_level)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e70wQoxzkgh"
      },
      "source": [
        "class VGGBase(TorchVisionModel):\n",
        "    \"\"\"\n",
        "    Base class for ResNet models that may be pretrained and fine tuned.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model_fn: Callable, pretrained=True):\n",
        "        super().__init__(model_fn(pretrained=pretrained))\n",
        "\n",
        "        last_layer_in = self._network.classifier[6].in_features\n",
        "        self._network.classifier[6] = nn.Linear(last_layer_in, 13)\n",
        "\n",
        "\n",
        "class VGG11BN(VGGBase):\n",
        "    \"\"\"\n",
        "    VGG11BN model that may be pretrained and fine tuned. The tuning_level\n",
        "    parameter controls the degree of fine tuning as depicted in the table\n",
        "    below.\n",
        "    \n",
        "        VGG11_BN            tuning_level\n",
        "        ----------------    ------------\n",
        "        features\n",
        "          [00-02] CNR           >= 5\n",
        "          [03] MaxPool2d        >= 5\n",
        "          [04-06] CNR           >= 4\n",
        "          [07] MaxPool2d        >= 4\n",
        "          [08-10] CNR           >= 3\n",
        "          [11-13] CNR           >= 3\n",
        "          [14] MaxPool2d        >= 3\n",
        "          [15-17] CNR           >= 2\n",
        "          [18-20] CNR           >= 2\n",
        "          [21] MaxPool2d        >= 2\n",
        "          [22-24] CNR           >= 1\n",
        "          [25-27] CNR           >= 1\n",
        "          [28] MaxPool2d        >= 1\n",
        "        avgpool                 >= 1\n",
        "        classifier              \n",
        "          [00-02] LRD           >= 0\n",
        "          [03-05] LRD           >= 0\n",
        "          [06] Linear           >= 0\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.vgg11_bn, pretrained)\n",
        "            \n",
        "        # ToDo: Omit layer types that do not have trainable parameters\n",
        "        tuning_params = [\n",
        "            TuningParam(0, \"classifier\", None),\n",
        "            TuningParam(1, \"avgpool\", None),\n",
        "            TuningParam(1, \"features\", self._inclusive_range(22, 28)),\n",
        "            TuningParam(2, \"features\", self._inclusive_range(15, 21)),\n",
        "            TuningParam(3, \"features\", self._inclusive_range(8, 14)),\n",
        "            TuningParam(4, \"features\", self._inclusive_range(4, 7)),\n",
        "            TuningParam(5, \"features\", self._inclusive_range(0, 3))\n",
        "        ]\n",
        "\n",
        "        self._freeze_layers(tuning_params, pretrained, tuning_level)\n",
        "\n",
        "\n",
        "class VGG13BN(VGGBase):\n",
        "    \"\"\"\n",
        "    VGG13BN model that may be pretrained and fine tuned. The tuning_level\n",
        "    parameter controls the degree of fine tuning as depicted in the table\n",
        "    below.\n",
        "    \n",
        "        VGG13_BN            tuning_level\n",
        "        ----------------    ------------\n",
        "        features\n",
        "          [00-02] CNR           >= 5\n",
        "          [03-05] CNR           >= 5\n",
        "          [06] MaxPool2d        >= 5\n",
        "          [07-09] CNR           >= 4\n",
        "          [10-12] CNR           >= 4\n",
        "          [13] MaxPool2d        >= 4\n",
        "          [14-16] CNR           >= 3\n",
        "          [17-19] CNR           >= 3\n",
        "          [20] MaxPool2d        >= 3\n",
        "          [21-23] CNR           >= 2\n",
        "          [24-26] CNR           >= 2\n",
        "          [27] MaxPool2d        >= 2\n",
        "          [28-30] CNR           >= 1\n",
        "          [31-33] CNR           >= 1\n",
        "          [34] MaxPool2d        >= 1\n",
        "        avgpool                 >= 1\n",
        "        classifier\n",
        "          [00-02] LRD           >= 0\n",
        "          [03-05] LRD           >= 0\n",
        "          [06] Linear           >= 0\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.vgg13_bn, pretrained)\n",
        "            \n",
        "        # ToDo: Omit layer types that do not have trainable parameters\n",
        "        tuning_params = [\n",
        "            TuningParam(0, \"classifier\", None),\n",
        "            TuningParam(1, \"avgpool\", None),\n",
        "            TuningParam(1, \"features\", self._inclusive_range(28, 34)),\n",
        "            TuningParam(2, \"features\", self._inclusive_range(21, 27)),\n",
        "            TuningParam(3, \"features\", self._inclusive_range(14, 20)),\n",
        "            TuningParam(4, \"features\", self._inclusive_range(7, 13)),\n",
        "            TuningParam(5, \"features\", self._inclusive_range(0, 6))\n",
        "        ]\n",
        "\n",
        "        self._freeze_layers(tuning_params, pretrained, tuning_level)\n",
        "\n",
        "\n",
        "class VGG16BN(VGGBase):\n",
        "    \"\"\"\n",
        "    VGG16BN model that may be pretrained and fine tuned. The tuning_level\n",
        "    parameter controls the degree of fine tuning as depicted in the table\n",
        "    below.\n",
        "    \n",
        "        VGG16_BN            tuning_level\n",
        "        ----------------    ------------\n",
        "        features\n",
        "          [00-02] CNR           >= 5\n",
        "          [03-05] CNR           >= 5\n",
        "          [06] MaxPool2d        >= 5\n",
        "          [07-09] CNR           >= 4\n",
        "          [10-12] CNR           >= 4\n",
        "          [13] MaxPool2d        >= 4\n",
        "          [14-16] CNR           >= 3\n",
        "          [17-19] CNR           >= 3\n",
        "          [20-22] CNR           >= 3\n",
        "          [23] MaxPool2d        >= 3\n",
        "          [24-26] CNR           >= 2\n",
        "          [27-29] CNR           >= 2\n",
        "          [30-32] CNR           >= 2\n",
        "          [33] MaxPool2d        >= 2\n",
        "          [34-36] CNR           >= 1\n",
        "          [37-39] CNR           >= 1\n",
        "          [40-42] CNR           >= 1\n",
        "          [43] MaxPool2d        >= 1\n",
        "        avgpool                 >= 1\n",
        "        classifier              \n",
        "          [00-02] LRD           >= 0\n",
        "          [03-05] LRD           >= 0\n",
        "          [06] Linear           >= 0\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.vgg16_bn, pretrained)\n",
        "            \n",
        "        # ToDo: Omit layer types that do not have trainable parameters\n",
        "        tuning_params = [\n",
        "            TuningParam(0, \"classifier\", None),\n",
        "            TuningParam(1, \"avgpool\", None),\n",
        "            TuningParam(1, \"features\", self._inclusive_range(34, 43)),\n",
        "            TuningParam(2, \"features\", self._inclusive_range(24, 33)),\n",
        "            TuningParam(3, \"features\", self._inclusive_range(14, 23)),\n",
        "            TuningParam(4, \"features\", self._inclusive_range(7, 13)),\n",
        "            TuningParam(5, \"features\", self._inclusive_range(0, 6))\n",
        "        ]\n",
        "\n",
        "        self._freeze_layers(tuning_params, pretrained, tuning_level)\n",
        "\n",
        "\n",
        "class VGG19BN(VGGBase):\n",
        "    \"\"\"\n",
        "    VGG19BN model that may be pretrained and fine tuned. The tuning_level\n",
        "    parameter controls the degree of fine tuning as depicted in the table\n",
        "    below.\n",
        "\n",
        "        VGG11_BN            tuning_level\n",
        "        ----------------    ------------\n",
        "        features\n",
        "          [00-02] CNR           >= 5\n",
        "          [03-05] CNR           >= 5\n",
        "          [06] MaxPool2d        >= 5\n",
        "          [07-09] CNR           >= 4\n",
        "          [10-12] CNR           >= 4\n",
        "          [13] MaxPool2d        >= 4\n",
        "          [14-16] CNR           >= 3\n",
        "          [17-19] CNR           >= 3\n",
        "          [20-22] CNR           >= 3\n",
        "          [23-25] CNR           >= 3\n",
        "          [26] MaxPool2d        >= 3\n",
        "          [27-29] CNR           >= 2\n",
        "          [30-32] CNR           >= 2\n",
        "          [33-35] CNR           >= 2\n",
        "          [36-38] CNR           >= 2\n",
        "          [39] MaxPool2d        >= 2\n",
        "          [40-42] CNR           >= 1\n",
        "          [43-45] CNR           >= 1\n",
        "          [46-48] CNR           >= 1\n",
        "          [49-51] CNR           >= 1\n",
        "          [52] MaxPool2d        >= 1\n",
        "        avgpool                 >= 1\n",
        "        classifier              \n",
        "          [00-02] LRD           >= 0\n",
        "          [03-05] LRD           >= 0\n",
        "          [06] Linear           >= 0\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.vgg19_bn, pretrained)\n",
        "                # ToDo: Omit layer types that do not have trainable parameters\n",
        "        tuning_params = [\n",
        "            TuningParam(0, \n",
        "    \"classifier\", None),\n",
        "            TuningParam(1, \"avgpool\", None),\n",
        "            TuningParam(1, \"features\", self._inclusive_range(40, 52)),\n",
        "            TuningParam(2, \"features\", self._inclusive_range(27, 39)),\n",
        "            TuningParam(3, \"features\", self._inclusive_range(14, 26)),\n",
        "            TuningParam(4, \"features\", self._inclusive_range(7, 13)),\n",
        "            TuningParam(5, \"features\", self._inclusive_range(0, 6))\n",
        "        ]\n",
        "\n",
        "        self._freeze_layers(tuning_params, pretrained, tuning_level)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aun0yxTfpuwq"
      },
      "source": [
        "class DenseNetBase(TorchVisionModel):\n",
        "    \"\"\"\n",
        "    Base class for DenseNet models that may be pretrained and fine tuned. The\n",
        "    tuning_level parameter controls the degree of fine tuning as depicted in\n",
        "    the table below.\n",
        "        \n",
        "        DenseNet          tuning_level\n",
        "        -------------     ------------\n",
        "        features\n",
        "          conv0               >= 5\n",
        "          norm0               >= 5\n",
        "          relu0               >= 5\n",
        "          pool0               >= 5\n",
        "          denseblock1         >= 4\n",
        "          transition1         >= 4\n",
        "          denseblock2         >= 3\n",
        "          transition2         >= 3\n",
        "          denseblock3         >= 2\n",
        "          transition3         >= 2\n",
        "          denseblock4         >= 1\n",
        "          norm5               >= 1\n",
        "        classifier            >= 0\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_fn: Callable, pretrained=True, tuning_level=0):\n",
        "        super().__init__(model_fn(pretrained=pretrained))\n",
        "\n",
        "        # change the output layer\n",
        "        last_layer_in = self._network.classifier.in_features\n",
        "        self._network.classifier = nn.Linear(last_layer_in, 13)\n",
        "\n",
        "        # ToDo: Omit layer types that do not have trainable parameters\n",
        "        tuning_params = [\n",
        "            TuningParam(0, \"classifier\", None),\n",
        "            TuningParam(1, \"features\", [\"denseblock4\", \"norm5\"]),\n",
        "            TuningParam(2, \"features\", [\"denseblock3\", \"transition3\"]),\n",
        "            TuningParam(3, \"features\", [\"denseblock2\", \"transition2\"]),\n",
        "            TuningParam(4, \"features\", [\"denseblock1\", \"transition1\"]),\n",
        "            TuningParam(5, \"features\", [\"conv0\", \"norm0\", \"relu0\", \"pool0\"])\n",
        "        ]\n",
        "\n",
        "        self._freeze_layers(tuning_params, pretrained, tuning_level)\n",
        "\n",
        "    \n",
        "class DenseNet121(DenseNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.densenet121, pretrained, tuning_level)\n",
        "\n",
        "class DenseNet169(DenseNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.densenet169, pretrained, tuning_level)\n",
        "\n",
        "class DenseNet201(DenseNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.densenet201, pretrained, tuning_level)\n",
        "\n",
        "class DenseNet161(DenseNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(models.densenet161, pretrained, tuning_level)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EfficientNetBase(TorchVisionModel):\n",
        "    \"\"\"\n",
        "    Base class for EfficientNet models that may be pretrained and fine tuned. The\n",
        "    tuning_level parameter controls the degree of fine tuning as depicted in\n",
        "    the table below.\n",
        "        \n",
        "        EfficienteNet     tuning_level\n",
        "        -------------     ------------\n",
        "        _conv_stem            >= 5\n",
        "        _bn0                  >= 5\n",
        "        _swish\n",
        "        _blocks               >= 5\n",
        "          [0] MBConvBlock     >= 5\n",
        "          [1] MBConvBlock     >= 5\n",
        "          [2] MBConvBlock     >= 5\n",
        "           ...                \n",
        "          [N-1] MBConvBlock   >= 1\n",
        "        _conv_head            >= 1\n",
        "        _bn1                  >= 1\n",
        "        _swish\n",
        "        _avg_pooling          >= 0\n",
        "        _dropout              >= 0\n",
        "        _fc                   >= 0\n",
        "\n",
        "    The number of MBConvBlock depends upon the depth as is given by the table below.\n",
        "    Each tuning level greater than zero, unfreezes 20% of the blocks.\n",
        "\n",
        "        Depth   Blocks\n",
        "        -----   ------\n",
        "          0       16\n",
        "          1       23\n",
        "          2       23\n",
        "          3       26\n",
        "          4       32\n",
        "          5       39\n",
        "          6       45\n",
        "          7       55\n",
        "\n",
        "    Note: The resulting blocks/layers from printing the model do not correspond to flow\n",
        "          of forward method. Does the the Pytorch EfficientNet implementation need to \n",
        "          override the nn.Module's \"iterator\" methods? For some reason, the _swish layer\n",
        "          appears after the fully connected _fc layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, depth=0, pretrained=True, tuning_level=0):\n",
        "        super(EfficientNetBase, self).__init__(self.__get_model(depth, pretrained))\n",
        "            \n",
        "        blocks = len(self._network._blocks)\n",
        "        levels = self.__compute_levels(blocks)\n",
        "\n",
        "        # ToDo: Omit layer types that do not have trainable parameters\n",
        "        tuning_params = [\n",
        "            TuningParam(0, \"_fc\", None),\n",
        "            TuningParam(0, \"_dropout\", None),\n",
        "            TuningParam(0, \"_avg_pooling\", None),\n",
        "            TuningParam(1, \"_bn1\", None),\n",
        "            TuningParam(1, \"_conv_head\", None),\n",
        "            TuningParam(1, \"_blocks\", self._inclusive_range(levels[4][0], levels[4][1])),\n",
        "            TuningParam(2, \"_blocks\", self._inclusive_range(levels[3][0], levels[3][1])),\n",
        "            TuningParam(3, \"_blocks\", self._inclusive_range(levels[2][0], levels[2][1])),\n",
        "            TuningParam(4, \"_blocks\", self._inclusive_range(levels[1][0], levels[1][1])),\n",
        "            TuningParam(5, \"_blocks\", self._inclusive_range(levels[0][0], levels[0][1])),\n",
        "            TuningParam(5, \"_bn0\", None),\n",
        "            TuningParam(5, \"_conv_stem\", None)\n",
        "        ]\n",
        "\n",
        "        self._freeze_layers(tuning_params, pretrained, tuning_level)\n",
        "\n",
        "    def __get_model(self, depth:int, pretrained:bool) -> nn.Module:\n",
        "        name = f\"efficientnet-b{depth}\"\n",
        "        if not pretrained:\n",
        "            return EfficientNet.from_name(name, num_classes=13)\n",
        "        else:\n",
        "            return EfficientNet.from_pretrained(name, num_classes=13)\n",
        "\n",
        "    def __compute_levels(self, blocks:int) -> List[Tuple[int, int]]:\n",
        "        div = int(blocks / 5)\n",
        "        rem = blocks - 5 * div\n",
        "        levels = []\n",
        "\n",
        "        s = 0\n",
        "        for i in range(1,6):\n",
        "            e = s + div - (0 if i <= rem else 1)\n",
        "            levels.append((s, e))\n",
        "            s = e + 1\n",
        "\n",
        "        return levels\n",
        "\n",
        "\n",
        "class EfficientNetB0(EfficientNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(0, pretrained, tuning_level)\n",
        "\n",
        "class EfficientNetB1(EfficientNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(1, pretrained, tuning_level)\n",
        "\n",
        "class EfficientNetB2(EfficientNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(2, pretrained, tuning_level)\n",
        "\n",
        "class EfficientNetB3(EfficientNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(3, pretrained, tuning_level)\n",
        "\n",
        "class EfficientNetB4(EfficientNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(4, pretrained, tuning_level)\n",
        "\n",
        "class EfficientNetB5(EfficientNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(5, pretrained, tuning_level)\n",
        "\n",
        "class EfficientNetB6(EfficientNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(6, pretrained, tuning_level)\n",
        "\n",
        "class EfficientNetB7(EfficientNetBase):\n",
        "    def __init__(self, pretrained=True, tuning_level=0):\n",
        "        super().__init__(7, pretrained, tuning_level)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHRk6r0s3-51"
      },
      "source": [
        "class Project1Model(nn.Module):\n",
        "    \"\"\"\n",
        "    Modified the last layer to output 13, rather than 3, features.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Convolution layers\n",
        "        self._body = nn.Sequential(\n",
        "            # input 3 x 224 x 224\n",
        "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=7, padding=3),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "            # input 24 * 112 * 112\n",
        "            nn.Conv2d(in_channels=16, out_channels=24, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(24),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "            # input 36 * 56 * 56\n",
        "            nn.Conv2d(in_channels=24, out_channels=36, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(36),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "            #input 54 * 28 * 28\n",
        "            nn.Conv2d(in_channels=36, out_channels=54, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(54),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "            #input 81 * 14 * 14\n",
        "            nn.Conv2d(in_channels=54, out_channels=81, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(81),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "\n",
        "        # Fully connected layers\n",
        "        self._head = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(in_features=81*7*7, out_features=1024), \n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(in_features=1024, out_features=256), \n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(in_features=256, out_features=13)            \n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self._body(x)\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        x = self._head(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ToDo: Put into a static class (if Python supports them).\n",
        "\n",
        "def get_tuning_suffix(pretrained:bool, tuning_level:int) -> str:\n",
        "    return f\"_PT{tuning_level}\" if pretrained else \"\"\n",
        "\n",
        "def get_learning_rate_suffix(learning_rate: float) -> str:\n",
        "    return f\"_LR{learning_rate:.0E}\".replace(\"E-0\", \"E-\")\n",
        "    \n",
        "model_params = {\n",
        "    EfficientNetB0: (224, 94),\n",
        "    EfficientNetB1: (240, 57),\n",
        "    EfficientNetB2: (260, 45),\n",
        "    EfficientNetB3: (300, 26),\n",
        "    EfficientNetB4: (380, 12),\n",
        "    EfficientNetB5: (456,  6),\n",
        "    EfficientNetB6: (528,  3),\n",
        "    EfficientNetB7: (600,  2),\n",
        "}\n",
        "\n",
        "def get_image_size(model_type: TorchVisionModel) -> int:\n",
        "    try:\n",
        "        return model_params[model_type][0]\n",
        "    except:\n",
        "        return 224\n",
        "\n",
        "def get_batch_size(model_type: TorchVisionModel) -> int:\n",
        "    try:\n",
        "        return model_params[model_type][1]\n",
        "    except:\n",
        "        return 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XjG813k3-52"
      },
      "source": [
        "## <font style=\"color:green\">6. Utils [5 Points]</font>\n",
        "\n",
        "Define your methods or classes which are not covered in the above sections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyWhT4sIpuwr"
      },
      "source": [
        "def create_confusion_matrix(cm, classes, model_name=None):\n",
        "    \"\"\"\n",
        "    Creates and returns a confusion matrix figure that can be saved to a file or .\n",
        "    \"\"\"\n",
        "\n",
        "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "    # compute accuracy, normalized confusion matrix\n",
        "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
        "    cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    # initialize the plot tick marks and title\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    title = \"Confusion Matrix\"\n",
        "    if model_name is not None:\n",
        "        title = title + \" ({})\".format(model_name)\n",
        "    \n",
        "    # plot the confusion matrix\n",
        "    plt.style.use('default')\n",
        "    fig = plt.figure(figsize=(11,10), tight_layout=True)\n",
        "    im = plt.imshow(cm_norm, interpolation=\"nearest\", cmap=plt.cm.Blues, vmin=0., vmax=1.)\n",
        "\n",
        "    plt.title(title + \"\\n\")\n",
        "    plt.xticks(tick_marks, classes, rotation=22.5)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(\n",
        "            j, i,\n",
        "            format(cm[i, j], \"d\") + \"\\n\" + format(cm_norm[i, j], \".2f\"), \n",
        "            horizontalalignment=\"center\",\n",
        "            verticalalignment=\"center\",\n",
        "            color=\"white\" if cm_norm[i, j] > 0.5 else \"black\"\n",
        "        )\n",
        "\n",
        "    plt.ylabel(\"Target Labels\")\n",
        "    plt.xlabel(\"Predicted Labels\\nAccuracy={:0.4f}\".format(accuracy))\n",
        "    \n",
        "    # plot the color bar\n",
        "    divider = make_axes_locatable(plt.gca())\n",
        "    cax = divider.append_axes(\"right\", size=0.3, pad=0.2)\n",
        "    plt.colorbar(im, cax=cax)\n",
        "\n",
        "    # close the plot and return the figure\n",
        "    plt.close()\n",
        "    return fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xth5ypVipuwr"
      },
      "source": [
        "### Output the architecture of several pretrained PyTorch models.\n",
        "\n",
        "The following models all have the same high level ResNet architecture.\n",
        "* ResNet-18\n",
        "* ResNet-34\n",
        "* ResNet-50\n",
        "* ResNet-101\n",
        "* ResNet-152\n",
        "* ResNeXt-50-32x4d\n",
        "* Wide ResNet-50-2\n",
        "* Wide ResNet-101-2\n",
        "\n",
        "The following models all have he same high level DenseNet architecture.\n",
        "* Densenet-121\n",
        "* Densenet-169\n",
        "* Densenet-201\n",
        "* Densenet-161\n",
        "\n",
        "The following models all have he same high level Efficient architecture except for the number of MBConvBlock blocks. Printing these models do not show the correct placement of the swish activation layers (even though the implementation of the forward method is correct).\n",
        "* EfficienteNet-B0\n",
        "* EfficienteNet-B1\n",
        "* EfficienteNet-B2\n",
        "* EfficienteNet-B3\n",
        "* EfficienteNet-B4\n",
        "* EfficienteNet-B5\n",
        "* EfficienteNet-B6\n",
        "* EfficienteNet-B7\n",
        "\n",
        "```\n",
        "print_top_level_model_blocks(models.resnet18(), False)\n",
        "print_top_level_model_blocks(models.densenet121(), True)\n",
        "print_top_level_model_blocks(models.vgg11_bn(), True)\n",
        "print_top_level_model_blocks(models.vgg13_bn(), True)\n",
        "print_top_level_model_blocks(models.vgg16_bn(), True)\n",
        "print_top_level_model_blocks(models.vgg19_bn(), True)\n",
        "```\n",
        "\n",
        "The (formatted) output the `print_top_level_model_blocks` statements yields the following.\n",
        "\n",
        "Note:\n",
        "* Groups of Conv2d, BatchNorm, and ReLU layers have been condensed to CNR\n",
        "* Groups of Linear, ReLU, and Dropout layers have been condensed to LRD\n",
        "* EfficienteNet added for comparision\n",
        "\n",
        "```\n",
        "ResNet  | DenseNet      | VGG11_BN       | VGG13_BN       | VGG16_BN       | VGG19_BN         | EfficienteNet\n",
        "------- | ------------- | -------------- | -------------- | -------------- | ---------------- | -------------\n",
        "conv1   | features      | features       | features       | features       | features         | _conv_stem\n",
        "bn1     |   conv0       |   [00-02] CNR  |   [00-02] CNR  |   [00-02] CNR  |   [00-02] CNR    | _bn0\n",
        "relu    |   norm0       |                |   [03-05] CNR  |   [03-05] CNR  |   [03-05] CNR    | _swish\n",
        "maxpool |   relu0       |   [03] MaxPool |   [06] MaxPool |   [06] MaxPool |   [06] MaxPool2d | _blocks\n",
        "layer1  |   pool0       |   [04-06] CNR  |   [07-09] CNR  |   [07-09] CNR  |   [07-09] CNR    |   [0] MBConvBlock\n",
        "layer2  |   denseblock1 |                |   [10-12] CNR  |   [10-12] CNR  |   [10-12] CNR    |   [2] MBConvBlock\n",
        "layer3  |   transition1 |   [07] MaxPool |   [13] MaxPool |   [13] MaxPool |   [13] MaxPool2d |   [3] MBConvBlock\n",
        "layer4  |   denseblock2 |   [08-10] CNR  |   [14-16] CNR  |   [14-16] CNR  |   [14-16] CNR    |    .       .\n",
        "avgpool |   transition2 |   [11-13] CNR  |   [17-19] CNR  |   [17-19] CNR  |   [17-19] CNR    |    .       .\n",
        "fc      |   denseblock3 |                |                |   [20-22] CNR  |   [20-22] CNR    |    .       .\n",
        "        |   transition3 |                |                |                |   [23-25] CNR    |   [N-1] MBConvBlock\n",
        "        |   denseblock4 |   [14] MaxPool |   [20] MaxPool |   [23] MaxPool |   [26] MaxPool2d | _conv_head\n",
        "        |   norm5       |   [15-17] CNR  |   [21-23] CNR  |   [24-26] CNR  |   [27-29] CNR    | _bn1\n",
        "        | classifier    |   [18-20] CNR  |   [24-26] CNR  |   [27-29] CNR  |   [30-32] CNR    | _swish\n",
        "        |               |                |                |   [30-32] CNR  |   [33-35] CNR    | _avg_pooling\n",
        "        |               |                |                |                |   [36-38] CNR    | _dropout\n",
        "        |               |   [21] MaxPool |   [27] MaxPool |   [33] MaxPool |   [39] MaxPool2d | _fc\n",
        "        |               |   [22-24] CNR  |   [28-30] CNR  |   [34-36] CNR  |   [40-42] CNR    | \n",
        "        |               |   [25-27] CNR  |   [31-33] CNR  |   [37-39] CNR  |   [43-45] CNR    | \n",
        "        |               |                |                |   [40-42] CNR  |   [46-48] CNR    | \n",
        "        |               |                |                |                |   [49-51] CNR    | \n",
        "        |               |   [28] MaxPool |   [34] MaxPool |   [43] MaxPool |   [52] MaxPool2d | \n",
        "        |               | avgpool        | avgpool        | avgpool        | avgpool          | \n",
        "        |               | classifier     | classifier     | classifier     | classifier       | \n",
        "        |               |   [00-02] LRD  |   [00-02] LRD  |   [00-02] LRD  |   [00-02] LRD    | \n",
        "        |               |   [03-05] LRD  |   [03-05] LRD  |   [03-05] LRD  |   [03-05] LRD    | \n",
        "        |               |   [06] Linear  |   [06] Linear  |   [06] Linear  |   [06] Linear    | \n",
        "```\n",
        "\n",
        "The `print_top_level_model_blocks` function was also used to test whether I properly implemented the fine tuning code. For example,\n",
        "\n",
        "```\n",
        "model = ResNet18(pretrained=True, tuning_level=0)\n",
        "print_top_level_model_blocks(model._network, include_grandchildren=False, display_requires_grad=True)\n",
        "\n",
        "    ResNet, requires_grad=Mixed\n",
        "      conv1, requires_grad=False\n",
        "      bn1, requires_grad=False\n",
        "      relu, requires_grad=N/A\n",
        "      maxpool, requires_grad=N/A\n",
        "      layer1, requires_grad=False\n",
        "      layer2, requires_grad=False\n",
        "      layer3, requires_grad=False\n",
        "      layer4, requires_grad=False\n",
        "      avgpool, requires_grad=N/A\n",
        "      fc, requires_grad=True#\n",
        "\n",
        "model = ResNet18(pretrained=True, tuning_level=1)\n",
        "print_top_level_model_blocks(model._network, include_grandchildren=False, display_requires_grad=True)\n",
        "\n",
        "    ResNet, requires_grad=Mixed\n",
        "      conv1, requires_grad=False\n",
        "      bn1, requires_grad=False\n",
        "      relu, requires_grad=N/A\n",
        "      maxpool, requires_grad=N/A\n",
        "      layer1, requires_grad=False\n",
        "      layer2, requires_grad=False\n",
        "      layer3, requires_grad=False\n",
        "      layer4, requires_grad=True\n",
        "      avgpool, requires_grad=N/A\n",
        "      fc, requires_grad=True\n",
        "\n",
        "...\n",
        "\n",
        "model = ResNet18(pretrained=True, tuning_level=5)\n",
        "print_top_level_model_blocks(model._network, include_grandchildren=False, display_requires_grad=True)\n",
        "\n",
        "    ResNet, requires_grad=True\n",
        "      conv1, requires_grad=True\n",
        "      bn1, requires_grad=True\n",
        "      relu, requires_grad=N/A\n",
        "      maxpool, requires_grad=N/A\n",
        "      layer1, requires_grad=True\n",
        "      layer2, requires_grad=True\n",
        "      layer3, requires_grad=True\n",
        "      layer4, requires_grad=True\n",
        "      avgpool, requires_grad=N/A\n",
        "      fc, requires_grad=True\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUG75Y54puwr"
      },
      "source": [
        "def get_requires_grad_status(block) -> str:\n",
        "    params = list(block.parameters())\n",
        "    if not params:\n",
        "        return \"N/A\"\n",
        "    \n",
        "    or_of_params = False\n",
        "    and_of_params = True\n",
        "    for param in params:\n",
        "        or_of_params = or_of_params or param.requires_grad\n",
        "        and_of_params = and_of_params and param.requires_grad\n",
        "    if or_of_params and and_of_params:\n",
        "        return \"True\"\n",
        "    elif not or_of_params and not and_of_params:\n",
        "        return \"False\"\n",
        "    else:\n",
        "        return \"Mixed\"\n",
        "\n",
        "\n",
        "def print_top_level_model_blocks(\n",
        "    model:nn.Module, \n",
        "    include_grandchildren:bool = False, \n",
        "    display_requires_grad = False\n",
        "):\n",
        "    status = \"\"\n",
        "    if display_requires_grad:\n",
        "        status = f\", requires_grad={get_requires_grad_status(model)}\"\n",
        "    print(f\"{type(model).__name__}{status}\")\n",
        "    for child in model.named_children():\n",
        "        if display_requires_grad:\n",
        "            status = f\", requires_grad={get_requires_grad_status(child[1])}\"\n",
        "        print(f\"  {child[0]}{status}\")\n",
        "        if include_grandchildren:\n",
        "            for grandchild in child[1].named_children():\n",
        "                if display_requires_grad:\n",
        "                    status = f\", requires_grad={get_requires_grad_status(grandchild[1])}\"\n",
        "                if not grandchild[0].isnumeric():\n",
        "                    print(f\"    { grandchild[0]}{status}\")\n",
        "                else:\n",
        "                    print(f\"    [{grandchild[0]}] {type(grandchild[1]).__name__}{status}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxmMNijlAb9v"
      },
      "source": [
        "def create_submission_file(path, exp, dataloader):\n",
        "    \"\"\"\n",
        "    ToDo: Need to test and execute on the best model.\n",
        "    \"\"\"\n",
        "\n",
        "    # create a dictionary of numeric labels to text labels\n",
        "    label_dict = {key: value \n",
        "        for key, value in zip(range(len(exp.class_names)), exp.class_names)}\n",
        "\n",
        "    # get predictions for the test data using the trained model            \n",
        "    fnames, labels = predict_test_data(exp.trained_model, dataloader, exp.device)\n",
        "\n",
        "    # convert the numeric labels to their text equivalents\n",
        "    labels = [label_dict[label] for label in labels]\n",
        "\n",
        "    # create a pandas data frame and write it to a CSV file\n",
        "    data_frame = pd.DataFrame({\"id\": fnames, \"class\": labels})\n",
        "    data_frame.to_csv(path, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssjl7-QG3-52"
      },
      "source": [
        "## <font style=\"color:green\">7. Experiment [5 Points]</font>\n",
        "\n",
        "Choose your optimizer and LR-scheduler and use the above methods and classes to train your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exu5ZBHV-U-G"
      },
      "source": [
        "### Base Experiment Classes\n",
        "\n",
        "The following base classes facilitate rapid experiment creation.\n",
        "* Experiment - Base class for the following classes.\n",
        "* VisualExperiment - Conduct data visualization experiments.\n",
        "* ModelExperiment - Conduct model training experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_datastore() -> KF13Datastore:\n",
        "    config_system = create_system_config()\n",
        "    config_dataset = create_dataset_config()\n",
        "    setup_system(config_system)\n",
        "    return KF13Datastore(\n",
        "        data_root = config_dataset.data_dir,\n",
        "        valid_size = config_dataset.valid_size,\n",
        "        random_seed = config_system.seed\n",
        "    )\n",
        "\n",
        "datastore = create_datastore()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQXZGHy0CzTZ"
      },
      "source": [
        "class Experiment(ABC):\n",
        "    def __init__(\n",
        "        self,\n",
        "        abbr: Optional[str] = None,\n",
        "        transform_resize: int = 256,\n",
        "        transform_crop_size: int = 224,\n",
        "        **config_overrides\n",
        "    ):\n",
        "        \"\"\"\n",
        "        This base class for data visualization and model training experiment does the following.\n",
        "        \n",
        "            - Creates the master configuration instance accomodating constructor overrides\n",
        "            - Sets up the system, e.g., ensures reproducibility, enables CUDA acceleration, etc.\n",
        "            - Configures experiment visualization \n",
        "        \"\"\"\n",
        "\n",
        "        # the experient abbreviation is not specified, use the class name removing the prefix\n",
        "        # \"Exp\" if present\n",
        "        if abbr is None:\n",
        "            name = type(self).__name__\n",
        "            self._abbr = name\n",
        "            if name.startswith(\"Exp\"):\n",
        "                self._abbr = name[3:]\n",
        "        else:\n",
        "            self._abbr = abbr\n",
        "        \n",
        "        # validate configuration overrides\n",
        "        unknown_overrides = np.setdiff1d(list(config_overrides.keys()), [\n",
        "            \"data_aug_color_enabled\",\n",
        "            \"data_aug_color_brightness\",\n",
        "            \"data_aug_color_contrast\",\n",
        "            \"data_aug_color_saturation\",\n",
        "            \"data_aug_color_hue\",\n",
        "            \"data_aug_horz_flip_prob\",\n",
        "            \"data_aug_vert_flip_prob\",\n",
        "            \"data_aug_affine_enabled\",\n",
        "            \"data_aug_affine_rotation\",\n",
        "            \"data_aug_affine_translate\",\n",
        "            \"data_aug_affine_scale\",\n",
        "            \"data_aug_affine_shear\",\n",
        "            \"data_aug_erasing_prob\",\n",
        "            \"data_aug_erasing_scale\",\n",
        "            \"data_aug_erasing_ratio\",\n",
        "            \"data_aug_erasing_random\",\n",
        "            \"data_aug_noise_prob\",\n",
        "            \"data_aug_noise_mean\",\n",
        "            \"data_aug_noise_std\",\n",
        "            \"data_loader_batch_size\",\n",
        "            \"data_loader_num_workers\",\n",
        "            \"optimizer_learning_rate\",\n",
        "            \"optimizer_momentum\",\n",
        "            \"optimizer_weight_decay\",\n",
        "            \"optimizer_betas\",\n",
        "            \"scheduler_gamma\",\n",
        "            \"scheduler_step_size\",\n",
        "            \"scheduler_milestones\",\n",
        "            \"scheduler_patience\",\n",
        "            \"scheduler_threshold\",\n",
        "            \"trainer_training_epochs\",\n",
        "            \"trainer_model_saving_period\",\n",
        "            \"trainer_stop_loss_epochs\",\n",
        "            \"trainer_stop_acc_epochs\",\n",
        "            \"trainer_stop_acc_ema_alpha\",\n",
        "            \"trainer_stop_acc_threshold\"\n",
        "        ]).tolist()\n",
        "        if len(unknown_overrides) > 0:\n",
        "            raise Exception(f\"Unknown configuration overrides: {unknown_overrides}\")\n",
        "\n",
        "        # ToDo: Apply patch if CUDA is not available.\n",
        "        self._resize = transform_resize\n",
        "        self._crop_size = transform_crop_size\n",
        "        self._config = create_master_config(\n",
        "            transform_resize,\n",
        "            transform_crop_size,\n",
        "            **config_overrides\n",
        "        )\n",
        "        \n",
        "        setup_system(self._config.system)\n",
        "                \n",
        "        self.__visualizer = None\n",
        "        \n",
        "    @abstractproperty\n",
        "    def class_names(self):\n",
        "        pass\n",
        "\n",
        "    @abstractproperty\n",
        "    def _visualizer_name(self) -> str:\n",
        "        pass\n",
        "\n",
        "    def _open_visualizer(self):\n",
        "        if self.__visualizer is None:\n",
        "            self.__visualizer = TensorBoardVisualizer(os.path.join(\n",
        "                self._config.system.proj_dir,\n",
        "                self._config.trainer.visualizer_dir, \n",
        "                self._visualizer_name\n",
        "            ))\n",
        "        return self.__visualizer\n",
        "\n",
        "    def _close_visualizer(self):\n",
        "        if self.__visualizer is not None:\n",
        "            self.__visualizer.close_tensorboard()\n",
        "            self.__visualizer = None\n",
        "\n",
        "\n",
        "class VisualExperiment(Experiment):\n",
        "    \"\"\"\n",
        "    This is the base class for data visualization experiments.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        abbr: Optional[str] = None,\n",
        "        log_originals: bool = True,\n",
        "        log_augmentations:bool = True,\n",
        "        log_augmentations_suffix: str = \"augmented\",\n",
        "        transform_resize: int = 256,\n",
        "        transform_crop_size: int = 224,\n",
        "        **config_overrides\n",
        "    ):\n",
        "        super().__init__(\n",
        "            abbr,\n",
        "            transform_resize,\n",
        "            transform_crop_size,\n",
        "            **config_overrides\n",
        "        )\n",
        "        \n",
        "        self.__library = datastore.library\n",
        "        self.__class_names = datastore.class_names\n",
        "        self.__log_originals = log_originals\n",
        "        self.__log_augmentations = log_augmentations\n",
        "        self.__log_augmentations_suffix = log_augmentations_suffix\n",
        "\n",
        "    def log_sample_images(\n",
        "        self, \n",
        "        num_of_contact_sheets: int = 1, \n",
        "        log_originals: Optional[bool] = None, \n",
        "        log_augmentations: Optional[bool] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Create a 6 x 6 grid of images for each type of food in the data and\n",
        "        log these images to the visualizer.\n",
        "        \"\"\"\n",
        "\n",
        "        if log_originals is None:\n",
        "            log_originals = self.__log_originals\n",
        "\n",
        "        if log_augmentations is None:\n",
        "            log_augmentations = self.__log_augmentations\n",
        "\n",
        "        # abort if not logging either originals or augmentations\n",
        "        if not log_originals and not log_augmentations:\n",
        "            return\n",
        "\n",
        "        visualizer = self._open_visualizer()\n",
        "\n",
        "        for food, fnames in self.__library.items():\n",
        "            \n",
        "            # since we want to visualize the same images before and after data\n",
        "            # augmentation, we need to shuffle the data ourselves rather than\n",
        "            # have the data loader do this for us\n",
        "            shuffled = random.sample(fnames, len(fnames))\n",
        "\n",
        "            # create food specific datasets\n",
        "            original_dataset = KF13Dataset(\n",
        "                image_root = datastore.image_root,\n",
        "                fnames = shuffled,\n",
        "                transform = self._config.dataset.visual_transforms\n",
        "            )\n",
        "            augmented_dataset = KF13Dataset(\n",
        "                image_root = datastore.image_root,\n",
        "                fnames = shuffled,\n",
        "                transform = self._config.dataset.visual_aug_transforms\n",
        "            )\n",
        "\n",
        "            # load 36 original and augmented images\n",
        "            original_dataloader = DataLoader(original_dataset, batch_size=36, shuffle=False)\n",
        "            original_iter = iter(original_dataloader)\n",
        "            augmented_dataloader = DataLoader(augmented_dataset, batch_size=36, shuffle=False)\n",
        "            augmented_iter = iter(augmented_dataloader)\n",
        "\n",
        "            for idx in range(num_of_contact_sheets):\n",
        "                try:\n",
        "                    original_images, _ = next(original_iter)\n",
        "                    augmented_images, _ = next(augmented_iter)\n",
        "\n",
        "                    # save images to project's image directory (needs to exist!)\n",
        "                    # if log_originals:\n",
        "                    #     original_name = f\"{self.__class_names[food]}{(idx + 1):02d}.jpg\"\n",
        "                    #     original_path = os.path.join(proj_dir, \"images\", \"data\", original_name)\n",
        "                    #     torchvision.utils.save_image(original_images, fp=original_path, nrow=6)\n",
        "                    # if log_augmentations:\n",
        "                    #     augmented_name = f\"{self.__class_names[food]}{(idx + 1):02d}_{self.__log_augmentations_suffix}.jpg\"\n",
        "                    #     augmented_path = os.path.join(proj_dir, \"images\", \"data\", augmented_name)\n",
        "                    #     torchvision.utils.save_image(augmented_images, fp=augmented_path, nrow=6)\n",
        "\n",
        "                    # add image grid to visualizer\n",
        "                    if log_originals:\n",
        "                        visualizer.add_image(\n",
        "                            tag=self.__class_names[food], \n",
        "                            image=torchvision.utils.make_grid(original_images, nrow=6)\n",
        "                        )\n",
        "                    if log_augmentations:\n",
        "                        visualizer.add_image(\n",
        "                            tag=self.__class_names[food] + f\" (self.__log_augmentations_suffix)\", \n",
        "                            image=torchvision.utils.make_grid(augmented_images, nrow=6)\n",
        "                        )\n",
        "\n",
        "                except StopIteration:\n",
        "                    break\n",
        "        \n",
        "        self._close_visualizer()\n",
        "\n",
        "    @property\n",
        "    def library(self):\n",
        "        return self.__library\n",
        "\n",
        "    @property\n",
        "    def class_names(self):\n",
        "        return self.__class_names\n",
        "\n",
        "    @property\n",
        "    def _visualizer_name(self) -> str:\n",
        "        return self._abbr + f\"_DV_RS_{self._resize}_CS_{self._crop_size}\"\n",
        "\n",
        "\n",
        "class ModelExperiment(Experiment):\n",
        "    \"\"\"\n",
        "    This is the base class for model training experiments.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        abbr: Optional[str] = None,\n",
        "        optimizer: Optimizer = Optimizer.SGD,\n",
        "        scheduler: LrScheduler = LrScheduler.STEP,\n",
        "        di_approach: DIApproach = DIApproach.WEIGHTED_RANDOM_SAMPLER,\n",
        "        disable_data_aug: bool = False,\n",
        "        use_data_subsets: bool = False,\n",
        "        transform_resize: int = 256,\n",
        "        transform_crop_size: int = 224,\n",
        "        **config_overrides\n",
        "    ):\n",
        "        super().__init__(\n",
        "            abbr,\n",
        "            transform_resize,\n",
        "            transform_crop_size,\n",
        "            **config_overrides\n",
        "        )\n",
        "\n",
        "        self.__use_data_subsets = use_data_subsets\n",
        "        self.__data = self._get_data()\n",
        "        self.__class_names = self.__data.class_names\n",
        "\n",
        "        test_transforms = self._config.dataset.test_transforms\n",
        "        train_transforms = self._config.dataset.train_transforms\n",
        "        if disable_data_aug:\n",
        "            train_transforms = test_transforms\n",
        "\n",
        "        loss_weight = None\n",
        "        use_random_sampler = False\n",
        "        if di_approach == DIApproach.WEIGHTED_LOSS_FUNCTION:\n",
        "            loss_weight = torch.FloatTensor(self.__data.class_weights)\n",
        "        elif di_approach == DIApproach.WEIGHTED_RANDOM_SAMPLER:\n",
        "            use_random_sampler = True\n",
        "\n",
        "        self.__train_loader = self.__data.get_train_data_loader(\n",
        "            transform = train_transforms,\n",
        "            batch_size = self._config.data_loader.batch_size,\n",
        "            num_workers = self._config.data_loader.num_workers,\n",
        "            use_random_sampler = use_random_sampler\n",
        "        )\n",
        "        \n",
        "        self.__valid_loader = self.__data.get_valid_data_loader(\n",
        "            transform = test_transforms,\n",
        "            batch_size = self._config.data_loader.batch_size,\n",
        "            num_workers = self._config.data_loader.num_workers\n",
        "        )                \n",
        "    \n",
        "        self.__model, model_id = self._get_model()\n",
        "        self.__model_name = self._abbr + \"_\" + model_id\n",
        "        self.__model_dir = os.path.join(self._config.system.proj_dir, self._config.trainer.model_dir)\n",
        "        self.__loss_fn = nn.CrossEntropyLoss(weight=loss_weight)\n",
        "        self.__metric_fn = AccuracyEstimator(topk=(1, )) # ToDo: Fix! (trainer.py expects a dictionary w/ 'top1' key)\n",
        "        self.__optimizer = get_optimizer(self.__model, optimizer, self._config.optimizer)\n",
        "        self.__scheduler = get_scheduler(self.__optimizer, scheduler, self._config.scheduler)\n",
        "\n",
        "    @property\n",
        "    def class_names(self):\n",
        "        return self.__class_names\n",
        "\n",
        "    @property\n",
        "    def config(self):\n",
        "        return self._config\n",
        "\n",
        "    @property\n",
        "    def device(self) -> torch.device:\n",
        "        return torch.device(self._config.trainer.device)\n",
        "\n",
        "    @property\n",
        "    def trained_model_path(self):\n",
        "        return os.path.join(self.__model_dir, self.__model_name + \".pt\")\n",
        "    \n",
        "    @property\n",
        "    def trained_model(self) -> nn.Module:\n",
        "        self.__load_model()\n",
        "        return self.__model\n",
        "\n",
        "    @property\n",
        "    def train_loader(self):\n",
        "        return self.__train_loader\n",
        "    \n",
        "    @property\n",
        "    def valid_loader(self):\n",
        "        return self.__valid_loader\n",
        "\n",
        "    def train(self):\n",
        "        device = self.device\n",
        "        self.__model = self.__model.to(device)\n",
        "        self.__loss_fn = self.__loss_fn.to(device)\n",
        "\n",
        "        visualizer = self._open_visualizer()\n",
        "        model_trainer = Trainer(\n",
        "            model=self.__model,\n",
        "            loader_train=self.__train_loader,\n",
        "            loader_test=self.__valid_loader,\n",
        "            loss_fn=self.__loss_fn,\n",
        "            metric_fn=self.__metric_fn,\n",
        "            optimizer=self.__optimizer,\n",
        "            lr_scheduler=self.__scheduler,\n",
        "            model_save_dir=self.__model_dir,\n",
        "            model_name=self.__model_name,\n",
        "            model_saving_period=self._config.trainer.model_saving_period,\n",
        "            stop_loss_epochs=self._config.trainer.stop_loss_epochs,\n",
        "            stop_acc_ema_alpha=self._config.trainer.stop_acc_ema_alpha,\n",
        "            stop_acc_epochs=self._config.trainer.stop_acc_epochs,\n",
        "            stop_acc_threshold=self._config.trainer.stop_acc_threshold,\n",
        "            device=device,\n",
        "            data_getter=itemgetter(0),\n",
        "            target_getter=itemgetter(1),\n",
        "            stage_progress=self._config.trainer.progress_bar,\n",
        "            visualizer=visualizer,\n",
        "            get_key_metric=itemgetter(\"top1\")\n",
        "        )\n",
        "        model_trainer.register_hook(\"end_epoch\", hooks.end_epoch_hook_classification)\n",
        "        metrics = model_trainer.fit(self._config.trainer.training_epochs)\n",
        "        self._close_visualizer()\n",
        "\n",
        "        return metrics\n",
        "    \n",
        "    def log_graph(self):\n",
        "        model = self.trained_model\n",
        "        images, _ = next(iter(self.valid_loader))\n",
        "        device = self.device\n",
        "\n",
        "        visualizer = self._open_visualizer()\n",
        "        visualizer.add_graph(model.to(device), images.to(device))\n",
        "        self._close_visualizer()\n",
        "        \n",
        "    def log_pr_curves(self):\n",
        "        targets, pred_probs = get_targets_and_pred_probs(\n",
        "            self.trained_model, \n",
        "            self.valid_loader,\n",
        "            self.device\n",
        "        )\n",
        "\n",
        "        visualizer = self._open_visualizer()\n",
        "        visualizer.add_pr_curves(self.__class_names, targets, pred_probs)\n",
        "        self._close_visualizer()\n",
        "    \n",
        "    def log_confusion_matrix(self):\n",
        "        targets, preds = predict_valid_data(\n",
        "            self.trained_model,\n",
        "            self.valid_loader,\n",
        "            self.device\n",
        "        )\n",
        "\n",
        "        visualizer = self._open_visualizer()\n",
        "        cm = confusion_matrix(targets, preds)\n",
        "        tag = f\"Confusion Matrix ({self.__model_name})\"\n",
        "        figure = create_confusion_matrix(cm, self.class_names, self.__model_name)\n",
        "        visualizer.add_figure(tag=tag, figure=figure, close=True)\n",
        "        self._close_visualizer()\n",
        "\n",
        "    \"\"\"\n",
        "    Protected methods that may or must be overridden by derived classes.\n",
        "    \"\"\"\n",
        "    \n",
        "    @property\n",
        "    def _visualizer_name(self) -> str:\n",
        "        return self.__model_name\n",
        "\n",
        "    def _get_data(self) -> KF13TrainingData:\n",
        "        return datastore.get_training_data(subset=self.__use_data_subsets)\n",
        "\n",
        "    @abstractmethod\n",
        "    def _get_model(self) -> Tuple[nn.Module, str]:\n",
        "        pass\n",
        "\n",
        "    \"\"\"\n",
        "    Private methods that should only be called by this class.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __load_model(self):\n",
        "        path = self.trained_model_path\n",
        "        if os.path.exists(path):\n",
        "            self.__model.load_state_dict(torch.load(path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1yMscAgpuws"
      },
      "source": [
        "def conduct(\n",
        "    exp: Experiment,\n",
        "    train_model:bool = True, \n",
        "    log_graph:bool = False,\n",
        "    log_pr_curves:bool = False,\n",
        "    log_confusion_matrix:bool = False,\n",
        "    free_experiment:bool = True):   \n",
        "    \"\"\"\n",
        "    This method conducts an visual or model experiment. A visual experiment logs original\n",
        "    and/or augmented images in 6 x 6 contact sheets to TensorBoard. A model experiment\n",
        "    performs the following steps and returns its training metrics.\n",
        "\n",
        "    1. Logs the model's graph.\n",
        "    2. Trains the model.\n",
        "    3. Logs the precision-recall curve for each food class.\n",
        "    4. Logs the confusion matrix.\n",
        "    \n",
        "    Note: Steps 3 and 4 are performed on the validation data set using model weights that\n",
        "          achieved the lowest average loss on the validaton data set.\n",
        "    \"\"\"\n",
        "    result = None\n",
        "\n",
        "    if isinstance(exp, VisualExperiment):\n",
        "        exp.log_sample_images()\n",
        "    \n",
        "    elif isinstance(exp, ModelExperiment):\n",
        "        if train_model:\n",
        "            result = exp.train()\n",
        "        if log_graph:\n",
        "            exp.log_graph()\n",
        "        if log_pr_curves:\n",
        "            exp.log_pr_curves()\n",
        "        if log_confusion_matrix:\n",
        "            exp.log_confusion_matrix()\n",
        "        \n",
        "    if free_experiment:\n",
        "        # after running several experiments, a \"RuntimeError: CUDA out of memory\"\n",
        "        # exception was raised ... trying to see whether explicitly deleting the\n",
        "        # experiment helps\n",
        "        del exp\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3KRsyK6puws"
      },
      "source": [
        "def is_outlier(points, thresh=3.5):\n",
        "    \"\"\"\n",
        "    Returns a boolean array with True if points are outliers and False \n",
        "    otherwise.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "        points : An numobservations by numdimensions array of observations\n",
        "        thresh : The modified z-score to use as a threshold. Observations with\n",
        "            a modified z-score (based on the median absolute deviation) greater\n",
        "            than this value will be classified as outliers.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "        mask : A numobservations-length boolean array.\n",
        "\n",
        "    References:\n",
        "    ----------\n",
        "        Boris Iglewicz and David Hoaglin (1993), \"Volume 16: How to Detect and\n",
        "        Handle Outliers\", The ASQC Basic References in Quality Control:\n",
        "        Statistical Techniques, Edward F. Mykytka, Ph.D., Editor. \n",
        "    \"\"\"\n",
        "    if len(points.shape) == 1:\n",
        "        points = points[:,None]\n",
        "        \n",
        "    median = np.median(points, axis=0)\n",
        "    diff = np.sum((points - median)**2, axis=-1)\n",
        "    diff = np.sqrt(diff)\n",
        "    med_abs_deviation = np.median(diff)\n",
        "\n",
        "    modified_z_score = 0.6745 * diff / med_abs_deviation\n",
        "\n",
        "    return modified_z_score > thresh\n",
        "\n",
        "\n",
        "class AnalyzeRuns(ABC):\n",
        "    \"\"\"\n",
        "    A utility class to do the following for a given TensorBoard run:\n",
        "    \n",
        "        - return a dictionary of scalars\n",
        "        - return accuracy at epoch where loss is lowest\n",
        "        - return overfitting metric\n",
        "        \n",
        "    Note: The overfitting metric is the slope of the test loss divided\n",
        "          by the train loss. A value of zero indicates no overfitting.\n",
        "    \"\"\"\n",
        "    def __init__(self, visualizer_dir):\n",
        "        self._visualizer_dir = visualizer_dir\n",
        "        self.__all_runs = os.listdir(os.path.join(visualizer_dir))\n",
        "\n",
        "    def _get_runs(self, filter:str) -> List[str]:\n",
        "        import re\n",
        "        runs = [run for run in self.__all_runs \n",
        "                            if re.search(filter, run) is not None]\n",
        "        runs.sort()\n",
        "        return runs\n",
        "\n",
        "    def get_scalars(self, run: str):\n",
        "        from tensorboard.backend.event_processing import event_accumulator\n",
        "        path = os.path.join(self._visualizer_dir, run)\n",
        "        event_acc = event_accumulator.EventAccumulator(path)\n",
        "        event_acc.Reload()\n",
        "\n",
        "        scalars = {}\n",
        "        for tag in sorted(event_acc.Tags()[\"scalars\"]):\n",
        "            x, y = [], []\n",
        "            for scalar_event in event_acc.Scalars(tag):\n",
        "                x.append(scalar_event.step)\n",
        "                y.append(scalar_event.value)\n",
        "            scalars[tag] = (np.asarray(x), np.asarray(y))\n",
        "        return scalars\n",
        "\n",
        "    @classmethod\n",
        "    def _get_min_loss(cls, scalars) -> float:\n",
        "        index = np.argmin(scalars[\"data/test_loss\"][1])\n",
        "        return scalars[\"data/test_loss\"][1][index].item()\n",
        "\n",
        "    @classmethod\n",
        "    def _get_auc_loss(cls, scalars) -> float:\n",
        "        return np.sum(scalars[\"data/test_loss\"][1])\n",
        "\n",
        "    @classmethod\n",
        "    def _get_accuracy(cls, scalars) -> float:\n",
        "        index = np.argmin(scalars[\"data/test_loss\"][1])\n",
        "        return scalars[\"data/test_metric:top1\"][1][index].item()\n",
        "\n",
        "    @classmethod\n",
        "    def _get_overfitting_metric(cls, scalars, alpha:float=0.3) -> float:\n",
        "        from numpy.polynomial.polynomial import polyfit\n",
        "        loss_tst = scalars[\"data/test_loss\"][1]\n",
        "        loss_trn = scalars[\"data/train_loss\"][1]\n",
        "        ratio = [tst / trn for tst, trn in zip(loss_tst, loss_trn)]   \n",
        "        _, m = polyfit(list(range(len(ratio))), ratio, 1)\n",
        "        return m\n",
        "\n",
        "    @classmethod\n",
        "    def _ema_smoothing(cls, data, alpha=0.3) -> List[float]:\n",
        "        data_ema = []\n",
        "        last = data[0]\n",
        "        for datum in data:\n",
        "            last = alpha * datum + (1 - alpha) * last\n",
        "            data_ema.append(last)\n",
        "        return data_ema\n",
        "\n",
        "\n",
        "class AnalyzeRunsForOverfitting(AnalyzeRuns):\n",
        "    \"\"\"\n",
        "    A utility class to do the following:\n",
        "    \n",
        "        - return a list of runs matching the filter\n",
        "        - return a dictionary of scalars for a given run\n",
        "        - return accuracy at epoch where loss is lowest for a given run\n",
        "        - return overfitting metric for a given run\n",
        "        - return a summary of all runs\n",
        "        - return a figure for a given run\n",
        "        \n",
        "    Note: The overfitting metric is the slope of the test loss divided\n",
        "          by the train loss. A value of zero indicates no overfitting.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, visualizer_dir:str, filter:str=\"^.*\"):\n",
        "        super().__init__(visualizer_dir)\n",
        "        self.__runs = self._get_runs(filter) \n",
        "\n",
        "    @property\n",
        "    def runs(self) -> List[str]:\n",
        "        return self.__runs\n",
        "    \n",
        "    def get_summary(self) -> List[Tuple[str, float, float]]:\n",
        "        summary = []\n",
        "        for run in self.runs:\n",
        "            scalars = self.get_scalars(run)\n",
        "            if len(scalars.keys()) == 4:\n",
        "                accuracy = AnalyzeRuns._get_accuracy(scalars)\n",
        "                test_loss = AnalyzeRuns._get_min_loss(scalars)\n",
        "                overfitting = AnalyzeRuns._get_overfitting_metric(scalars)\n",
        "                summary.append((run, test_loss, accuracy, overfitting))\n",
        "        return summary\n",
        "\n",
        "    @classmethod\n",
        "    def get_plot(cls, scalars, title, alpha=0.3):\n",
        "        from numpy.polynomial.polynomial import polyfit\n",
        "        accuracy = AnalyzeRuns._get_accuracy(scalars)\n",
        "        loss_tst = scalars[\"data/test_loss\"][1]\n",
        "        loss_trn = scalars[\"data/train_loss\"][1]\n",
        "        loss_tst_ema = AnalyzeRuns._ema_smoothing(loss_tst, alpha)\n",
        "        loss_trn_ema = AnalyzeRuns._ema_smoothing(loss_trn, alpha)\n",
        "\n",
        "        x = list(range(len(loss_tst_ema)))\n",
        "        ratio = [tst / trn for tst, trn in zip(loss_tst, loss_trn)]       \n",
        "        b, m = polyfit(x, ratio, 1)\n",
        "        ratio_bf = [m * x1 + b for x1 in x] \n",
        "\n",
        "        fig = plt.figure(figsize=(8,4))\n",
        "        plt.suptitle(f\"{title} ({accuracy:.2f}%)\")\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.title(\"loss\")\n",
        "        plt.plot(x, loss_trn, color=\"#ffa0a0\")\n",
        "        plt.plot(x, loss_trn_ema, color=\"#ff0000\", label=\"test\")\n",
        "        plt.plot(x, loss_tst, color=\"#a0a0ff\")\n",
        "        plt.plot(x, loss_tst_ema, color=\"#0000ff\", label=\"train\")\n",
        "        plt.legend()\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.title(f\"loss (overfit: {m:.3f})\")\n",
        "        plt.gca().set_ylim([0.0, 4.0])\n",
        "        plt.plot(x, ratio, color=\"#a0ffa0\")\n",
        "        plt.plot(x, ratio_bf, color=\"#00ff00\", label=\"test/train\")\n",
        "        plt.legend()\n",
        "        plt.close()\n",
        "        return fig\n",
        "\n",
        "\n",
        "class AnalyzeRunsForLearningRate(AnalyzeRuns):\n",
        "    \"\"\"\n",
        "    A utility class to do the following:\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, visualizer_dir, filters:List[str]):\n",
        "        super().__init__(visualizer_dir)\n",
        "        self.__sets = [self._get_runs(filter) for filter in filters] \n",
        "\n",
        "    @property\n",
        "    def sets(self) -> List[List[str]]:\n",
        "        return self.__sets\n",
        "    \n",
        "    def get_summary(self, runs: List[str]) -> List[Tuple[str, float, float, float, float]]:\n",
        "        summary = []\n",
        "        for run in runs:\n",
        "            scalars = self.get_scalars(run)\n",
        "            if len(scalars.keys()) == 4:\n",
        "                lr = scalars[\"data/learning_rate\"][1][0].item()\n",
        "                min_loss = AnalyzeRuns._get_min_loss(scalars)\n",
        "                auc_loss = AnalyzeRuns._get_auc_loss(scalars)\n",
        "                accuracy = AnalyzeRuns._get_accuracy(scalars)\n",
        "                summary.append((run, lr, min_loss, auc_loss, accuracy))\n",
        "        return summary\n",
        "\n",
        "    @classmethod\n",
        "    def get_name(cls, summary) -> str:\n",
        "        import re\n",
        "        try:\n",
        "            run = summary[0][0]\n",
        "            name = list(re.split(\"_LR[1-9]E[\\-+]]?[0-9]+\", run)[0])\n",
        "            name[2] = '_'\n",
        "            return \"\".join(name)\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    @classmethod\n",
        "    def get_plot(cls, summary, title:Optional[str]=None):\n",
        "        # convert tuples to list and transpose 2D list\n",
        "        summary = list(map(list, zip(*[list(t) for t in summary])))\n",
        "\n",
        "        # extract arrays to plot\n",
        "        lr = np.array(summary[1])\n",
        "        min_loss = np.array(summary[2])\n",
        "        auc_loss = np.array(summary[3])\n",
        "        accuracy = np.array(summary[4])\n",
        "        min_loss_flt = min_loss[~is_outlier(min_loss)]\n",
        "        auc_loss_flt = auc_loss[~is_outlier(auc_loss)]\n",
        "\n",
        "        # since outliers are eliminated, explicitly set axis limits\n",
        "        xlim = [0.9 * lr[0], 1.1 * lr[-1]]\n",
        "        ylim_min_loss = [0.9 * np.min(min_loss_flt), 1.1 * np.max(min_loss_flt)]\n",
        "        ylim_auc_loss = [0.9 * np.min(auc_loss_flt), 1.1 * np.max(auc_loss_flt)]\n",
        "\n",
        "        fig = plt.figure(figsize=(12,4))\n",
        "        if title is not None:\n",
        "            plt.suptitle(f\"{title}\")\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.title(\"min loss\")\n",
        "        plt.gca().set_xlim(xlim)\n",
        "        plt.gca().set_ylim(ylim_min_loss)\n",
        "        plt.plot(lr, min_loss, \"r\")\n",
        "        plt.xscale('log')\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.title(\"auc loss\")\n",
        "        plt.gca().set_xlim(xlim)\n",
        "        plt.gca().set_ylim(ylim_auc_loss)\n",
        "        plt.plot(lr, auc_loss, \"g\")\n",
        "        plt.xscale('log')\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.title(\"accuracy\")\n",
        "        plt.gca().set_xlim(xlim)\n",
        "        plt.plot(lr, accuracy, \"b\")\n",
        "        plt.xscale('log')\n",
        "        plt.close()\n",
        "        return fig\n",
        "\n",
        "def analyze_runs_for_overfitting(filter: str = \"^.*\"):\n",
        "    config = create_trainer_config()\n",
        "    analyze = AnalyzeRunsForOverfitting(os.path.join(proj_dir, config.visualizer_dir), filter)\n",
        "\n",
        "    # create a markdown table\n",
        "    print(\"|Experiment|Test Loss|Accuracy|Overfitting Metric|\")\n",
        "    print(\"|:---|:---:|:---:|:---:|\")\n",
        "    for item in analyze.get_summary():\n",
        "        print(f\"|{item[0]}|{item[1]:.3f}|{item[2]:.2f}|{item[3]:.3f}|\")\n",
        "\n",
        "    # save loss figures ... destination directory must exist!\n",
        "    for run in analyze.runs:\n",
        "        try:\n",
        "            path = os.path.join(proj_dir, \"images\", \"loss\", run + \".png\")\n",
        "            scalars = analyze.get_scalars(run)\n",
        "            if len(scalars.keys()) == 4:\n",
        "                fig = AnalyzeRunsForOverfitting.get_plot(scalars, run)\n",
        "                fig.savefig(path, facecolor=\"#ffffff\")\n",
        "                plt.close(fig)\n",
        "        except:\n",
        "            print(f\"Could not plot '{run}' for overfitting analysis.\")\n",
        "\n",
        "def analyze_runs_for_learning_rate(filters: List[str]):\n",
        "    config = create_trainer_config()\n",
        "    analyze = AnalyzeRunsForLearningRate(os.path.join(proj_dir, config.visualizer_dir), filters)\n",
        "    sets = analyze.sets\n",
        "\n",
        "    for (filter, exp_set) in zip(filters, sets):\n",
        "        # create a markdown tables \n",
        "        print(f\"**Experiment Set: {filter}**\")\n",
        "        print()\n",
        "        print(\"|Experiment|Learning Rate|Min Loss|AUC Loss|Accuracy|\")\n",
        "        print(\"|:---|:---:|:---:|:---:|:---:|\")\n",
        "        summary = analyze.get_summary(exp_set)\n",
        "        for item in summary:\n",
        "            print(f\"|{item[0]}|{item[1]:.2e}|{item[2]:.2f}|{item[3]:.2f}|{item[4]:.2f}|\")\n",
        "        print()\n",
        "\n",
        "        # save learning rate figures ... destination directory must exist\n",
        "        name = AnalyzeRunsForLearningRate.get_name(summary)\n",
        "        path = os.path.join(proj_dir, \"images\", \"learning_rate\", name + \".png\")\n",
        "        try:\n",
        "            fig = AnalyzeRunsForLearningRate.get_plot(summary, f\"Experiment Filter: {filter}\")\n",
        "            fig.savefig(path, facecolor=\"#ffffff\")\n",
        "            plt.close(fig)\n",
        "        except:\n",
        "            print(f\"Could not plot '{filter}' for learning rate analysis.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLi5W6s06tZO"
      },
      "source": [
        "## Experiment Group A: Data Visualization and Training Pipeline Check\n",
        "\n",
        "### Introduction\n",
        "\n",
        "This first set of experiments logs contact sheets, 6 x 6 grids of images, of each food type to the visualizer with and without data augmentation.\n",
        "\n",
        "The second set of experiments train only the classifier layer (fc) of the pre-trained Resnet18 model using a subset of the data without augmentation to check the training pipeline. In the first experiment, training will stop after 100 epochs. In the second experiment, training will stop after 100 epochs or when the smoothed accuracy (computed by [expontential moving average](https://towardsdatascience.com/moving-averages-in-python-16170e20f6c#144e) with an alpha = 0.3) does not decrease by 2% within 10 epochs.\n",
        "\n",
        "The third set of experiments varies the number of data loader worker threads to determine the optimal number for future experiments. These experiments stop after 11 epochs. The time between logging the first and eleventh epochs' test metrics divided by 10 will be used to evaluate data loading efficiency. Since the purpose to evaluate data loading, not the forward/back-propogation training cycle, a smaller model, ResNet18, was used. Furthermore, saving the model's state is disabled to eliminate its time contribution.\n",
        "\n",
        "### Results\n",
        "\n",
        "I used the data augmentation transforms I created for project 1. The data validation experiment revealed issues with these transforms. First, the color jitter transform dramatically changed the image's color. While this was not detrimental to classifying cats, dogs, and pandas; I suspect it may reduce accuracy on the KenyanFood13 dataset. Second, the amount of translation and scaling was too agressive.\n",
        "\n",
        "To properly set the data augmentation transform parameters, I ran several experiments not shown in this notebook. These experiments disabled all but one type of augmentation in order to \"tune\" it. For example, to properly set the hue parameter of the color jitter transform, I disabled the horizontal/vertical flips, affine, and erase transforms. Furthermore, I set the color jitter's brightness, contrast, and saturation to the values that would produce the original image. I then found acceptable minimum and maximum values for the hue parameter. After conducting all of these data augmentation tuning experiments, I updated the configuration file and re-ran the data visualization experiment. I visualized the entire dataset to external files, but only logged the following 6 x 6 contact sheets to Tensorboard.\n",
        "\n",
        "* ExpAAA - Original Images\n",
        "* ExpAAA - \"Tuned\" Augmented Images\n",
        "* ExpAAB - \"Untuned\" Augmented Images\n",
        "\n",
        "The training pipeline check experiment performed as expected. The training and test loss decreased and the accuracy increased to approximtely 60%.\n",
        "\n",
        "The results of the data loader experiments are shown below. For a small number of data loader threads, adding additional threads significantly increases the data loader's efficiency thereby allowing it to keep up with the GPU. However, past six or seven threads, the impact is minimal on a small model.\n",
        "\n",
        "**Table A1.** The impact of the number of data loader worker threads on a training cycle.\n",
        "\n",
        "|Experiment|Workers|Time/Epoch|\n",
        "|:---:|:---:|:---:|\n",
        "|ACA|1|02:23|\n",
        "|ACB|2|01:16|\n",
        "|ACC|3|00:53|\n",
        "|ACD|4|00:40|\n",
        "|ACE|5|00:33|\n",
        "|ACF|6|00:28|\n",
        "|ACG|7|00:25|\n",
        "|ACH|8|00:22|\n",
        "|ACI|9|00:22|\n",
        "|ACJ|10|00:22|\n",
        "|ACK|11|00:21|\n",
        "|ACL|12|00:21|\n",
        "|ACM|13|00:20|\n",
        "|ACN|14|00:19|\n",
        "|ACO|15|00:18|\n",
        "|ACP|16|00:18|\n",
        "\n",
        "### Conclusions\n",
        "\n",
        "The data augmentation looks reasonable. The training pipeline appears to work as expected. Since I am training models on a workstation with 8 hyper-threaded cores for a total of 16 CPUs and GPU acceleration, I dedicated 8 to 12 worker threads to the data loader. As the model complexity increases, the impact of fewer worker threads lessens because each worker has more time to load and transform images before the GPU requires the next batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nCwE0ULBBU6"
      },
      "source": [
        "class ExpAAA(VisualExperiment):\n",
        "    \"\"\"\n",
        "    Data Visualization Experiment\n",
        "    Log original and augmented images using \"tuned\" parameters.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "class ExpAAB(VisualExperiment):\n",
        "    \"\"\"\n",
        "    Data Visualization Experiment\n",
        "    Log augmented images using Project 1's \"untuned\" parameters.\n",
        "    \"\"\"   \n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            log_originals = False,\n",
        "            log_augmentations = True,\n",
        "            data_aug_color_enabled = True,\n",
        "            data_aug_color_brightness = (0.75, 1.25),\n",
        "            data_aug_color_contrast = (0.75, 1.25),\n",
        "            data_aug_color_saturation = (0.75, 1.25),\n",
        "            data_aug_color_hue = (-0.25, 0.25),\n",
        "            data_aug_horz_flip_prob = 0.5,\n",
        "            data_aug_vert_flip_prob = 0.5,\n",
        "            data_aug_affine_enabled = True,\n",
        "            data_aug_affine_rotation = 45,\n",
        "            data_aug_affine_translate = (0.2, 0.2),\n",
        "            data_aug_affine_scale = (0.8, 1.2),\n",
        "            data_aug_affine_shear = (0.0, 0.0),\n",
        "            data_aug_erasing_prob = 0.5,\n",
        "            data_aug_erasing_scale = (0.02, 0.33),\n",
        "            data_aug_erasing_ratio = (0.3, 3.3)\n",
        "        )\n",
        "\n",
        "class ExpABA(ModelExperiment):\n",
        "    \"\"\"\n",
        "    Training Pipeline Check\n",
        "    Stop after 40 epoochs\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            use_data_subsets = True,\n",
        "            disable_data_aug = True,\n",
        "            trainer_training_epochs = 100,\n",
        "            scheduler_step_size = 20\n",
        "        )\n",
        "    def _get_model(self):\n",
        "        return ResNet18(pretrained=True, tuning_level=0), \"ResNet18-PT0\"\n",
        "\n",
        "class ExpABB(ModelExperiment):\n",
        "    \"\"\"\n",
        "    Training Pipeline Check\n",
        "    Stop after 40 epochs or where the accuracy increases little.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            use_data_subsets = True,\n",
        "            disable_data_aug = True,\n",
        "            trainer_training_epochs = 100, \n",
        "            scheduler_step_size = 20,\n",
        "            trainer_stop_acc_epochs = 10,\n",
        "            trainer_stop_acc_ema_alpha = 0.3,\n",
        "            trainer_stop_acc_threshold = 2.0\n",
        "        )\n",
        "    def _get_model(self):\n",
        "        return ResNet18(pretrained=True, tuning_level=0), \"ResNet18-PT0\"\n",
        "\n",
        "class ExpAC_(ModelExperiment):\n",
        "    \"\"\"\n",
        "    Data Loader Optimization\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        exp_id: str, # expects a single uppercase letter\n",
        "        data_loader_num_workers: int\n",
        "    ):\n",
        "        super().__init__(\n",
        "            abbr = \"AC\" + exp_id,\n",
        "            scheduler_step_size = 20,\n",
        "            trainer_training_epochs = 11,\n",
        "            trainer_model_saving_period = -1, # disable\n",
        "            data_loader_num_workers = data_loader_num_workers\n",
        "        )\n",
        "    def _get_model(self):\n",
        "        return ResNet18(pretrained=True, tuning_level=0), \"ResNet18-PT0\"\n",
        "\n",
        "def conduct_group_A():\n",
        "    # set 1 - visualize data\n",
        "    conduct(ExpAAA())\n",
        "    conduct(ExpAAB())\n",
        "    # set 2 - check trainer pipeline\n",
        "    conduct(ExpABA())\n",
        "    conduct(ExpABB())\n",
        "    # set 3 - optimizer data loader\n",
        "    conduct(ExpAC_('A', 1))\n",
        "    conduct(ExpAC_('B', 2))\n",
        "    conduct(ExpAC_('C', 3))\n",
        "    conduct(ExpAC_('D', 4))\n",
        "    conduct(ExpAC_('E', 5))\n",
        "    conduct(ExpAC_('F', 6))\n",
        "    conduct(ExpAC_('G', 7))\n",
        "    conduct(ExpAC_('H', 8))\n",
        "    conduct(ExpAC_('I', 9))\n",
        "    conduct(ExpAC_('J', 10))\n",
        "    conduct(ExpAC_('K', 11))\n",
        "    conduct(ExpAC_('L', 12))\n",
        "    conduct(ExpAC_('M', 13))\n",
        "    conduct(ExpAC_('N', 14))\n",
        "    conduct(ExpAC_('O', 15))\n",
        "    conduct(ExpAC_('P', 16))       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Experiment Group B: Learning Rate\n",
        "\n",
        "### Introduction\n",
        "\n",
        "The purpose of these experiments is to explore the impact of learning rate on training the following pretrained models.\n",
        "\n",
        "* EfficientNet-B0 (Set A)\n",
        "* EfficientNet-B2 (Set B)\n",
        "* EfficientNet-B4 (Set C)\n",
        "\n",
        "The entire network was trained, i.e., no layers were frozen. Each model was trained for 10 epochs with learning rates that varied between 1.00e-06 and 1.00e-02 with 4 learning rates per decade.\n",
        "\n",
        "### Results\n",
        "\n",
        "For each set of experiments, the following metrics were plotted as a function of learning rate in Figures B1, B2, and B3.\n",
        "\n",
        "* Minimum test loss (min loss)\n",
        "* Area under the test loss curve (auc loss)\n",
        "* Test accuracy at the epoch of minimum test loss (accuracy)\n",
        "\n",
        "![EfficientNet-B0 learning rate results](https://media.githubusercontent.com/media/blazingcayenne/deep_learning_with_pytorch_project2/main/images/BA__EfficientNetB0_PT5.png?raw=true)<br>\n",
        "**Figure B1.** The impact of learning rate on the EfficientNet-B0 model.\n",
        "\n",
        "![EfficientNet-B2 learning rate results](https://media.githubusercontent.com/media/blazingcayenne/deep_learning_with_pytorch_project2/main/images/BB__EfficientNetB2_PT5.png?raw=true)<br>\n",
        "**Figure B2.** The impact of learning rate on the EfficientNet-B2 model.\n",
        "\n",
        "![EfficientNet-B4 learning rate results](https://media.githubusercontent.com/media/blazingcayenne/deep_learning_with_pytorch_project2/main/images/BC__EfficientNetB4_PT5.png?raw=true)<br>\n",
        "**Figure B3.** The impact of learning rate on the EfficientNet-B4 model.\n",
        "\n",
        "Table B1 enumerates the aforementioned metrics for each set's experiment with the lowest test loss.\n",
        "\n",
        "**Table B1.** The impact of learning rate on the EfficientNet models.\n",
        "\n",
        "|Experiment|Learning Rate|Min Loss|AUC Loss|Accuracy|\n",
        "|:---|:---:|:---:|:---:|:---:|\n",
        "|BAK_EfficientNetB0_PT5_LR3E-4|3.16e-04|0.76|9.69|75.22|\n",
        "|BBI_EfficientNetB2_PT5_LR1E-4|1.00e-04|0.69|8.51|77.63|\n",
        "|BCH_EfficientNetB4_PT5_LR6E-5|5.62e-05|0.57|6.80|81.88|\n",
        "\n",
        "### Conclusions\n",
        "\n",
        "The optimal learning rate appears to slightly decrease as the model complexity increases. Hence, a maximum learning rate of 1.00e-04 will be used in future EfficientNet experiments."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ExpB__(ModelExperiment):\n",
        "    def __init__(\n",
        "        self,\n",
        "        set_id: str, # expects a single uppercase letter\n",
        "        exp_id: str, # expects a single uppercase letter\n",
        "        pretrained: bool,\n",
        "        tuning_level: int,\n",
        "        learning_rate: float,\n",
        "        model_type: TorchVisionModel,\n",
        "        model_abbr: str\n",
        "    ):\n",
        "        image_size = get_image_size(model_type)\n",
        "        batch_size = get_batch_size(model_type)\n",
        "        tuning_suffix = get_tuning_suffix(pretrained, tuning_level)\n",
        "        learning_rate_suffix = get_learning_rate_suffix(learning_rate)\n",
        "\n",
        "        self.__pretrained = pretrained\n",
        "        self.__tuning_level = tuning_level\n",
        "        self.__model_type = model_type\n",
        "        self.__model_abbr = model_abbr + tuning_suffix + learning_rate_suffix\n",
        "\n",
        "        super().__init__(\n",
        "            abbr = 'B' + set_id + exp_id,\n",
        "            transform_resize = int(image_size * 16 / 14),\n",
        "            transform_crop_size = image_size,\n",
        "            data_loader_num_workers = 12,\n",
        "            data_loader_batch_size = batch_size,\n",
        "            optimizer = Optimizer.ADAM,\n",
        "            optimizer_learning_rate = learning_rate,\n",
        "            scheduler_step_size = 200, # disable scheduler\n",
        "            trainer_training_epochs = 10\n",
        "        )\n",
        "\n",
        "    def _get_model(self):\n",
        "        return self.__model_type(self.__pretrained, self.__tuning_level), self.__model_abbr\n",
        "\n",
        "def conduct_group_B():\n",
        "    set_id = 'A'\n",
        "    for model_type in [EfficientNetB0, EfficientNetB2, EfficientNetB4]:\n",
        "        model_abbr = model_type.__name__\n",
        "        exp_id = 'A'\n",
        "        for idx in range(17):\n",
        "            learning_rate = 10**((idx - 24) / 4.0)\n",
        "            conduct(ExpB__(set_id, exp_id, True, 5, learning_rate, model_type, model_abbr))\n",
        "            exp_id = chr(ord(exp_id[0]) + 1)\n",
        "        set_id = chr(ord(set_id[0]) + 1)"
      ]
    },
    {
      "source": [
        "## Experiment Group C: Model Size\n",
        "\n",
        "### Introduction\n",
        "\n",
        "The purpose of these experiments is to explore the impact of EfficientNet model size on bias and variance. If the model is too small for the dataset, then the accuracy will suffer (high bias). However, if the model is too large for the dataset, then it will overfit the data (high variance). Hence, it is important to select the appropriately sized model for the KenyanFood13 dataset. The following pretrained EfficientNet models will be evaluated.\n",
        "\n",
        "* EfficientNet-B3 (Set A)\n",
        "* EfficientNet-B4 (Set B)\n",
        "* EfficientNet-B5 (Set C)\n",
        "* EfficientNet-B6 (Set D)\n",
        "\n",
        "Each of the models under test will be trained using the following approaches to test the strategies given by [Marcelino](https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751) and [Gupta](https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/).\n",
        "\n",
        "* Only the classifier is trained (\\_PT0).\n",
        "* The classifier and ~ 40% of convolutional base is trained (\\_PT2).\n",
        "* The entire network is trained (\\_PT5).\n",
        "\n",
        "The models were trained for 35 epochs. The learning rate started at 1.00E-04 and was multipied by the $\\sqrt{0.1}$ every 10 epochs.\n",
        "\n",
        "### Results\n",
        "\n",
        "The test accuracy at the epoch where the test loss is lowest is used to compare model bias. An overfitting metric was developed to compare model variance. The overfitting metric is computed as follows.\n",
        "\n",
        "* The test loss is divided by the training loss at each epoch.\n",
        "* The loss ratios are fitted to a line (polynomial of order 1).\n",
        "* The overfitting metric is the slope of the best fit line.\n",
        "\n",
        "The overfitting metric is zero is the test loss decreases at the same rate as the training loss. It increases as the test loss decreases at a slower rate than the training loss.\n",
        "\n",
        "Tables C1, C2, and C3 enumerate the minimum test loss, test accuracy at the epoch with minimum test less, and overfitting metric for the three aforementioned transfer learning strategies. Figures C1, C2, C3 are loss plots from three sample runs. The first sample does not depict any overfitting. The second and third samples do show some overfitting.\n",
        "\n",
        "**Table C1:** Analysis of runs where only the classifier is trained.\n",
        "\n",
        "|Experiment|Test Loss|Accuracy|Overfitting Metric|\n",
        "|:---|:---:|:---:|:---:|\n",
        "|CAA_EfficientNetB3_PT0|1.546|54.96|-0.001|\n",
        "|CBA_EfficientNetB4_PT0|1.456|55.43|-0.001|\n",
        "|CCA_EfficientNetB5_PT0|1.503|55.12|-0.001|\n",
        "|CDA_EfficientNetB6_PT0|1.529|54.13|-0.002|\n",
        "<br>\n",
        "\n",
        "**Table C2:** Analysis of runs where classifier and ~ 40% of convolutional base is trained.\n",
        "\n",
        "|Experiment|Test Loss|Accuracy|Overfitting Metric|\n",
        "|:---|:---:|:---:|:---:|\n",
        "|CAB_EfficientNetB3_PT2|0.673|78.11|0.075|\n",
        "|CBB_EfficientNetB4_PT2|0.619|80.89|0.118|\n",
        "|CCB_EfficientNetB5_PT2|0.640|80.05|0.130|\n",
        "|CDB_EfficientNetB6_PT2|0.744|79.51|0.142|\n",
        "<br>\n",
        "\n",
        "**Table C3:** Analysis of runs where the entire network is trained.\n",
        "\n",
        "|Experiment|Test Loss|Accuracy|Overfitting Metric|\n",
        "|:---|:---:|:---:|:---:|\n",
        "|CAC_EfficientNetB3_PT5|0.615|81.26|0.125|\n",
        "|CBC_EfficientNetB4_PT5|0.601|82.03|0.195|\n",
        "|CCC_EfficientNetB5_PT5|0.576|82.11|0.208|\n",
        "|CDC_EfficientNetB6_PT5|0.605|81.88|0.203|\n",
        "<br>\n",
        "\n",
        "![EfficientNet-B4 classifier only training loss plots](https://media.githubusercontent.com/media/blazingcayenne/deep_learning_with_pytorch_project2/main/images/CBA_EfficientNetB4_PT0.png?raw=true)<br>\n",
        "**Figure C1:** The loss plots of training only the classifier of a pretrained EfficientNet-B4 model.<br>\n",
        "\n",
        "![EfficientNet-B4 entire network training loss plots](https://media.githubusercontent.com/media/blazingcayenne/deep_learning_with_pytorch_project2/main/images/CBC_EfficientNetB4_PT5.png?raw=true)<br>\n",
        "**Figure C2:** The loss plots of training the entire network of a pretrained EfficientNet-B4 model.<br>\n",
        "\n",
        "![EfficientNet-B6 entire network training loss plots](https://media.githubusercontent.com/media/blazingcayenne/deep_learning_with_pytorch_project2/main/images/CDC_EfficientNetB6_PT5.png?raw=true)<br>\n",
        "**Figure C3:** The loss plots of training the entire network of a pretrained EfficientNet-B6 model.\n",
        "\n",
        "### Conclusions\n",
        "\n",
        "Training only the classifier of the EfficientNet models has a higher bias (lower accuracy) than training part of all of the convolutional base. Surprisingly, training the entire convolutional base yields slightly better accuracy than training the last 40% of the convolutional base. Regarding model complexity, the EfficientNet-B4 model seems appropriately sized."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVC8J7OXpuwv"
      },
      "source": [
        " class ExpC__(ModelExperiment):\n",
        "    def __init__(\n",
        "        self,\n",
        "        set_id: str, # expects a single uppercase letter\n",
        "        exp_id: str, # expects a single uppercase letter\n",
        "        pretrained: bool,\n",
        "        tuning_level: int,\n",
        "        model_type: TorchVisionModel,\n",
        "        model_abbr: str\n",
        "    ):\n",
        "        image_size = get_image_size(model_type)\n",
        "        batch_size = get_batch_size(model_type)\n",
        "        tuning_suffix = get_tuning_suffix(pretrained, tuning_level)\n",
        "\n",
        "        self.__pretrained = pretrained\n",
        "        self.__tuning_level = tuning_level\n",
        "        self.__model_type = model_type\n",
        "        self.__model_abbr = model_abbr + tuning_suffix\n",
        "\n",
        "        super().__init__(\n",
        "            abbr = 'C' + set_id + exp_id,\n",
        "            transform_resize = int(image_size * 16 / 14),\n",
        "            transform_crop_size = image_size,\n",
        "            data_loader_num_workers = 12,\n",
        "            data_loader_batch_size = batch_size,\n",
        "            optimizer = Optimizer.ADAM,\n",
        "            optimizer_learning_rate = 1.0E-04,\n",
        "            scheduler = LrScheduler.MULTI_STEP,\n",
        "            scheduler_gamma = math.sqrt(0.1),\n",
        "            scheduler_milestones = [10, 20, 30],\n",
        "            trainer_training_epochs = 35\n",
        "        )\n",
        "\n",
        "    def _get_model(self):\n",
        "        return self.__model_type(self.__pretrained, self.__tuning_level), self.__model_abbr\n",
        "\n",
        "def conduct_group_C():\n",
        "    set_id = 'A'\n",
        "    for model_type in [EfficientNetB3, EfficientNetB4, EfficientNetB5, EfficientNetB6]:\n",
        "        model_abbr = model_type.__name__\n",
        "        conduct(ExpC__(set_id, 'A', True, 0, model_type, model_abbr))\n",
        "        conduct(ExpC__(set_id, 'B', True, 2, model_type, model_abbr))\n",
        "        conduct(ExpC__(set_id, 'C', True, 5, model_type, model_abbr))\n",
        "        set_id = chr(ord(set_id[0]) + 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Experiment Group D: Increased Regularization and Dataset Imbalance Approaches\n",
        "\n",
        "### Introduction\n",
        "\n",
        "This group of experiments explores the impact of increased regularization and dataset imbalance approaches. Because this project explores pretrained models, regularization options are limited. Gathering more data is not an option due to time constraints and lack of familarity with Kenyan food. Feasible regularization options include additional data augmentation and L2 / Ridge Regression.\n",
        "\n",
        "This group has four sets.\n",
        "\n",
        "* Set A explores increased data augmentation.\n",
        "* Set B explores increased L2 regularization.\n",
        "* Set C explores data imbalance approaches.\n",
        "* Set D combines the \"best\" of the above.\n",
        "\n",
        "Data augmentation was increased by adding shear, random color erasing, and Gaussian noise. Figures D1, D2, and D3 depict nine samples of chapati with no data augmentation, default data augmentation, and enhanced data augmentation respecitivity.\n",
        "\n",
        "![3x3 grid of chapati with no data augmentations](https://media.githubusercontent.com/media/blazingcayenne/deep_learning_with_pytorch_project2/main/images/chapati01_.jpg?raw=true)<br>\n",
        "**Figure D1:** Nine samples of chapati with no data augmentation.\n",
        "\n",
        "![3x3 grid of chapati with default data augmentations](https://media.githubusercontent.com/media/blazingcayenne/deep_learning_with_pytorch_project2/main/images/chapati01_aug.jpg?raw=true)<br>\n",
        "**Figure D2:** Nine samples of chapati with default data augmentation.\n",
        "\n",
        "![3x3 grid of chapati with enhanced data augmentations](https://media.githubusercontent.com/media/blazingcayenne/deep_learning_with_pytorch_project2/main/images/chapati01_enh.jpg?raw=true)<br>\n",
        "**Figure D3:** Nine samples of chapati with enhanced data augmentation.\n",
        "\n",
        "### Results\n",
        "\n",
        "Table D1 enumerates the results of the Group D experiments. Both the default and enhanced data augmentation reduced overfitting and increased the model's accuracy. The enhanced data augmentation reduced overfitting better than the default data augmentation at the expense of accuracy. L2 regularization likewise reduced overfitting at the expense of accuracy. Igoring dataset imbalance or using a weighted loss function slightly reduced accuracy. Combining a small increase in L2 regularization along with enhanced data augmentation slighly decreased accuracy.\n",
        "\n",
        "**Table D1:** Analysis of Group D Experiments.\n",
        "\n",
        "|Experiment|Test Loss|Accuracy|Overfitting Metric|\n",
        "|:---|:---:|:---:|:---:|\n",
        "|DAA_EfficientNetB4_DA_NON|0.722|77.37|1.238|\n",
        "|DAB_EfficientNetB4_DA_DEF|0.551|82.87|0.076|\n",
        "|DAC_EfficientNetB4_DA_ENH|0.584|80.50|0.056|\n",
        "|DBA_EfficientNetB4_WD_E-3|0.551|81.80|0.080|\n",
        "|DBB_EfficientNetB4_WD-E-2|0.622|79.74|0.042|\n",
        "|DBC_EfficientNetB4_WD_E-1|1.060|68.12|0.006|\n",
        "|DBD_EfficientNetB4_WD_E-0|2.005|36.09|0.004|\n",
        "|DCA_EfficientNetB4_DI_NON|0.528|81.88|0.068|\n",
        "|DCB_EfficientNetB4_DI_WLF|0.556|81.19|0.065|\n",
        "|DDA_EfficientNetB4_AS_DEF|0.551|81.80|0.059|\n",
        "|DDB_EfficientNetB4_AS_ENH|0.575|81.80|0.042|\n",
        "\n",
        "### Conclusions\n",
        "\n",
        "None of the experiments in this group improved the model's accuracy. Surprisingly, increasing regularization had a small detrimental impact on accuracy."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_aug_enhanced = {\n",
        "    \"data_aug_affine_shear\": (-20., 20.),\n",
        "    \"data_aug_erasing_random\": True,\n",
        "    \"data_aug_noise_prob\": 0.5,\n",
        "    \"data_aug_noise_std\": 0.1\n",
        "}\n",
        "\n",
        "class DataAugMode(Enum):\n",
        "    NONE = auto()\n",
        "    DEFAULT = auto()\n",
        "    ENHANCED = auto()\n",
        "\n",
        "class ExpDV_(VisualExperiment):\n",
        "    def __init__(\n",
        "        self,\n",
        "        exp_id: str, # expects a single uppercase letter\n",
        "        aug_mode: DataAugMode = DataAugMode.DEFAULT\n",
        "    ):\n",
        "        config_overrides = {}\n",
        "        if aug_mode == DataAugMode.NONE:\n",
        "            suffix = \"aug\"\n",
        "            data_aug = False\n",
        "        elif aug_mode == DataAugMode.DEFAULT:\n",
        "            suffix = \"aug\"\n",
        "            data_aug = True\n",
        "        elif aug_mode == DataAugMode.ENHANCED:\n",
        "            suffix = \"enh\"\n",
        "            data_aug = True\n",
        "            config_overrides.update(data_aug_enhanced)\n",
        "\n",
        "        super().__init__(\n",
        "            abbr = 'DV' + exp_id,\n",
        "            log_originals = not data_aug,\n",
        "            log_augmentations = data_aug,\n",
        "            log_augmentations_suffix = suffix,\n",
        "            **config_overrides\n",
        "        )\n",
        "\n",
        "\n",
        "class ExpD__(ModelExperiment):\n",
        "    def __init__(\n",
        "        self,\n",
        "        set_id: str, # expects a single uppercase letter\n",
        "        exp_id: str, # expects a single uppercase letter\n",
        "        model_suffix: str,\n",
        "        aug_mode: DataAugMode = DataAugMode.DEFAULT,\n",
        "        di_approach: DIApproach = DIApproach.WEIGHTED_RANDOM_SAMPLER,\n",
        "        weight_decay: float = 0.0001,\n",
        "        alt_scheduler: bool = False\n",
        "    ):\n",
        "        pretrained = True\n",
        "        tuning_level = 5\n",
        "        model_type = EfficientNetB4\n",
        "        model_abbr = model_type.__name__\n",
        "        image_size = get_image_size(model_type)\n",
        "        batch_size = get_batch_size(model_type)\n",
        "\n",
        "        self.__pretrained = pretrained\n",
        "        self.__tuning_level = tuning_level\n",
        "        self.__model_type = model_type\n",
        "        self.__model_abbr = model_abbr + model_suffix\n",
        "\n",
        "        config_overrides = {\n",
        "            \"data_loader_num_workers\": 12,\n",
        "            \"data_loader_batch_size\": batch_size,\n",
        "            \"optimizer\": Optimizer.ADAM,\n",
        "            \"optimizer_learning_rate\": 1.0E-04 * math.sqrt(0.1),\n",
        "            \"optimizer_weight_decay\": weight_decay,\n",
        "        }\n",
        "\n",
        "        if not alt_scheduler:\n",
        "            scheduler = LrScheduler.STEP\n",
        "            config_overrides.update({\n",
        "                \"scheduler_step_size\": 200, # disable scheduler\n",
        "                \"trainer_training_epochs\": 20\n",
        "            })\n",
        "        else:\n",
        "            scheduler = LrScheduler.MULTI_STEP\n",
        "            config_overrides.update({\n",
        "                \"scheduler_gamma\": math.sqrt(0.1),\n",
        "                \"scheduler_milestones\": (20, 30, 40, 50),\n",
        "                \"trainer_training_epochs\": 60\n",
        "            })\n",
        "\n",
        "        if aug_mode == DataAugMode.NONE:\n",
        "            disable_data_aug = True\n",
        "        elif aug_mode == DataAugMode.DEFAULT:\n",
        "            disable_data_aug = False\n",
        "        elif aug_mode == DataAugMode.ENHANCED:\n",
        "            disable_data_aug = False\n",
        "            config_overrides.update(data_aug_enhanced)\n",
        "\n",
        "        super().__init__(\n",
        "            abbr = 'D' + set_id + exp_id,\n",
        "            scheduler = scheduler,\n",
        "            transform_resize = int(image_size * 16 / 14),\n",
        "            transform_crop_size = image_size,\n",
        "            disable_data_aug = disable_data_aug,\n",
        "            di_approach = di_approach,\n",
        "            **config_overrides\n",
        "        )\n",
        "\n",
        "    def _get_model(self):\n",
        "        return self.__model_type(self.__pretrained, self.__tuning_level), self.__model_abbr\n",
        "\n",
        "def conduct_group_D():\n",
        "    conduct(ExpD__('A', 'A', \"_DA_NON\", aug_mode=DataAugMode.NONE))\n",
        "    conduct(ExpD__('A', 'B', \"_DA_DEF\", aug_mode=DataAugMode.DEFAULT))\n",
        "    conduct(ExpD__('A', 'C', \"_DA_ENH\", aug_mode=DataAugMode.ENHANCED))\n",
        "    conduct(ExpD__('B', 'A', \"_WD_E-3\", weight_decay=0.001))\n",
        "    conduct(ExpD__('B', 'B', \"_WD-E-2\", weight_decay=0.01))\n",
        "    conduct(ExpD__('B', 'C', \"_WD_E-1\", weight_decay=0.1))\n",
        "    conduct(ExpD__('B', 'D', \"_WD_E-0\", weight_decay=1.))\n",
        "    conduct(ExpD__('C', 'A', \"_DI_NON\", di_approach=DIApproach.NONE))\n",
        "    conduct(ExpD__('C', 'B', \"_DI_WLF\", di_approach=DIApproach.WEIGHTED_LOSS_FUNCTION))\n",
        "    conduct(ExpD__('D', 'A', \"_AS_DEF\", aug_mode=DataAugMode.DEFAULT, weight_decay=0.001, alt_scheduler=True))\n",
        "    conduct(ExpD__('D', 'B', \"_AS_ENH\", aug_mode=DataAugMode.ENHANCED, weight_decay=0.001, alt_scheduler=True))\n",
        "    #conduct(ExpDV_('A', aug_mode=DataAugMode.NONE))\n",
        "    #conduct(ExpDV_('B', aug_mode=DataAugMode.DEFAULT))\n",
        "    #conduct(ExpDV_('C', aug_mode=DataAugMode.ENHANCED))    "
      ]
    },
    {
      "source": [
        "## Experiment Group E: Two Stage Model\n",
        "\n",
        "### Introduction\n",
        "\n",
        "The confusion matrix from the most accurate models was re-ordered (in another notebook) using a greedy method so that the largest number of misclassifications were closest to the diagnoal. From this analysis, it was determined that a modest accuracy improvement was possible if models could more accurately classify images belonging to two or three classes rather than 13. The experiments in this group explore whether any of the theoretical improvements can be realized.\n",
        "\n",
        "A two stage model with three second stages was implemented. The stage definitions are as follows. The numbers in backets are labels and the numbers after the colon are the number of samples in each class.\n",
        "\n",
        "```\n",
        "Stage1:\n",
        "  [00] githeri: 479\n",
        "  [01] mandazi: 620\n",
        "  [02] masalachips: 438\n",
        "  [03] matoke: 483\n",
        "  [04] mukimo: 212\n",
        "  [05] pilau: 329\n",
        "  [06] group1: 1494\n",
        "  [07] group2: 1451\n",
        "  [08] group3: 1030\n",
        "\n",
        "Stage2a:\n",
        "  [00] bhaji: 632\n",
        "  [01] chapati: 862\n",
        "\n",
        "Stage2b:\n",
        "  [00] kachumbari: 494\n",
        "  [01] kukuchoma: 173\n",
        "  [02] nyamachoma: 784\n",
        "\n",
        "Stage2c:\n",
        "  [00] sukumawiki: 402\n",
        "  [01] ugali: 628\n",
        "```\n",
        "\n",
        "### Results\n",
        "\n",
        "Table E1 enumerates the results of each stage as well as entire two-stage model. The first stage's accuracy was better than the best single stage model. Likewise, Stage2a accurately classified bhaji and chapati samples. Stage2b and Stage2c's accuracies were slighly lower than the best single stage model. Disappointingly, the two-stage model's accuracy was slightly lower than the best single stage model.\n",
        "\n",
        "**Table E1:** The analysis of the two stage model.\n",
        "|Experiment|Test Loss|Accuracy|Overfitting Metric|\n",
        "|:---|:---:|:---:|:---:|\n",
        "|EAA_EfficientNetB4_Stage1|0.354|89.14|0.027|\n",
        "|EAB_EfficientNetB4_Stage2a|0.170|94.00|0.019|\n",
        "|EAC_EfficientNetB4_Stage2b|0.520|80.00|0.034|\n",
        "|EAD_EfficientNetB4_Stage2c|0.543|80.56|0.051|\n",
        "|EAE_EfficientNetB4_TwoStage||81.73||\n",
        "\n",
        "The confusion matrices of each stage are shown below in non-graphical form.\n",
        "\n",
        "```\n",
        "Two Stage - Accuracy: 81.72782874617737\n",
        "\n",
        "[[ 95   7   0   2   0   4   1   2   2   1   5   1   6]\n",
        " [  8 157   0   1   0   0   0   1   0   0   1   1   3]\n",
        " [  1   2  91   0   0   0   1   0   0   0   0   0   1]\n",
        " [  2   1   2  68   3   2   0   0   0   9   4   2   6]\n",
        " [  0   0   0   1  20   0   1   1   0  12   0   0   0]\n",
        " [  1   0   0   0   1 121   0   0   0   0   0   0   1]\n",
        " [  3   0   0   1   1   0  82   0   0   0   0   0   1]\n",
        " [  6   2   1   0   0   1   1  79   1   1   1   1   3]\n",
        " [  0   0   1   0   0   1   2   1  34   0   0   1   2]\n",
        " [  2   5   1  11  10   2   1   0   0 117   4   0   4]\n",
        " [  0   0   0   0   0   0   0   1   0   0  63   2   0]\n",
        " [  0   3   4   3   0   1   0   3   0   1   1  43  21]\n",
        " [  0   1   0   5   0   0   0   3   0   3   0  15  99]]\n",
        "\n",
        "[126 172  96  99  35 124  88  97  42 157  66  80 126]\n",
        "\n",
        "1069 / 1308 = 0.8172782874617737\n",
        "\n",
        "|Experiment|Test Loss|Accuracy|Overfitting Metric|\n",
        "|:---|:---:|:---:|:---:|\n",
        "|EAA_EfficientNetB4_Stage1|0.354|89.14|0.027|\n",
        "|EAB_EfficientNetB4_Stage2a|0.170|94.00|0.019|\n",
        "|EAC_EfficientNetB4_Stage2b|0.520|80.00|0.034|\n",
        "|EAD_EfficientNetB4_Stage2c|0.543|80.56|0.051|\n",
        "\n",
        "stage_results[0]\n",
        "\n",
        "[[ 91   0   1   0   0   0   3   0   1]\n",
        " [  0 121   0   0   0   0   1   1   1]\n",
        " [  0   0  82   0   0   0   3   2   1]\n",
        " [  1   1   1  79   1   1   8   1   4]\n",
        " [  1   1   2   1  34   0   0   0   3]\n",
        " [  0   0   0   1   0  63   0   0   2]\n",
        " [  0   4   1   3   2   6 267   4  11]\n",
        " [  3   4   2   1   0   8  10 251  12]\n",
        " [  4   1   0   6   0   1   4  12 178]]\n",
        "\n",
        "1166 / 1308 = 0.8914373088685015\n",
        "\n",
        "stage_results[1]\n",
        "\n",
        "[[ 95   7   0]\n",
        " [  8 157   0]\n",
        " [ 15  14   0]]\n",
        "\n",
        "252 / 296 = 0.8513513513513513\n",
        "\n",
        "stage_results[2]\n",
        "\n",
        "[[ 68   3   9   0]\n",
        " [  1  20  12   0]\n",
        " [ 11  10 117   0]\n",
        " [ 12   2   6   0]]\n",
        "\n",
        "205 / 271 = 0.7564575645756457\n",
        "\n",
        "stage_results[3]\n",
        "\n",
        "[[43 21  0]\n",
        " [15 99  0]\n",
        " [ 8 27  0]]\n",
        "\n",
        "142 / 213 = 0.6666666666666666\n",
        "```\n",
        "\n",
        "### Conclusions\n",
        "\n",
        "The two-stage model's accuracy was slightly lower than the best single stage model's accuracy."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "                                                            # original\n",
        "                                                            # label(s)\n",
        "stage_info = [                                              # --------\n",
        "    'githeri',                                              #  2\n",
        "    'mandazi',                                              #  5\n",
        "    'masalachips',                                          #  6\n",
        "    'matoke',                                               #  7\n",
        "    'mukimo',                                               #  8\n",
        "    'pilau',                                                #  10\n",
        "    ('group1', ['bhaji', 'chapati']),                       #  0, 1\n",
        "    ('group2', ['kachumbari', 'kukuchoma', 'nyamachoma']),  #  3, 4, 9\n",
        "    ('group3', ['sukumawiki', 'ugali'])                     #  11, 12\n",
        "]\n",
        "\n",
        "def print_stages(stages):\n",
        "    if isinstance(stages, KF13TrainingData):\n",
        "        for index, (cname, ccount) in enumerate(zip(stages.class_names, stages.class_counts)):\n",
        "            print(f\"[{index:02d}] {cname}: {ccount}\")\n",
        "\n",
        "    else:\n",
        "        first = True\n",
        "        for stage in stages:\n",
        "            if not first: print()\n",
        "            print(f\"{stage[0]}:\")\n",
        "            for index, (cname, ccount) in enumerate(zip(stage[1].class_names, stage[1].class_counts)):\n",
        "                print(f\"  [{index:02d}] {cname}: {ccount}\")\n",
        "            first = False\n",
        "\n",
        "classifier_stages = datastore.get_two_stage_training_data(stage_info)\n",
        "print_stages(classifier_stages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TwoStageModel:\n",
        "    def __init__(\n",
        "        self, \n",
        "        stage_exp: List[ModelExperiment],\n",
        "        stage_info: List[Union[str, Tuple[str, List[str]]]],\n",
        "        class_names: List[str],\n",
        "        fnames: List[str],\n",
        "        labels: List[int] = None\n",
        "    ):\n",
        "        self.__stage_exp = stage_exp\n",
        "        self.__stage_info = stage_info\n",
        "        self.__class_names = class_names\n",
        "        self.__fnames = fnames\n",
        "        self.__labels = labels\n",
        "\n",
        "    def inference(self) -> Dict[str, List[int]]:\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        stage_results = []\n",
        "        model_results = {}\n",
        "        stage_inf = [self.__stage_info]\n",
        "        stage_idx = [list(range(len(self.__fnames)))]\n",
        "        stages = datastore.create_stages(self.__stage_info)\n",
        "\n",
        "        for idx in range(len(self.__stage_exp)):\n",
        "            exp = self.__stage_exp[idx]\n",
        "            model = exp.trained_model\n",
        "            config = exp.config\n",
        "            device = exp.device\n",
        "            mapping = stages[idx][1]\n",
        "\n",
        "            dataset = KF13IndexedDataset(\n",
        "                datastore.image_root,\n",
        "                stage_idx[idx],\n",
        "                self.__fnames,\n",
        "                [mapping[label] for label in self.__labels],\n",
        "                config.dataset.test_transforms\n",
        "            )\n",
        "\n",
        "            data_loader = DataLoader(\n",
        "                dataset = dataset,\n",
        "                batch_size = config.data_loader.batch_size,\n",
        "                num_workers = config.data_loader.num_workers,\n",
        "                shuffle = False\n",
        "            )\n",
        "\n",
        "            results, targets, preds = TwoStageModel.__predict_two_stage_valid_data(\n",
        "                model, \n",
        "                data_loader, \n",
        "                device, \n",
        "                len(exp.class_names)\n",
        "            )\n",
        "\n",
        "            stage_results.append((targets, preds))\n",
        "            for stage, result in zip(stage_inf[idx], results):\n",
        "                if isinstance(stage, str):\n",
        "                    model_results[stage] = result\n",
        "                else:\n",
        "                    stage_inf.append(stage[1])\n",
        "                    stage_idx.append(result)\n",
        "\n",
        "            del exp\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        return model_results, stage_results\n",
        "        \n",
        "    def get_targets_and_preds(\n",
        "        self, \n",
        "        results: Dict[str, List[int]], \n",
        "        preserve_order: bool = False\n",
        "    ) -> Tuple[List[int], List[int]]:\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        class_dict = datastore.class_dict\n",
        "        if not preserve_order:\n",
        "            preds = []\n",
        "            targets = []\n",
        "            for (class_name, indices) in results.items():\n",
        "                preds.extend([class_dict[class_name]] * len(indices))\n",
        "                targets.extend([self.__labels[index] for index in indices])\n",
        "        else:\n",
        "            count = len(self.__labels)\n",
        "            preds = [-1] * count\n",
        "            targets = [-1] * count\n",
        "            for (class_name, indices) in results.items():\n",
        "                pred = class_dict[class_name]\n",
        "                for index in indices:\n",
        "                    preds[index] = pred\n",
        "                    targets[index] = self.__labels[index]\n",
        "\n",
        "        return targets, preds\n",
        "\n",
        "    @classmethod\n",
        "    def compute_accuracy(cls, targets:List[int], preds:List[int]) -> float:\n",
        "        count = 0\n",
        "        for (pred, target) in zip(preds, targets):    \n",
        "            if pred == target:\n",
        "                count += 1\n",
        "        return float(count) / len(preds)\n",
        "\n",
        "    @classmethod\n",
        "    def __predict_two_stage_valid_data(cls, model, dataloader, device, num_classes):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        model.to(device)  # send model to cpu or cuda\n",
        "        model.eval()      # set model to evaluation mode\n",
        "        preds = []\n",
        "        targets = []\n",
        "        results = [[] for _ in range(num_classes)]\n",
        "        for idx, data, target in dataloader:\n",
        "            pred, _ = predict_batch(model, data.to(device), max_prob=True)\n",
        "            preds.extend(pred.tolist())\n",
        "            targets.extend(target.tolist())\n",
        "            for (_idx, _pred) in zip(idx.numpy(), pred):\n",
        "                results[_pred].append(_idx)       \n",
        "        return results, targets, preds"
      ]
    },
    {
      "source": [
        "### Validate TwoStageModel Class\n",
        "\n",
        "To validate the two stage model class, the predictions/targets generated by a ModelExperiment derived class will be compared to the predictions/targets generated by the TwoStageModel class (albeit using the aforementioned derived class' model as the only stage).\n",
        "\n",
        "**ModelExperiment Derived Class Test**\n",
        "```\n",
        "exp1a = ExpD__('A', 'B', \"_DA_DEF\", aug_mode=DataAugMode.DEFAULT)\n",
        "targets1a, preds1a = predict_valid_data(\n",
        "    exp1a.trained_model, \n",
        "    exp1a.valid_loader, \n",
        "    exp1a.device\n",
        ")\n",
        "preds1a = preds1a.tolist()\n",
        "targets1a = targets1a.tolist()\n",
        "\n",
        "accuracy1a = TwoStageModel.compute_accuracy(targets1a, preds1a)\n",
        "cm1a = confusion_matrix(targets1a, preds1a)\n",
        "trace1a = np.trace(cm1a)\n",
        "total1a = np.sum(cm1a)\n",
        "\n",
        "print(f\"One Stage - Accuracy: {100 * accuracy1a}\")\n",
        "print()\n",
        "print(cm1a)\n",
        "print()\n",
        "print(np.sum(cm1a, axis=1))\n",
        "print()\n",
        "print(f\"{trace1a} / {total1a} = {trace1a / total1a}\")\n",
        "```\n",
        "\n",
        "**TwoStageModel Test**\n",
        "```\n",
        "data = datastore.get_training_data()\n",
        "class_names = data.class_names\n",
        "\n",
        "model1b = TwoStageModel(\n",
        "    stage_exp = [exp1a],\n",
        "    stage_info = class_names,\n",
        "    class_names = data.class_names,\n",
        "    fnames = data.valid_fnames,\n",
        "    labels = data.valid_labels\n",
        ")\n",
        "\n",
        "model_results1b, stage_results1b = model1b.inference()\n",
        "targets1b, preds1b = model1b.get_targets_and_preds(model_results1b, preserve_order=True)\n",
        "\n",
        "accuracy1b = TwoStageModel.compute_accuracy(targets1b, preds1b)\n",
        "cm1b = confusion_matrix(targets1b, preds1b)\n",
        "trace1b = np.trace(cm1b)\n",
        "total1b = np.sum(cm1b)\n",
        "\n",
        "print(f\"Two Stage - Accuracy: {100 * accuracy1b}\")\n",
        "print()\n",
        "print(cm1b)\n",
        "print()\n",
        "print(np.sum(cm1b, axis=1))\n",
        "print()\n",
        "print(f\"{trace1b} / {total1b} = {trace1b / total1b}\")\n",
        "```\n",
        "\n",
        "The predictions and targets were identical both both approaches and their test code produced the following output.\n",
        "\n",
        "```\n",
        "One/Two Stage - Accuracy: 82.87461773700305\n",
        "\n",
        "[[ 94   6   1   2   2   4   2   2   2   2   4   2   3]\n",
        " [  7 151   0   2   1   2   0   1   0   0   3   2   3]\n",
        " [  0   1  95   0   0   0   0   0   0   0   0   0   0]\n",
        " [  2   0   2  79   3   1   0   1   0   6   2   2   1]\n",
        " [  0   0   0   3  21   0   0   1   0  10   0   0   0]\n",
        " [  2   0   0   0   1 120   0   0   0   0   0   0   1]\n",
        " [  3   0   0   2   1   0  80   1   0   0   0   0   1]\n",
        " [  5   1   2   1   0   0   0  80   0   3   2   0   3]\n",
        " [  0   0   1   1   0   0   1   1  36   0   0   1   1]\n",
        " [  0   7   1  11  10   0   3   1   0 115   3   3   3]\n",
        " [  1   0   0   1   0   0   0   1   0   0  62   1   0]\n",
        " [  0   1   3   1   0   0   0   1   0   2   1  60  11]\n",
        " [  0   1   1   6   1   0   0   3   0   4   1  18  91]]\n",
        "\n",
        "[126 172  96  99  35 124  88  97  42 157  66  80 126]\n",
        "\n",
        "1084 / 1308 = 0.8287461773700305\n",
        "```"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ExpEA_(ModelExperiment):\n",
        "    def __init__(self, stage:str):\n",
        "\n",
        "        pretrained = True\n",
        "        tuning_level = 5\n",
        "        model_type = EfficientNetB4\n",
        "        image_size = get_image_size(model_type)\n",
        "        batch_size = get_batch_size(model_type)\n",
        "\n",
        "        self.__pretrained = pretrained\n",
        "        self.__tuning_level = tuning_level\n",
        "        self.__model_type = model_type\n",
        "\n",
        "        if stage.lower() == \"stage1\":\n",
        "            exp_id = 'A'\n",
        "            self.__split_index = 0\n",
        "        elif stage.lower() == \"stage2a\":\n",
        "            exp_id = 'B'\n",
        "            self.__split_index = 1\n",
        "        elif stage.lower() == \"stage2b\":\n",
        "            exp_id = 'C'\n",
        "            self.__split_index = 2\n",
        "        elif stage.lower() == \"stage2c\":\n",
        "            exp_id = 'D'\n",
        "            self.__split_index = 3\n",
        "\n",
        "        super().__init__(\n",
        "            abbr = \"EA\" + exp_id,\n",
        "            transform_resize = int(image_size * 16 / 14),\n",
        "            transform_crop_size = image_size,\n",
        "            data_loader_num_workers = 12,\n",
        "            data_loader_batch_size = batch_size,\n",
        "            optimizer = Optimizer.ADAM,\n",
        "            optimizer_learning_rate = 1.0E-04 * math.sqrt(0.1),\n",
        "            scheduler_step_size = 10,\n",
        "            scheduler_gamma = math.sqrt(0.1),\n",
        "            trainer_training_epochs = 100,\n",
        "            trainer_stop_loss_epochs = 15\n",
        "        )\n",
        "\n",
        "    def _get_data(self) -> KF13TrainingData:\n",
        "        stage = classifier_stages[self.__split_index]\n",
        "        self.__model_abbr = self.__model_type.__name__ + '_' + stage[0]\n",
        "        return stage[1]\n",
        "        \n",
        "    def _get_model(self):\n",
        "        return self.__model_type(self.__pretrained, self.__tuning_level), self.__model_abbr\n",
        "\n",
        "def conduct_group_E():\n",
        "    conduct(ExpEA_(\"Stage1\"))\n",
        "    conduct(ExpEA_(\"Stage2A\"))\n",
        "    conduct(ExpEA_(\"Stage2B\"))\n",
        "    conduct(ExpEA_(\"Stage2C\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def conduct_two_stage_model_experiment():\n",
        "    data = datastore.get_training_data()\n",
        "    class_names = data.class_names\n",
        "\n",
        "    model2 = TwoStageModel(\n",
        "        stage_exp = [ExpEA_(\"Stage1\"), ExpEA_(\"Stage2a\"), ExpEA_(\"Stage2b\"), ExpEA_(\"Stage2c\")],\n",
        "        stage_info = stage_info,\n",
        "        class_names = data.class_names,\n",
        "        fnames = data.valid_fnames,\n",
        "        labels = data.valid_labels\n",
        "    )\n",
        "\n",
        "    model_results2, stage_results2 = model2.inference()\n",
        "    targets2, preds2 = model2.get_targets_and_preds(model_results2, preserve_order=True)\n",
        "\n",
        "    accuracy2 = TwoStageModel.compute_accuracy(targets2, preds2)\n",
        "    cm2 = confusion_matrix(targets2, preds2)\n",
        "    trace2 = np.trace(cm2)\n",
        "    total2 = np.sum(cm2)\n",
        "\n",
        "    print(f\"Two Stage - Accuracy: {100 * accuracy2}\")\n",
        "    print()\n",
        "    print(cm2)\n",
        "    print()\n",
        "    print(np.sum(cm2, axis=1))\n",
        "    print()\n",
        "    print(f\"{trace2} / {total2} = {trace2 / total2}\")\n",
        "    for idx, stage_result in enumerate(stage_results2):\n",
        "        stage_preds = stage_result[1]\n",
        "        stage_targets = stage_result[0]\n",
        "        stage_cm = confusion_matrix(stage_targets, stage_preds)\n",
        "        stage_trace = np.trace(stage_cm)\n",
        "        stage_total = np.sum(stage_cm)\n",
        "        print()\n",
        "        print(f\"stage_results[{idx}]\")\n",
        "        print()\n",
        "        print(stage_cm)\n",
        "        print()\n",
        "        print(f\"{stage_trace} / {stage_total} = {stage_trace / stage_total}\")\n",
        "\n",
        "#conduct_two_stage_model_experiment()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx9Gbfds3-53"
      },
      "source": [
        "### <font style=\"color:blue\">Main Function</font>\n",
        "\n",
        "A simple function that conducts groups of experiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FFvSmzspuww"
      },
      "source": [
        "def analyze_runs():\n",
        "    # Warning: Make sure analysis destination directories exist!\n",
        "    analyze_runs_for_learning_rate(filters=[\"^BA\", \"^BB\", \"^BC\"]) # analyze Group B experiments\n",
        "    analyze_runs_for_overfitting(filter=\"^C\")                     # analyze Group C experiments\n",
        "    analyze_runs_for_overfitting(filter=\"^D\")                     # analyze Group D experiments\n",
        "    analyze_runs_for_overfitting(filter=\"^E\")                     # analyze Group E experiments\n",
        "\n",
        "def create_submission():\n",
        "    # fetch the experiment that trained the most accurate model\n",
        "    best_exp = ExpD__('A', 'B', \"_DA_DEF\", aug_mode=DataAugMode.DEFAULT)\n",
        "\n",
        "    # log its confusion matrix and precision-recall curves\n",
        "    conduct(best_exp, train_model=False, log_confusion_matrix=True, log_pr_curves=True)\n",
        "\n",
        "    # create the submission file that classifies the private test data\n",
        "    config = best_exp.config\n",
        "    create_submission_file(\n",
        "        path = os.path.join(proj_dir, \"submission.csv\"),\n",
        "        exp = best_exp,\n",
        "        dataloader = datastore.get_test_data_loader(\n",
        "            transform = config.dataset.test_transforms,\n",
        "            batch_size = config.data_loader.batch_size,\n",
        "            num_workers = config.data_loader.num_workers\n",
        "        )\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9Q6_bS9lDh8"
      },
      "source": [
        "def main():   \n",
        "    for group in ['A', 'B', 'C', 'D', 'E']:\n",
        "        if group == 'A':\n",
        "            conduct_group_A()\n",
        "        elif group == 'B':\n",
        "            conduct_group_B()\n",
        "        elif group == 'C':\n",
        "            conduct_group_C()\n",
        "        elif group == 'D':\n",
        "            conduct_group_D()\n",
        "        elif group == 'E':\n",
        "            conduct_group_E()\n",
        "    analyze_runs()\n",
        "    create_submission()\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4Fe0mLZ3-53",
        "scrolled": false
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    print(\"Uncomment the 'main()' function to run project.\")\n",
        "    #main()    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "## Project Conclusions\n",
        "\n",
        "The experiment with the best accuracy was **DAB_EfficientNetB4_DA_DEF**. This experiment fine tuned the entire convolutional base and classifier of a pretrained EfficientNet-B4 model using the default data augmentation, a weighted random sampler, a very small L2 regularization. Its accuracy in classifying the samples in the validation set is 82.87%. Its confusion matrix is depicted in Figure 3.\n",
        "\n",
        "![The confusion matrix of the most accurate model.](https://media.githubusercontent.com/media/blazingcayenne/deep_learning_with_pytorch_project2/main/images/DAB_Confusion_Matrix.png?raw=true)<br>\n",
        "**Figure 3:** The confusion matrix of the most accurate model.\n",
        "\n",
        "### Future Exploration\n",
        "\n",
        "The following areas warrant future exploration.\n",
        "\n",
        "* Investigate training other pretrained model families to determine whether they yield greater accuracy than the EfficientNet model family of the KenyanFood13 dataset.\n",
        "* Investigate why increasing the regularization not only reduced overfitting, but also reduced the model's accuracy.\n",
        "* Investigate whether the accuracies of the second stages of the two-stage model could be increased, thereby increasing the overall accuracy of the two-stage model.\n",
        "* Investigate whether ensembles of models from different families increase accuracy. In other words, are some model families better at classifying problematic samples than others?"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFt15GPK3-53"
      },
      "source": [
        "## <font style=\"color:green\">8. TensorBoard Dev Scalars Log Link [5 Points]</font>\n",
        "\n",
        "Share your tensorboard scalars logs link in this section. You can also share (not mandatory) your GitHub link if you have pushed this project in GitHub. \n",
        "\n",
        "For example, [Find Project2 logs here](https://tensorboard.dev/experiment/kMJ4YU0wSNG0IkjrluQ5Dg/#scalars)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh36KFmK3-53"
      },
      "source": [
        "## <font style=\"color:green\">9. Kaggle Profile Link [50 Points]</font>\n",
        "\n",
        "Share your Kaggle profile link here with us so that we can give points for the competition score. \n",
        "\n",
        "You should have a minimum accuracy of `75%` on the test data to get all points. If accuracy is less than `70%`, you will not get any points for the section. \n",
        "\n",
        "**You must have to submit `submission.csv` (prediction for images in `test.csv`) in `Submit Predictions` tab in Kaggle to get any evaluation in this section.**"
      ]
    }
  ]
}